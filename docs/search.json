[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Visual above made with Nathaniel Phillip’s VisualResume.\nI am currently working as a Data Scientist at Cogstate where I get to work with our Science and Data teams work with outcomes from computerized, cognitive assessments and other outcomes data (e.g., COA, ClinRO, PRO).\nI started learning R in my gradate statistics classes to avoid the future pains of having to use or only being familiar with SPSS. Learning R then launched me into a bigger role at Bracket (now Signant Health) where I had the pleasure to work with our Clinical Science team to understand and interpret their data. This gave us the opportunity to learn more about our technologies, approaches, and quality of reporting in the clinical trials we were assisting. Here I was a charter member of the Data Science team and have the pleasure of enhancing our trial monitoring products, training other Data Science programmers, and contributing to automation and reporting efforts of our Clinical Data Management team. I worked as the Principal Data Analyst for Madrigal Pharmaceuticals, helping our Research & Development team monitor our current trials and prepare summary reports. This was a bit of a departure for me to work with more lab and imaging data and in an area I didn’t have any experience: liver disease. Despite that, there were plenty of data I recognized: from clinical operations, data management, and even some interested PRO QoL data.\nI have a Masters of Art in Psychology and have spent my graduate and undergrad classes engaging in Neuropsychology and Cognitive topics. Some of these topics have included understanding how music affects our ability on tests of aptitude. This, of course, was the basis of my Masters Thesis and hours of reading journal article after journal article. Of course I’ve also been very interested in immersing myself more into statistics, statistical programming, and special psychometric subjects. Please browse the Anthology page where I have tried to compile projects, papers, and presentations I’ve written in and out of class.\nAnd of course, here is my dog’s IG TheLabBuckley. It’s mostly him sleeping."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "J M Barbone",
    "section": "",
    "text": "R\n\n\ndplyr\n\n\ntidyselect\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\npsychology\n\n\ndatavis\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dplyr-select/index.html",
    "href": "posts/dplyr-select/index.html",
    "title": "Selecting columns in dplyr",
    "section": "",
    "text": "Let’s walk through an example.\n\nlibrary(dplyr, warn.conflicts = FALSE)\n\nWe’ll use a data set from the psych package that has relatively, very short column names. We’ll want to take these and make them a bit more specific without incurring much penalty with ourselves. Trying to manage column names with spaces and other special characters can be a real thorn in the index finger.\n\ndata(\"bfi\", package = \"psych\")\nsapa <- tibble::as_tibble(bfi)[1:100, c(1:2, 6:7, 11:12, 16:17, 26:28)] # shorter\nsapa\n#> # A tibble: 100 × 11\n#>       A1    A2    C1    C2    E1    E2    N1    N2 gender education   age\n#>    <int> <int> <int> <int> <int> <int> <int> <int>  <int>     <int> <int>\n#>  1     2     4     2     3     3     3     3     4      1        NA    16\n#>  2     2     4     5     4     1     1     3     3      2        NA    18\n#>  3     5     4     4     5     2     4     4     5      2        NA    17\n#>  4     4     4     4     4     5     3     2     5      2        NA    17\n#>  5     2     3     4     4     2     2     2     3      1        NA    17\n#>  6     6     6     6     6     2     1     3     5      2         3    21\n#>  7     2     5     5     4     4     3     1     2      1        NA    18\n#>  8     4     3     3     2     3     6     6     3      1         2    19\n#>  9     4     3     6     6     5     3     5     5      1         1    19\n#> 10     2     5     6     5     2     2     5     5      2        NA    17\n#> # … with 90 more rows\n\nWe can create a named vector to help keep track of the longer, more specific names of our output data. The first 8 columns will be renamed and the demographic information will be moved to the start.\n\nlong_names <- c(\n  \"gender\",\n  \"education\",\n  \"age\",\n  \"Indifferent to feelings\"      = \"A1\",\n  \"Inquire about well-being\"     = \"A2\",\n  \"Exacting about work\"          = \"C1\",\n  \"Continue until perfection\"    = \"C2\",\n  \"Don't talk a lot\"             = \"E1\",\n  \"Difficult to approach others\" = \"E2\",\n  \"Get angry easily\"             = \"N1\",\n  \"Get irritated easily\"         = \"N2\"\n)\n\nHere’s the typical solution.\n\nsapa %>% \n  select(\n    gender,\n    education,\n    age,\n    `Indifferent to feelings`      = A1,\n    `Inquire about well-being`     = A2,\n    `Exacting about work`          = C1,\n    `Continue until perfection`    = C2,\n    `Don't talk a lot`             = E1,\n    `Difficult to approach others` = E2,\n    `Get angry easily`             = N1,\n    `Get irritated easily`         = N2\n  )\n#> # A tibble: 100 × 11\n#>    gender education   age `Indifferent to fe…` `Inquire about…` `Exacting abou…`\n#>     <int>     <int> <int>                <int>            <int>            <int>\n#>  1      1        NA    16                    2                4                2\n#>  2      2        NA    18                    2                4                5\n#>  3      2        NA    17                    5                4                4\n#>  4      2        NA    17                    4                4                4\n#>  5      1        NA    17                    2                3                4\n#>  6      2         3    21                    6                6                6\n#>  7      1        NA    18                    2                5                5\n#>  8      1         2    19                    4                3                3\n#>  9      1         1    19                    4                3                6\n#> 10      2        NA    17                    2                5                6\n#> # … with 90 more rows, and 5 more variables: `Continue until perfection` <int>,\n#> #   `Don't talk a lot` <int>, `Difficult to approach others` <int>,\n#> #   `Get angry easily` <int>, `Get irritated easily` <int>\n\nWe can use the tidyselect::all_of() function without as it is reexported with dplyr.\n\nsapa %>% \n  select(all_of(long_names))\n#> # A tibble: 100 × 11\n#>    gender education   age `Indifferent to fe…` `Inquire about…` `Exacting abou…`\n#>     <int>     <int> <int>                <int>            <int>            <int>\n#>  1      1        NA    16                    2                4                2\n#>  2      2        NA    18                    2                4                5\n#>  3      2        NA    17                    5                4                4\n#>  4      2        NA    17                    4                4                4\n#>  5      1        NA    17                    2                3                4\n#>  6      2         3    21                    6                6                6\n#>  7      1        NA    18                    2                5                5\n#>  8      1         2    19                    4                3                3\n#>  9      1         1    19                    4                3                6\n#> 10      2        NA    17                    2                5                6\n#> # … with 90 more rows, and 5 more variables: `Continue until perfection` <int>,\n#> #   `Don't talk a lot` <int>, `Difficult to approach others` <int>,\n#> #   `Get angry easily` <int>, `Get irritated easily` <int>\n\n\n\nBut is it faster?\n\n\nfoo_select_all_of <- function() {\n  long_names <- c(\n    \"gender\",\n    \"education\",\n    \"age\",\n    \"Indifferent to feelings\"      = \"A1\",\n    \"Inquire about well-being\"     = \"A2\",\n    \"Exacting about work\"          = \"C1\",\n    \"Continue until perfection\"    = \"C2\",\n    \"Don't talk a lot\"             = \"E1\",\n    \"Difficult to approach others\" = \"E2\",\n    \"Get angry easily\"             = \"N1\",\n    \"Get irritated easily\"         = \"N2\"\n  )\n  \n  sapa %>% \n  select(all_of(long_names))\n}\n\nfoo_select <- function() {\n  sapa %>% \n  select(\n    gender,\n    education,\n    age,\n    `Indifferent to feelings`      = A1,\n    `Inquire about well-being`     = A2,\n    `Exacting about work`          = C1,\n    `Continue until perfection`    = C2,\n    `Don't talk a lot`             = E1,\n    `Difficult to approach others` = E2,\n    `Get angry easily`             = N1,\n    `Get irritated easily`         = N2\n  )\n}\n\nbench::mark(\n  foo_select_all_of(),\n  foo_select()\n)\n#> # A tibble: 2 × 6\n#>   expression               min   median `itr/sec` mem_alloc `gc/sec`\n#>   <bch:expr>          <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n#> 1 foo_select_all_of()   1.51ms   1.78ms      502.    8.67KB     21.2\n#> 2 foo_select()          5.82ms   6.91ms      119.   41.45KB     22.8\n\nYes.\n\nWe get the same result and don’t need to clog up the piped if we need to do some mutation, grouping, summarising, etc. This also lets us separate out definitions of the data in case we need to change things:\n\nlong_names_less <- long_names[c(1, 3, grep(\"about\", names(long_names)))]\n\nsapa %>% \n  select(all_of(long_names_less))\n#> # A tibble: 100 × 4\n#>    gender   age `Inquire about well-being` `Exacting about work`\n#>     <int> <int>                      <int>                 <int>\n#>  1      1    16                          4                     2\n#>  2      2    18                          4                     5\n#>  3      2    17                          4                     4\n#>  4      2    17                          4                     4\n#>  5      1    17                          3                     4\n#>  6      2    21                          6                     6\n#>  7      1    18                          5                     5\n#>  8      1    19                          3                     3\n#>  9      1    19                          3                     6\n#> 10      2    17                          5                     6\n#> # … with 90 more rows\n\n\nUsing any_of() instead we could essentially pre-define more “programming” and “output” names and pass it to whatever you are working with. This has been useful by establishing a saved vector of names and using it across multiple reports to keep our naming convention consistent.\n\nWe can even write some short functions in case we need to use an output we’ve created before:\n\nnames_fill <- function(x) {\n  nm <- names(x)\n  blanks <- nm == \"\"\n  names(x)[blanks] <- x[blanks]\n  x\n}\n\n\nsapa2 <- sapa %>% select(all_of(long_names))\nlong_names_switched <- mark::names_switch(names_fill(long_names))\nlong_names\n#>                                                           \n#>                     \"gender\"                  \"education\" \n#>                                   Indifferent to feelings \n#>                        \"age\"                         \"A1\" \n#>     Inquire about well-being          Exacting about work \n#>                         \"A2\"                         \"C1\" \n#>    Continue until perfection             Don't talk a lot \n#>                         \"C2\"                         \"E1\" \n#> Difficult to approach others             Get angry easily \n#>                         \"E2\"                         \"N1\" \n#>         Get irritated easily \n#>                         \"N2\"\nlong_names_switched\n#>                         gender                      education \n#>                       \"gender\"                    \"education\" \n#>                            age                             A1 \n#>                          \"age\"      \"Indifferent to feelings\" \n#>                             A2                             C1 \n#>     \"Inquire about well-being\"          \"Exacting about work\" \n#>                             C2                             E1 \n#>    \"Continue until perfection\"             \"Don't talk a lot\" \n#>                             E2                             N1 \n#> \"Difficult to approach others\"             \"Get angry easily\" \n#>                             N2 \n#>         \"Get irritated easily\"\n\nsapa2 %>% \n  select(all_of(long_names_switched))\n#> # A tibble: 100 × 11\n#>    gender education   age    A1    A2    C1    C2    E1    E2    N1    N2\n#>     <int>     <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n#>  1      1        NA    16     2     4     2     3     3     3     3     4\n#>  2      2        NA    18     2     4     5     4     1     1     3     3\n#>  3      2        NA    17     5     4     4     5     2     4     4     5\n#>  4      2        NA    17     4     4     4     4     5     3     2     5\n#>  5      1        NA    17     2     3     4     4     2     2     2     3\n#>  6      2         3    21     6     6     6     6     2     1     3     5\n#>  7      1        NA    18     2     5     5     4     4     3     1     2\n#>  8      1         2    19     4     3     3     2     3     6     6     3\n#>  9      1         1    19     4     3     6     6     5     3     5     5\n#> 10      2        NA    17     2     5     6     5     2     2     5     5\n#> # … with 90 more rows\n\nAnd now our names are back to normal."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/dunning-kruger/index.html#introduction",
    "href": "posts/dunning-kruger/index.html#introduction",
    "title": "Dunning-Kruger Effect",
    "section": "Introduction",
    "text": "Introduction\nIn my Organization Psychology graduate class at West Chester University, one of our assigned readings (among others) for our week on emotions and moods was (Sheldon, Dunning, and Ames 2014). This article focused on another finding relating to the Dunning-Kruger effect in the workplace. This time, in a task related to emotional intelligence (EI). During the time I remember vaguely hearing somewhere I will never recall, some mathematical issues relating to this well-known psychological phenomenon mentioned in many introductory text books.\nThe Dunning-Kruger effect is founding on the concept that an individual that lacks expertise will be more confident in their abilities than they really are, or overestimate their performance on a task. Yet experts may underestimate their own performance or abilities or be more accurate in their estimations. One thing we can derive from this is possibly that those with lower skill will overestimate their abilities while those more skilled will underestimate their abilities.\nThe following is in direct relation to an article by (Sheldon, Dunning, and Ames 2014). They report a significant relationship between an individual’s actual performance and the difference between their perceived ability and actual performance in three conditions (\\(r_1 = -0.83\\), \\(p_1 < .001\\); \\(r_2 = -0.87\\), \\(p_2 < .001\\); \\(r_3 = -0.84\\), \\(p_3 < .001\\)).\nThey also used these two graphs to representing their findings:\n\n\n\nFigure 1. Overestimation of emotional intelligence (left panel) and performance on the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT; right panel) as a function of actual performance on the MSCEIT\n\n\nWe’ll go through and understand why this can be misleading and how to replicate the Dunning-Kruger effect with random data. Yes, random data. Data that are random."
  },
  {
    "objectID": "posts/dunning-kruger/index.html#set-up",
    "href": "posts/dunning-kruger/index.html#set-up",
    "title": "Dunning-Kruger Effect",
    "section": "Set up",
    "text": "Set up\nSo let’s place with some data and see what we get. First, let’s setup our .Rmd file and choose a specific randomization seed so we can come back to our results (Douglas 1989):\n\nset.seed(42)\n\noptions(tidyverse.quiet = TRUE) ## silences warnings\nlibrary(tidyverse)\nlibrary(mark)                   ## percentile_rank() | github.com/jmbarbone/mark\n\n\nAttaching package: 'mark'\n\n\nThe following object is masked from 'package:purrr':\n\n    none\n\nlibrary(broom)                  ## tidying statistical outputs into tables\ntheme_set(theme_minimal())\n\n\nf_nbins <- function(x, n = 6) {\n  dplyr::ntile(x, n) / n\n}"
  },
  {
    "objectID": "posts/dunning-kruger/index.html#random-data",
    "href": "posts/dunning-kruger/index.html#random-data",
    "title": "Dunning-Kruger Effect",
    "section": "Random data",
    "text": "Random data\nWe’ll start by creating a data frame with two vectors of independent, random data. These will be our randomly assigned percentile ranks of actual and estimate’d performance.\nTo clarify, the calculation of percentile rank is as follows:\n\\[\\text{PR}_i = \\frac{c_\\ell + 0.5 f_i}{N} \\times 100%\\]\nWhere \\(c_\\ell\\) is the count of scores lower than the score of interest, \\(f_i\\) is the frequency of the score of interest, and \\(N\\) is the total number of scores. With this formula, our percentile ranks will always be 0 < \\(PR_i\\) < 100.\n\nrandom_data <- \n  tibble(\n    actual = rnorm(1000),\n    estimate = rnorm(1000)\n  ) %>% \n  mutate(across(everything(), percentile_rank))\n\nrandom_data\n\n# A tibble: 1,000 × 2\n   actual estimate\n    <dbl>    <dbl>\n 1  0.920   0.988 \n 2  0.292   0.704 \n 3  0.657   0.846 \n 4  0.738   0.647 \n 5  0.670   0.150 \n 6  0.462   0.274 \n 7  0.947   0.576 \n 8  0.464   0.0005\n 9  0.980   0.198 \n10  0.480   0.799 \n# … with 990 more rows\n\n\nWe also want to bin our data together just like in the article.\n\nbins <- \n  random_data %>% \n  mutate(\n    difference = estimate - actual,\n    bin = f_nbins(actual, 5)\n  ) %>% \n  group_by(bin) %>% \n  summarise(\n    n = n(),\n    mean = mean(difference)\n  )\n\nbins\n\n# A tibble: 5 × 3\n    bin     n    mean\n  <dbl> <int>   <dbl>\n1   0.2   200  0.398 \n2   0.4   200  0.208 \n3   0.6   200  0.0120\n4   0.8   200 -0.215 \n5   1     200 -0.402 \n\n\nNow we’ll plot the data and take a look at this.\n\nggplot(random_data, aes(x = actual, y = estimate - actual)) +\n  geom_point(alpha = .2) +\n  geom_smooth(formula = \"y ~ x\", method = lm, se = FALSE, col = \"red\") +\n  geom_point(data = bins, aes(x = bin, y = mean), col = \"blue\", shape = 1, size = 5) +\n  geom_line( data = bins, aes(x = bin, y = mean), col = \"blue\", size = 1) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  labs(\n    title = \"Independent random samples of 'Actual' and 'Estimate' performance\",\n    x = \"'Actual' performance (Percentile Rank)\",\n    y = \"Percentile Overestimation\\n(estimate - actual)\"\n  )\n\n\n\n\nAlready we’re seeing a trend very similar to that reported in the article. What we also notice is that there are bounds to the overestimation value as a factor of the individual’s actual performance. An individual that performs at the 99th percentile cannot overestimate their own performance (but can be accurate) - much like an individual in the lower percentiles would unlikely underestimate. These is additionally worse by the use of a score derived in reference to others."
  },
  {
    "objectID": "posts/dunning-kruger/index.html#adjusting-random-data",
    "href": "posts/dunning-kruger/index.html#adjusting-random-data",
    "title": "Dunning-Kruger Effect",
    "section": "Adjusting random data",
    "text": "Adjusting random data\nSo now we’re going to take some data and use some rough estimates for means. We’ll use the results from the study of interest. So simplicity, I’ll just use the rough means of the n, means, and sd reported from the first two studies.\n\nWe’ll shape our normal distributions around the values found in the paper. These values, to be clear, are the percentile ranks either estimated from the participant or the actual ones as they compare to percentile ranking among U.S. adults in EI. As such, we won’t need to use the percentile_rank() again.\n\n\n\nadj_random <- tibble(\n  actual     = rnorm(161, 42.2, sd = 25.1) / 100,\n  estimate   = rnorm(161, 77.5, sd = 13.1) / 100,\n  difference = actual - estimate,\n  bin        = f_nbins(actual, 5)\n)\n\nadj_bins <- \n  adj_random  %>% \n  group_by(bin) %>% \n  summarise(\n    n = n(),\n    mean = mean(difference)\n  )\n\nLet’s also take a look at the correlations we have. As expected, we have no correlation with random data. The article reported correlations of .20 and .19 between estimated and actual performance. Clearly, people are not that great at estimating their own performance.\n\ncor.test(~ actual + estimate, data = adj_random)\n\n\n    Pearson's product-moment correlation\n\ndata:  actual and estimate\nt = 1.3, df = 159, p-value = 0.1955\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.05296245  0.25321085\nsample estimates:\n      cor \n0.1025525 \n\n\nWell, no surprise that that our correlations are a weaker and less statistically significant, we’re using random data after all.\nNow we’re going to run a correlation on the actual scores and the difference between the estimated and actual performance.\n\ncor_test_result <- cor.test(~ actual + difference, data = adj_random)\ncor_test_result\n\n\n    Pearson's product-moment correlation\n\ndata:  actual and difference\nt = 21.717, df = 159, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8197724 0.8991906\nsample estimates:\n      cor \n0.8647931 \n\n\nNow, look at that. We have found an even more significant, negative correlation. This is roughly similar to those reported by in this article. This is with data that has absolutely no relationship between the two variables, as we have justed established.\nSo why is this?"
  },
  {
    "objectID": "posts/dunning-kruger/index.html#plotting-adjusted-random-data",
    "href": "posts/dunning-kruger/index.html#plotting-adjusted-random-data",
    "title": "Dunning-Kruger Effect",
    "section": "Plotting adjusted random data",
    "text": "Plotting adjusted random data\nLet’s graph out our results with a little more care this time.\n\nggplot(adj_random, aes(x = actual, y = difference)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_point(alpha = .1) +\n  geom_smooth(\n    formula = \"y ~ x\",\n    method = \"lm\",\n    se = FALSE,\n    col = \"red\"\n  ) +\n  geom_point(\n    data = adj_bins,\n    aes(x = bin, y = mean),\n    col = \"blue\",\n    shape = 1,\n    size = 5\n  ) +\n  geom_line(\n    data = adj_bins,\n    aes(x = bin, y = mean),\n    col = \"blue\",\n    size = 1\n  ) +\n  labs(\n    title = \"Randomly generated differences in 'actual' vs 'estimated' performance\",\n    subtitle = \"Estimate: M = 75, SD = 15; Actual: M = 5, SD = 25\",\n    x = \"Actual performance\",\n    y = \"Estimated - Actual performance\"\n  ) +\n  annotate(\n    geom = \"text\",\n    label = glue::glue_data(cor_test_result, \"r = {round(estimate, 3)}, p = {format(p.value)}\"),\n    x = .65,\n    y = .50,\n    hjust = \"left\"\n  )"
  },
  {
    "objectID": "posts/dunning-kruger/index.html#more-random-data",
    "href": "posts/dunning-kruger/index.html#more-random-data",
    "title": "Dunning-Kruger Effect",
    "section": "More random data",
    "text": "More random data\nSo what if we repeated this several times?\n\nrandom_helper <- function(x) {\n  set.seed(42 + x)\n  tibble(\n    actual   = rnorm(161, 42.2, sd = 25.1) / 100,\n    estimate = rnorm(161, 77.5, sd = 13.1) / 100,\n  ) %>%\n    mutate(across(everything(), percentile_rank))\n}\n\nsev_random <- \n  as.list(seq(100)) %>% \n  map(random_helper) %>% \n  bind_rows(.id = \"id\") %>% \n  mutate(id = as.numeric(id))\n\nsev_bins <- sev_random %>% \n  group_by(id) %>% \n  mutate(\n    difference = estimate - actual,\n    bin = f_nbins(actual, 5) \n  ) %>% \n  group_by(id, bin) %>% \n  summarise(\n    n = n(),\n    mean_est = mean(estimate),\n    mean_diff = mean(difference)\n  )\n\nggplot(sev_bins, aes(x = bin, y = mean_est, col = factor(id))) +\n  geom_point() +\n  geom_line() +\n  # scale_color_discrete(name = \"Randomization\") +\n  scale_color_discrete(guide = FALSE) +\n  scale_y_continuous(limits = c(0, 1)) + \n  labs(x = \"Actual\", y = \"Estimate\")\n\nWarning: It is deprecated to specify `guide = FALSE` to remove a guide. Please\nuse `guide = \"none\"` instead.\n\n\n\n\n\nSo what if we actually run a correlation on these numbers? We’ll create a nested function and install the broom package to help tidy up our results.\n\nrun_correlations <- function(x, item_x, item_y) {\n  corr_helper <- function(x, item_x, item_y) {\n    formula <- str_c(\"~\", item_x, \"+\", item_y, sep = \" \")\n    cor.test(eval(parse(text = formula)), data = x)\n  }\n  \n  x %>% \n    nest(data = -id) %>% \n    mutate(\n      corr = map(data, corr_helper, item_x, item_y),\n      tidy = map(corr, tidy)\n    ) %>% \n    unnest(tidy) %>% \n    select(where(negate(is.list)))\n}\n\n(x <- run_correlations(sev_random, \"actual\", \"estimate\") %>% arrange(p.value))\n\n# A tibble: 100 × 9\n      id estimate statistic p.value parameter conf.low conf.high method         \n   <dbl>    <dbl>     <dbl>   <dbl>     <int>    <dbl>     <dbl> <chr>          \n 1    35   -0.207     -2.67 0.00830       159 -0.351    -0.0545  Pearson's prod…\n 2    24    0.185      2.37 0.0188        159  0.0312    0.330   Pearson's prod…\n 3    99    0.170      2.17 0.0314        159  0.0154    0.316   Pearson's prod…\n 4    22   -0.164     -2.09 0.0379        159 -0.311    -0.00930 Pearson's prod…\n 5    90   -0.159     -2.03 0.0443        159 -0.306    -0.00417 Pearson's prod…\n 6     6    0.148      1.89 0.0610        159 -0.00685   0.296   Pearson's prod…\n 7     9    0.141      1.79 0.0747        159 -0.0141    0.289   Pearson's prod…\n 8    97    0.133      1.70 0.0914        159 -0.0216    0.282   Pearson's prod…\n 9    51   -0.127     -1.61 0.109         159 -0.276     0.0285  Pearson's prod…\n10    89   -0.124     -1.57 0.118         159 -0.273     0.0316  Pearson's prod…\n# … with 90 more rows, and 1 more variable: alternative <chr>\n\n(y <- run_correlations(sev_bins, \"bin\", \"mean_est\") %>% arrange(p.value))\n\n# A tibble: 100 × 9\n# Groups:   id [100]\n      id estimate statistic p.value parameter conf.low conf.high method         \n   <dbl>    <dbl>     <dbl>   <dbl>     <int>    <dbl>     <dbl> <chr>          \n 1    99    0.936      4.62  0.0191         3    0.312     0.996 Pearson's prod…\n 2    35   -0.936     -4.61  0.0192         3   -0.996    -0.309 Pearson's prod…\n 3    28   -0.907     -3.72  0.0337         3   -0.994    -0.122 Pearson's prod…\n 4    41    0.845      2.74  0.0715         3   -0.146     0.990 Pearson's prod…\n 5    68    0.836      2.64  0.0780         3   -0.177     0.989 Pearson's prod…\n 6     5   -0.804     -2.34  0.101          3   -0.987     0.269 Pearson's prod…\n 7     6    0.796      2.28  0.107          3   -0.290     0.986 Pearson's prod…\n 8    10    0.786      2.20  0.115          3   -0.314     0.985 Pearson's prod…\n 9    89   -0.761     -2.03  0.135          3   -0.983     0.369 Pearson's prod…\n10    13   -0.758     -2.01  0.138          3   -0.983     0.376 Pearson's prod…\n# … with 90 more rows, and 1 more variable: alternative <chr>\n\nmean(x$p.value < .05)\n\n[1] 0.05\n\nmean(y$p.value < .05)\n\n[1] 0.03\n\n\nWhen we calculated a correlation with the mean estimates we actually got a significant result from a few of our runs. If fact, about 5% or less are statistically significant… Let’s pull that one out to look at it again.\n\nsignificant_ids <- x %>% filter(p.value < .05) %>% pull(id) %>% as.character()\ntemp <- sev_bins %>% filter(id %in% significant_ids)\n\nsev_random %>% \n  filter(id %in% significant_ids) %>% \n  ggplot(aes(x = actual, y = estimate, group = factor(id), color = factor(id))) +\n  geom_point(alpha = .2) +\n  geom_point(data = temp, aes(x = bin, y = mean_est)) +\n  geom_line(data = temp, aes(x = bin, y = mean_est))\n\n\n\n\nSo there you have it. A successful replication of this ‘effect’ with random data.\nBut why is this? This is partly because individuals at the lowest quantiles will have a greater likelihood of over-estimating their performance and those at the highest quantiles will underestimate. An individual that performs at the 99th quantile will have almost no choice but to estimate their performance to be below that of reality (see also (Nuhfer et al. 2016)). This seems to be further worsened by the bound nature of the scores. Were these scores and estimates to be something not bound in such a way (for instance the speed in which an individual could complete an assessment) examning the relationship between actual and estimate performance could yield more valid results. These graphical representations and analyses should be cautioned as they are not very meaningful to understanding their effects."
  },
  {
    "objectID": "posts/three-ways/index.html#set-up",
    "href": "posts/three-ways/index.html#set-up",
    "title": "Solving one task with three equivalent solutions: for, do.call, vapply",
    "section": "Set up",
    "text": "Set up\nLoading our tidyverse packages first. Because we will be grouping and summarising data in a data.frame, I really don’t feel like going through the trouble of using base solutions.\n\noptions(tidyverse.quiet = TRUE) # Silences messages\nlibrary(tidyverse)\n\nLet’s create some random data to represent our subjects, our doses, and an arbitrary time metric. We’re also going to make this harder by using the difftime.\nWe’re also going to grab some names using a function from the wakefield package which you can use to create fake data, appropriately named. [^note]\n^note: I’m not too savvy with this package yet but would like to experiment more for building dummy data sets.\n\nsubj <- sort(sample(wakefield::name(500), 1e4, TRUE))\ndose <- sample(seq.int(10, 80, 10), 1e4, TRUE)\ntime <- as.difftime(runif(1e4) * 100, units = \"days\")\n\ndf <- tibble(subj, dose, time)\ndf\n\n# A tibble: 10,000 × 3\n   subj     dose time          \n   <chr>   <dbl> <drtn>        \n 1 Aadhya     10 81.430584 days\n 2 Aadhya     10 80.324186 days\n 3 Aadhya     40 64.917350 days\n 4 Aadhya     80 17.631323 days\n 5 Aadhya     40  8.970711 days\n 6 Aadhya     20 96.518079 days\n 7 Aadhya     60 61.493537 days\n 8 Aadhya     20 29.073014 days\n 9 Aadhya     80  6.221622 days\n10 Aaditri    50 54.387006 days\n# … with 9,990 more rows\n\n\nFinding the total sum of time at each dose is easy:\n\ndf %>% \n  group_by(subj, dose) %>% \n  summarise(time = sum(time), .groups = \"drop\") %>% \n  pivot_wider(\n    names_from = \"dose\",\n    names_sort = TRUE,\n    values_from = \"time\"\n    )\n\n# A tibble: 500 × 9\n   subj    `10`            `20`           `30`     `40`  `50`  `60`  `70`  `80` \n   <chr>   <drtn>          <drtn>         <drtn>   <drt> <drt> <drt> <drt> <drt>\n 1 Aadhya  161.754770 days 125.59109 days        …  73.…     …  61.…     …  23.…\n 2 Aaditri  30.671230 days 172.20370 days        … 224.… 131.… 193.…  67.…  93.…\n 3 Aaleeya 174.678539 days 115.27412 days 159.963… 198.… 184.…  77.… 214.… 292.…\n 4 Aaleyah 183.860531 days  88.26322 days        …   9.… 261.… 259.… 214.… 100.…\n 5 Aamia    70.429895 days 149.78110 days 122.847…  40.…  32.…  74.… 116.… 161.…\n 6 Aarish  232.188970 days 234.42468 days  70.517…     …   7.… 111.… 153.… 118.…\n 7 Abdulah   8.890604 days  37.66665 days 130.246…     …     … 215.… 159.… 162.…\n 8 Abhinav 221.372920 days 108.87628 days 114.363… 280.… 136.… 100.…  97.…  95.…\n 9 Abhiram  73.235939 days 182.81309 days 295.836… 107.…  96.… 137.… 123.…  68.…\n10 Abyade  107.638230 days 596.18206 days 168.711… 146.…  61.…  34.… 281.… 149.…\n# … with 490 more rows\n\n\nOh, it looks like we have some missing values (see the first line where we are missing doses of 30, 50, and 70 for Aadhya.). tidyr::pivot_wider() allows us to fill in the missing values but like many tidyverse functions, we’ll have to be explicit with the type so as to not case any accidental issues.\n\ndifftime0 <- as.difftime(0, units = \"days\")\n\nres <- df %>% \n  group_by(subj, dose) %>% \n  summarise(time = sum(time), .groups = \"drop\") %>% \n  pivot_wider(\n    names_from = \"dose\",\n    names_sort = TRUE,\n    values_from = \"time\",\n    values_fill = difftime0\n  )\nres\n\n# A tibble: 500 × 9\n   subj    `10`            `20`           `30`     `40`  `50`  `60`  `70`  `80` \n   <chr>   <drtn>          <drtn>         <drtn>   <drt> <drt> <drt> <drt> <drt>\n 1 Aadhya  161.754770 days 125.59109 days   0.000…  73.…   0.…  61.…   0.…  23.…\n 2 Aaditri  30.671230 days 172.20370 days   0.000… 224.… 131.… 193.…  67.…  93.…\n 3 Aaleeya 174.678539 days 115.27412 days 159.963… 198.… 184.…  77.… 214.… 292.…\n 4 Aaleyah 183.860531 days  88.26322 days   0.000…   9.… 261.… 259.… 214.… 100.…\n 5 Aamia    70.429895 days 149.78110 days 122.847…  40.…  32.…  74.… 116.… 161.…\n 6 Aarish  232.188970 days 234.42468 days  70.517…   0.…   7.… 111.… 153.… 118.…\n 7 Abdulah   8.890604 days  37.66665 days 130.246…   0.…   0.… 215.… 159.… 162.…\n 8 Abhinav 221.372920 days 108.87628 days 114.363… 280.… 136.… 100.…  97.…  95.…\n 9 Abhiram  73.235939 days 182.81309 days 295.836… 107.…  96.… 137.… 123.…  68.…\n10 Abyade  107.638230 days 596.18206 days 168.711… 146.…  61.…  34.… 281.… 149.…\n# … with 490 more rows\n\n\nThis is much more troublesome as we have to create single length difftime vector.\nNow, we need to find the total length of time a subject was at each dose or greater.. This I wasn’t able to do with any built in functions (although I could have missed it). I’m not claiming this is the best function or anything, it’s not, but I have 3 different solutions and thought it was enough to spend my evening writing a blog post.\n\nSolution 1: The for Loop\nNever use a for loop, except if you have to. We’ll start with\n\nfoo1 <- function(x, y) {\n  out <- y\n  \n  for (i in seq_along(y)) {\n    out[i] <- sum(y[x >= x[i]])\n  }\n  \n  out\n}\n\n\n\nSolution 2: Combining lapply\nWhen having to play with dates before, I found that the way I could retain the date values and still use a function from the apply family was to stick with the lapply() and then use the do.call() function to apply the combine function over my list. lapply() retains the original classes and using the do.call(c, ...) method will turn my list into a vector without removing the structure of the output\n\nfoo2 <- function(x, y) {\n  out <- lapply(x, function(xx) sum(y[x >= xx]))\n  do.call(c, out)\n}\n\nLet’s see how this plays out. If we use other methods, we lose the difftime class, which is noticeable as we don’t get our message of Time differences in days before our results.\n\nx <- lapply(dose, function(xx) sum(time[dose >= xx])) %>% unlist() %>% head()\nprint_with_class_type(x)\n\nclass  numeric\ntypeof double\n[1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16\n\nx <- sapply(dose, function(xx) sum(time[dose >= xx])) %>% head()\nprint_with_class_type(x)\n\nclass  numeric\ntypeof double\n[1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16\n\nx <- foo2(dose, time) %>% head()\nprint_with_class_type(x)\n\nclass  difftime\ntypeof double\nTime differences in days[1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16\n\n\n\n\nSolution 3: vapply with subset assigning\nHere’s another neat little trick that with a good use case. We’re going to subset our out object (again, a copy of the y intput) and assign over it the result of our vapply. We’re also going to cheat and use the first position of y as our FUN.VALUE.\n\nfoo3 <- function(x, y) {\n  out <- y\n  out[] <- vapply(x, function(xx) sum(y[x >= xx]), y[1], USE.NAMES = FALSE)\n  out\n}\n\nLet’s take look just like before. vapply() will try to simplify the FUN.VALUE but as long as we use a single vector from the original input we can safely assign it back into our mock subset without worrying about losing our classes.\n\nx <- vapply(dose, function(xx) sum(time[dose >= xx]), time[1]) %>% head()\nprint_with_class_type(x)\n\nclass  numeric\ntypeof double\n[1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16\n\nx < -foo3(dose, time) %>% head()\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE\n\nprint_with_class_type(x)\n\nclass  numeric\ntypeof double\n[1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16"
  },
  {
    "objectID": "posts/three-ways/index.html#benchmarks",
    "href": "posts/three-ways/index.html#benchmarks",
    "title": "Solving one task with three equivalent solutions: for, do.call, vapply",
    "section": "Benchmarks",
    "text": "Benchmarks\nNow, all three of these solutions produce the same results and are fairly equivalent in human legibility. This means, for me at least, that the function which runs the fastest would be the result I keep. We’ll employ the bench package and eponymous name for our consideration.\nOf course, for this we’ll be running on the vectors first. We’ll also make certain that all of our outputs are the same with check = TRUE (default) to make sure we didn’t miss anything either.\n\nbench::mark(\n  `1` = foo1(dose, time),\n  `2` = foo2(dose, time),\n  `3` = foo3(dose, time),\n  check = TRUE\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n\n\n# A tibble: 3 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 1             3.89s    3.89s     0.257    2.96GB     14.9\n2 2             2.86s    2.86s     0.350    2.22GB     12.2\n3 3             2.81s    2.81s     0.356    2.21GB     12.8\n\n\nAnd, well, they all run about the same. That kind of just leaves us with the sinking feeling that all of this was futile and that for loops really aren’t that bad. In fact, the for loop solution may be the easiest to read and doesn’t use any tricks that someone reviewing your code may not understand at first.\nWe know we have some missing values in our data, so we’re going to use the the tidyr::complete() function to help with that.\n\nres\n\n# A tibble: 500 × 9\n   subj    `10`            `20`           `30`     `40`  `50`  `60`  `70`  `80` \n   <chr>   <drtn>          <drtn>         <drtn>   <drt> <drt> <drt> <drt> <drt>\n 1 Aadhya  161.754770 days 125.59109 days   0.000…  73.…   0.…  61.…   0.…  23.…\n 2 Aaditri  30.671230 days 172.20370 days   0.000… 224.… 131.… 193.…  67.…  93.…\n 3 Aaleeya 174.678539 days 115.27412 days 159.963… 198.… 184.…  77.… 214.… 292.…\n 4 Aaleyah 183.860531 days  88.26322 days   0.000…   9.… 261.… 259.… 214.… 100.…\n 5 Aamia    70.429895 days 149.78110 days 122.847…  40.…  32.…  74.… 116.… 161.…\n 6 Aarish  232.188970 days 234.42468 days  70.517…   0.…   7.… 111.… 153.… 118.…\n 7 Abdulah   8.890604 days  37.66665 days 130.246…   0.…   0.… 215.… 159.… 162.…\n 8 Abhinav 221.372920 days 108.87628 days 114.363… 280.… 136.… 100.…  97.…  95.…\n 9 Abhiram  73.235939 days 182.81309 days 295.836… 107.…  96.… 137.… 123.…  68.…\n10 Abyade  107.638230 days 596.18206 days 168.711… 146.…  61.…  34.… 281.… 149.…\n# … with 490 more rows\n\ndf %>% \n  complete(subj, dose, fill = list(time = difftime0)) %>% \n  group_by(subj, dose) %>% \n  summarise(time = sum(time), .groups = \"drop_last\") %>%\n  mutate(time = foo1(dose, time)) %>% \n  pivot_wider(\n    names_from = \"dose\",\n    names_sort = TRUE,\n    values_from = \"time\"\n  )\n\n# A tibble: 500 × 9\n# Groups:   subj [500]\n   subj    `10`           `20`           `30`      `40`  `50`  `60`  `70`  `80` \n   <chr>   <drtn>         <drtn>         <drtn>    <drt> <drt> <drt> <drt> <drt>\n 1 Aadhya   446.5804 days  284.8256 days  159.234… 159.…  85.…  85.…  23.…  23.…\n 2 Aaditri  913.9830 days  883.3118 days  711.108… 711.… 486.… 354.… 161.…  93.…\n 3 Aaleeya 1417.4559 days 1242.7773 days 1127.503… 967.… 769.… 584.… 507.… 292.…\n 4 Aaleyah 1117.0842 days  933.2237 days  844.960… 844.… 835.… 573.… 314.… 100.…\n 5 Aamia    769.4857 days  699.0558 days  549.274… 426.… 385.… 352.… 278.… 161.…\n 6 Aarish   927.1830 days  694.9940 days  460.569… 390.… 390.… 382.… 271.… 118.…\n 7 Abdulah  714.1323 days  705.2417 days  667.575… 537.… 537.… 537.… 321.… 162.…\n 8 Abhinav 1154.8195 days  933.4466 days  824.570… 710.… 429.… 292.… 192.…  95.…\n 9 Abhiram 1086.0283 days 1012.7924 days  829.979… 534.… 426.… 329.… 192.…  68.…\n10 Abyade  1545.7654 days 1438.1272 days  841.945… 673.… 526.… 465.… 431.… 149.…\n# … with 490 more rows\n\n\nAnd there you go. Three solutions, all the same.\nSometimes it’s useful to try to optimize code. Other times it’s just results in a blog post.\nAs long as your code is easy to read and not apparently slow, it’s probably fine."
  },
  {
    "objectID": "anthology.html",
    "href": "anthology.html",
    "title": "Anthology",
    "section": "",
    "text": "Below you may find links to some academic and professional work I have completed. The decision to include papers and presentations from class was not to artificially expand this section but was made to demonstrate the quality of work I aim to put forth.\nItalicized authors represent co-primary authorship."
  },
  {
    "objectID": "anthology.html#papers",
    "href": "anthology.html#papers",
    "title": "Anthology",
    "section": "Papers",
    "text": "Papers\n\n2021\nYounossi, Z. M., Stepanova, M., Taub, R. A., Barbone, J. M., & Harrison, S. A. (2021) Hepatic fat reduction due to resmetirom in patients with nonalcoholic steatohepatitis is associated with improvement of quality of life.\n\n\n2019\nBarbone, J. M. (2019) The effects of participant-selected background music on executive function task performance. Unpublished Masters thesis. West Chester University of Pennsylvania. https://digitalcommons.wcupa.edu/all_theses/42.\nSolomon, T. M., Barbone, J. M., Feaster, H. T., & Miller, D.S. (2019) Pilot validation of an electronic Alzheimer’s Disease Assessment Scale – cognitive subscale (eADAS-cog). The Journal of Prevention of Alzheimer’s Disease, 6, 237-241. doi: 10.14283/jpad.2019.27.\n\n\n2018\nBarbone, J. M. (2018, December) Replication in Social Psychology. Final paper for Advanced Social Psychology at West Chester University, PA, USA. [University paper]\nBarbone, J. M. (2018, May) Emotionally salient music with word list presentation. Final paper/research proposal for Cognitive-Affective Bases of Behavior at West Chester University, PA, USA. [University paper]\n\n\n2017\nBarbone, J. M. (2017, December) On music and language: Benefits of formal training for school-age children’s literacy and linguistic development. Final paper for Developmental Bases of Psychology at West Chester University, PA, USA. [University paper]\nBarbone, J. M. (2017, June) A scale for rating the need of control in social settings: Initial development and reliability. Final paper for Psychometrics at West Chester University, PA, USA. [University paper]\nIrani, F., Barbone, J. M., Beausoleil, J., & Gerald, L. B. (2017) Is asthma associated with impaired cognition? A meta-analytic review. Journal of Clinical and Experimental Neuropsychology, 39(10), 956-978. doi: 10.1080/13803395.2017.1288802."
  },
  {
    "objectID": "anthology.html#presentations",
    "href": "anthology.html#presentations",
    "title": "Anthology",
    "section": "Presentations",
    "text": "Presentations\n\n2018\nBarbone, J. M. (2018, December) Replication in Social Psychology. Slides presented for Advanced Social Psychology at West Chester University of Pennsylvania, PA, USA. [University presentation]\nWessels, A. M., Barbone, J. M., DiGregorio, D. T., Miller, D. S., Mullen, J. A., & Sims, J. R. (2018, October) Lanabecestat: Rater performance and error characteristics of efficacy assessments in the DAYBREAK-ALK study. Slides presented at Clinical Trials for Alzheimer’s Disease, Barcelona, Spain.\nBarbone, J. M. (2018, May) Music, Memory, and Emotion. Slides presented for Cognitive-Affective Basis of Behavior at West Chester Unviversity of Pennsylvania, PA, USA. [University presentation]\nBarbone, J. M. (2018, April) Efficacy of PECS in children with ASD and severe language impairment. Slides presented for Issues in Autism at West Chester University of Pennsylvania, PA, USA. [University presentation]. Part I. Part III.\nNote: the above presentation reported on results from a hypothetical study.\n\n\n2017\nBarbone, J. M. & Griffing, C. (2017, November) The effect of drug use and pregnancy. Slides presented for Developmental Bases of Psychology at West Chester University of Pennsylvania, PA, USA. [University presentation]\n\n\n2015\nIrani, F., Barbone, J. M., & Beausoleil, J. (2015, August) The impact of asthma on cognitive functioning. Slides presented at the American Psychological Association Convention, Toronto, Canada."
  },
  {
    "objectID": "anthology.html#posters",
    "href": "anthology.html#posters",
    "title": "Anthology",
    "section": "Posters",
    "text": "Posters\n\n2020\nHarrison, S. A., Taub, R. A. Karsdal, M. A., Franc, J. Bashir, M. R., Barbone, J. M., Neff, G., Gunn, N. T., & Moussa, S. (2020, November). Algorithm for predicting advanced NASH fibrosis on screening biopsy in resmetirom phase 3 MAESTRO-NASH clinical trial. Poster presented at the AASLD Liver Meeting Digit Experience conference.\nYounossi, Z. M., Stepanova, M., Taub, R. A., Barbone, J. M., Moussa, S., Harrison, S. A. (2020, November) Improvement of health-related quality of life is associated with improvement of fat fraction by MRI-PDFF in patients with nonalcoholic steatohepatitis treated with resmetirom. Poster presented at the AASLD Liver Meeting Digit Experience conference.\nRoy, M, Brown, J., Hong, B, Benecke, R., Chen-Tackett, Z., Zhou, W., Barbone, J. M., Feaster, H. T., & Sachs G. (2020, September) eCOA prompted MADRS interview: Balancing through assessment and efficiency. Poster presented at ISCTM.\nCrittenden, K., Machizawa, S., Feaster, H. T., Barbone, J. M., Hong, B., Verma, P., and Zhou, W. (2021, April) Cross-Cultural differences in PANSS item ratings: Comparisons of six geo-cultural regions. Poster presented at the 2020 Schizophrenia International Research Society (SIRS) Conference, Florence, Italy.\nHarrison, S. A., Taub, R. A, Barbone, J. M., Franc, J., & Karsdal, M. A. (2020, March) Resmetirom, a beta selective thyroid hormone receptor agonist, reduces net collagen III deposition in nonalcoholic Steatohepatitis. Poster to have been presented at American Association for the Study of Liver Diseases (AASLD) Emerging Topic Conference 2020, Nuclear Receptors in Nonalcoholic Fatty Liver Diseases (conference cancelled).\n\n\n2019\nFeaster, H. T., Barbone, J. M., Miller, D. S., & Solomon, T. M. (2019, July) Rater remediation on the ADAS-cog leads to longitudinal improvement in clinical trial data quality. Poster to be presented at the Alzheimer’s Association International Conference (AAIC), Los Angeles, C.A., USA.\nKaras, S. M, Barbone, J. M., Hong, B., Solomon, T. M., & Feaster, H. T. (2019, July) Impact of data quality programs on MMSE inclusion criteria in Alzheimer’s disease clinical research trials. Poster presented at the Alzheimer’s Association International Conference (AAIC), Los Angeles, C.A., USA.\nSolomon, T. M., Feaster, H. T., Karas, S. M., Garcia-Valdecasas Colell, M., Barbone, J. M.**, DiGregorio, D. T., DeBonis, D., & Miller, D. S. (2019, July) The evolution of technology in data quality programs for Alzheimer’s disease clinical trials: What have we learned and where are we going?. Poster presented at the Alzheimer’s Association International Conference (AAIC), Los Angeles, C.A., USA.\nOlt, J., Khan, R., Feaster, H. T., Solomon, T. M., Barbone, J. M., Platko, J. V., Sanderson, B., Bodart, S., Garner, K. S., & Byrom, B. (2019, May) clinician training reduces variability in clinician reported outcomes of skin surface symptoms in a dermatology study. Poster presented at ISPOR 2019, New Orleans, L.A., USA.\nBarbone, J. M. & Shivde, G. (2019, May) The effects of participant selected background music on executive task performance. Poster presented at the Association for Psychological Science, Washington, D.C., USA.\n\n\n2018\nBarbone, J. M. (2018, November) Trends in self-reported music use while studying: Implications for research. Poster presented at Research & Creative Activity Day at West Chester University, West Chester, PA, USA.\nBarbone, J. M., Solomon, T. M., Feaster, H. T., Garcia-Valdecasas Colell, M., & Miller, D. S. (2018, September) MMSE screening data quality for Alzheimer’s disease studies across countries Poster presented at Clinical Trials for Alzheimer’s Disease, Barcelona, Spain.\nGarcia-Valdecasas Colell, M., Barbone, J. M., Solomon, T. M., Feaster, H. T., Tott, N., & Miller, D. S. (2018, September) Identifying the impact of rater change on Mini-mental State Examination and Clinical Dementia Rating scale data in multi-national Alzheimer’s disease clinical trials. Poster presented at Clinical Trials for Alzheimer’s Disease, Barcelona, Spain. Link pending.\nSolomon, T. M., Barbone, J. M., Feaster, H. T., Garcia-Valdecases Colell, M., DiGregorio, D. T., Karas, S. M., Maddock, M. R., & Miller, D. S. (2018, September). The presence of identical scoring on the MMSE and ADCS-ADL in Alzheimer’s disease clinical trials using enhanced electronic Clinical Outcome Assessment (eCOA) devices. Poster presented at Clinical Trials for Alzheimer’s Disease, Barcelona, Spain.\nKaras, S. M., Barbone, J. M., DeBonis, D., & Solomon, T. M. (2018, July). The use of statistical modeling to complement data quality programs. Poster presented at the Alzheimer’s Disease Association International Conference, Chicago, IL, USA.\nFeaster, H. T., Barbone, J. M., Garcia-Valdecasas Colell, M., Miller, D. S., & Solomon, T. M. (2018, July). Electronic ADAS-Cog (eADAS-Cog) data quality: How do countries compare? Poster presented at the Alzheimer’s Disease Association International Conference, Chicago, IL, USA.\nSolomon, T. M., Barbone, J. M., Miller, D. S., & Feaster, H. T. (2018, July) Pilot validation of an electronic Alzheimer’s Disease Assessment Scale – Cognitive subscale (eADAS-Cog). Poster presented at the Alzheimer’s Disease Association International Conference, Chicago, IL, USA.\nSolomon, T. M., Karas, S. M., Barbone, J. M., DiGregorio, D. T., Miller, D. S., & Feaster, H. T. (2018, July) Analysis of the rates and types of errors on enhanced eCOA version of the Alzheimer’s Disease Assessment Scale – Cognitive Subscale and Mini-Mental State Examination used in clinical trials of dementia. Poster presented at the Alzheimer’s Association International Conference, Chicago, IL, USA.\nSeichepine, D. R., Solomon, T. M., Jacobs, E. B., Barbone, J. M., DiGregorio, D. T., & Miller, D. S. (2018, February) Comparison between flat and enhanced eCOA of MMSE Attention and Calculation in clinical trials of Alzheimer’s disease. Poster presented at the International Society for CNS Clinical Trials and Methodology Autumn 14th Annual Scientific Meeting, Washington, DC, USA.\n\n\n2017\nEnger, B. & Barbone, J. M. (2017, November) Distribution of performance and incorrect ratings in qualification video scoring. Poster presented at CNS Summit, Boca Raton, FL, USA.\nSolomon, T. M, Feaster, H .T., Barbone, J. M., & Miller, D. S. (2017, November) Utilizing audio review to improve ADCS-ADL data quality. Poster presented at Clinical Trials on Alzheimer’s Disease, Boston, MA, USA.\nSolomon, T. M., Barbone, J. M., Karas, S. M, & Feaster, H. T. (2017, November) Longitudinal impact of audio review on data quality. Poster presented at the Clinical Trials on Alzheimer’s Disease, Boston, MA, USA.\nFeaster, H. T., Solomon, T. M., Abi-Saab, D., Vogt, A., Barbone, J. M., Harrison, J., & Miller, D. S. (2017, July) The impact of electronic clinical outcome assessments (eCOA) on Alzheimer’s disease clinical trial data quality. Poster to be presented at the Alzheimer’s Association International Conference, London, England, UK.\nFeaster, H. T., Solomon, T. M., Barbone, J. M., & Miller, D. S. (2017, May) Using iterative user experience design to improve electronic clinical outcomes assessment data quality. Poster presented at the International Society for Pharmacoeconomics and Outcomes Research, Boston, MA, USA.\n\n\n2015\nBarbone, J. M., Saxena, S. & Irani, F. (2015, August) Does asthma effect neuropsychological performance? A meta-analysis. Poster presented at the American Psychological Association Convention Program, Toronto, Canada.\nMulligan, R. D., Barbone, J. M., Saxena, S., Carey, D., & Clappsy, J. (2015, April) Asthma cognition data collection. Poster presented at West Chester University’s Psychology Research Day, West Chester, PA, USA.\nBarbone, J. M., Saxena, S., & Irani, F. (2015, March) Neuropsychological performance in individuals with asthma: two meta-analyses. Poster presented at the Eastern Psychological Association Conference, Philadelphia, PA, USA."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "::: \nCurriculum vitae :::"
  }
]