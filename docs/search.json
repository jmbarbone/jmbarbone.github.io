[
  {
    "objectID": "anthology.html",
    "href": "anthology.html",
    "title": "Anthology",
    "section": "",
    "text": "Below you may find links to some academic and professional work I have completed. The decision to include papers and presentations from class was not to artificially expand this section but was made to demonstrate the quality of work I aim to put forth.\nItalicized authors represent co-primary authorship."
  },
  {
    "objectID": "anthology.html#papers",
    "href": "anthology.html#papers",
    "title": "Anthology",
    "section": "Papers",
    "text": "Papers\n\n2021\nYounossi, Z. M., Stepanova, M., Taub, R. A., Barbone, J. M., & Harrison, S. A. (2021) Hepatic fat reduction due to resmetirom in patients with nonalcoholic steatohepatitis is associated with improvement of quality of life.\n\n\n2019\nBarbone, J. M. (2019) The effects of participant-selected background music on executive function task performance. Unpublished Masters thesis. West Chester University of Pennsylvania. https://digitalcommons.wcupa.edu/all_theses/42.\nSolomon, T. M., Barbone, J. M., Feaster, H. T., & Miller, D.S. (2019) Pilot validation of an electronic Alzheimer’s Disease Assessment Scale – cognitive subscale (eADAS-cog). The Journal of Prevention of Alzheimer’s Disease, 6, 237-241. doi: 10.14283/jpad.2019.27.\n\n\n2018\nBarbone, J. M. (2018, December) Replication in Social Psychology. Final paper for Advanced Social Psychology at West Chester University, PA, USA. [University paper]\nBarbone, J. M. (2018, May) Emotionally salient music with word list presentation. Final paper/research proposal for Cognitive-Affective Bases of Behavior at West Chester University, PA, USA. [University paper]\n\n\n2017\nBarbone, J. M. (2017, December) On music and language: Benefits of formal training for school-age children’s literacy and linguistic development. Final paper for Developmental Bases of Psychology at West Chester University, PA, USA. [University paper]\nBarbone, J. M. (2017, June) A scale for rating the need of control in social settings: Initial development and reliability. Final paper for Psychometrics at West Chester University, PA, USA. [University paper]\nIrani, F., Barbone, J. M., Beausoleil, J., & Gerald, L. B. (2017) Is asthma associated with impaired cognition? A meta-analytic review. Journal of Clinical and Experimental Neuropsychology, 39(10), 956-978. doi: 10.1080/13803395.2017.1288802."
  },
  {
    "objectID": "anthology.html#presentations",
    "href": "anthology.html#presentations",
    "title": "Anthology",
    "section": "Presentations",
    "text": "Presentations\n\n2022\n\n\nEdgar, C. J., Maruff, P., Harrison, J. E, Barbone, J. M., Leventhal, R., & Alam, J. J. (April, 2022) Validity and reliability of a composite cognitive outcome measure for clinical trials in dementia with Lewy bodies. Slides and presented at the 16th International Conference on Alzheimer’s & Parkinson’s Diseases, Barcelona, Spain.\n\n\n2018\nBarbone, J. M. (2018, December) Replication in Social Psychology. Slides presented for Advanced Social Psychology at West Chester University of Pennsylvania, PA, USA. [University presentation]\nWessels, A. M., Barbone, J. M., DiGregorio, D. T., Miller, D. S., Mullen, J. A., & Sims, J. R. (2018, October) Lanabecestat: Rater performance and error characteristics of efficacy assessments in the DAYBREAK-ALK study. Slides presented at Clinical Trials for Alzheimer’s Disease, Barcelona, Spain.\nBarbone, J. M. (2018, May) Music, Memory, and Emotion. Slides presented for Cognitive-Affective Basis of Behavior at West Chester Unviversity of Pennsylvania, PA, USA. [University presentation]\nBarbone, J. M. (2018, April) Efficacy of PECS in children with ASD and severe language impairment. Slides presented for Issues in Autism at West Chester University of Pennsylvania, PA, USA. [University presentation]. Part I. Part III.\nNote: the above presentation reported on results from a hypothetical study.\n\n\n2017\nBarbone, J. M. & Griffing, C. (2017, November) The effect of drug use and pregnancy. Slides presented for Developmental Bases of Psychology at West Chester University of Pennsylvania, PA, USA. [University presentation]\n\n\n2015\nIrani, F., Barbone, J. M., & Beausoleil, J. (2015, August) The impact of asthma on cognitive functioning. Slides presented at the American Psychological Association Convention, Toronto, Canada."
  },
  {
    "objectID": "anthology.html#posters",
    "href": "anthology.html#posters",
    "title": "Anthology",
    "section": "Posters",
    "text": "Posters\n\n2022\nWacker, S. & Barbone, J. M. (April, 2022) Rater perspectives on Applied Training of cognitive clinical outcome assessments, delivered by neuropsychology experts. Poster presented at the 16th International Conference on Alzheimer’s & Parkinson’s Diseases, Barcelona, Spain.\n\n\n2020\nHarrison, S. A., Taub, R. A. Karsdal, M. A., Franc, J. Bashir, M. R., Barbone, J. M., Neff, G., Gunn, N. T., & Moussa, S. (2020, November). Algorithm for predicting advanced NASH fibrosis on screening biopsy in resmetirom phase 3 MAESTRO-NASH clinical trial. Poster presented at the AASLD Liver Meeting Digit Experience conference.\nYounossi, Z. M., Stepanova, M., Taub, R. A., Barbone, J. M., Moussa, S., Harrison, S. A. (2020, November) Improvement of health-related quality of life is associated with improvement of fat fraction by MRI-PDFF in patients with nonalcoholic steatohepatitis treated with resmetirom. Poster presented at the AASLD Liver Meeting Digit Experience conference.\nRoy, M, Brown, J., Hong, B, Benecke, R., Chen-Tackett, Z., Zhou, W., Barbone, J. M., Feaster, H. T., & Sachs G. (2020, September) eCOA prompted MADRS interview: Balancing through assessment and efficiency. Poster presented at ISCTM.\nCrittenden, K., Machizawa, S., Feaster, H. T., Barbone, J. M., Hong, B., Verma, P., and Zhou, W. (2021, April) Cross-Cultural differences in PANSS item ratings: Comparisons of six geo-cultural regions. Poster presented at the 2020 Schizophrenia International Research Society (SIRS) Conference, Florence, Italy.\nHarrison, S. A., Taub, R. A, Barbone, J. M., Franc, J., & Karsdal, M. A. (2020, March) Resmetirom, a beta selective thyroid hormone receptor agonist, reduces net collagen III deposition in nonalcoholic Steatohepatitis. Poster to have been presented at American Association for the Study of Liver Diseases (AASLD) Emerging Topic Conference 2020, Nuclear Receptors in Nonalcoholic Fatty Liver Diseases (conference cancelled).\n\n\n2019\nFeaster, H. T., Barbone, J. M., Miller, D. S., & Solomon, T. M. (2019, July) Rater remediation on the ADAS-cog leads to longitudinal improvement in clinical trial data quality. Poster to be presented at the Alzheimer’s Association International Conference (AAIC), Los Angeles, C.A., USA.\nKaras, S. M, Barbone, J. M., Hong, B., Solomon, T. M., & Feaster, H. T. (2019, July) Impact of data quality programs on MMSE inclusion criteria in Alzheimer’s disease clinical research trials. Poster presented at the Alzheimer’s Association International Conference (AAIC), Los Angeles, C.A., USA.\nSolomon, T. M., Feaster, H. T., Karas, S. M., Garcia-Valdecasas Colell, M., Barbone, J. M.**, DiGregorio, D. T., DeBonis, D., & Miller, D. S. (2019, July) The evolution of technology in data quality programs for Alzheimer’s disease clinical trials: What have we learned and where are we going?. Poster presented at the Alzheimer’s Association International Conference (AAIC), Los Angeles, C.A., USA.\nOlt, J., Khan, R., Feaster, H. T., Solomon, T. M., Barbone, J. M., Platko, J. V., Sanderson, B., Bodart, S., Garner, K. S., & Byrom, B. (2019, May) clinician training reduces variability in clinician reported outcomes of skin surface symptoms in a dermatology study. Poster presented at ISPOR 2019, New Orleans, L.A., USA.\nBarbone, J. M. & Shivde, G. (2019, May) The effects of participant selected background music on executive task performance. Poster presented at the Association for Psychological Science, Washington, D.C., USA.\n\n\n2018\nBarbone, J. M. (2018, November) Trends in self-reported music use while studying: Implications for research. Poster presented at Research & Creative Activity Day at West Chester University, West Chester, PA, USA.\nBarbone, J. M., Solomon, T. M., Feaster, H. T., Garcia-Valdecasas Colell, M., & Miller, D. S. (2018, September) MMSE screening data quality for Alzheimer’s disease studies across countries Poster presented at Clinical Trials for Alzheimer’s Disease, Barcelona, Spain.\nGarcia-Valdecasas Colell, M., Barbone, J. M., Solomon, T. M., Feaster, H. T., Tott, N., & Miller, D. S. (2018, September) Identifying the impact of rater change on Mini-mental State Examination and Clinical Dementia Rating scale data in multi-national Alzheimer’s disease clinical trials. Poster presented at Clinical Trials for Alzheimer’s Disease, Barcelona, Spain. Link pending.\nSolomon, T. M., Barbone, J. M., Feaster, H. T., Garcia-Valdecases Colell, M., DiGregorio, D. T., Karas, S. M., Maddock, M. R., & Miller, D. S. (2018, September). The presence of identical scoring on the MMSE and ADCS-ADL in Alzheimer’s disease clinical trials using enhanced electronic Clinical Outcome Assessment (eCOA) devices. Poster presented at Clinical Trials for Alzheimer’s Disease, Barcelona, Spain.\nKaras, S. M., Barbone, J. M., DeBonis, D., & Solomon, T. M. (2018, July). The use of statistical modeling to complement data quality programs. Poster presented at the Alzheimer’s Disease Association International Conference, Chicago, IL, USA.\nFeaster, H. T., Barbone, J. M., Garcia-Valdecasas Colell, M., Miller, D. S., & Solomon, T. M. (2018, July). Electronic ADAS-Cog (eADAS-Cog) data quality: How do countries compare? Poster presented at the Alzheimer’s Disease Association International Conference, Chicago, IL, USA.\nSolomon, T. M., Barbone, J. M., Miller, D. S., & Feaster, H. T. (2018, July) Pilot validation of an electronic Alzheimer’s Disease Assessment Scale – Cognitive subscale (eADAS-Cog). Poster presented at the Alzheimer’s Disease Association International Conference, Chicago, IL, USA.\nSolomon, T. M., Karas, S. M., Barbone, J. M., DiGregorio, D. T., Miller, D. S., & Feaster, H. T. (2018, July) Analysis of the rates and types of errors on enhanced eCOA version of the Alzheimer’s Disease Assessment Scale – Cognitive Subscale and Mini-Mental State Examination used in clinical trials of dementia. Poster presented at the Alzheimer’s Association International Conference, Chicago, IL, USA.\nSeichepine, D. R., Solomon, T. M., Jacobs, E. B., Barbone, J. M., DiGregorio, D. T., & Miller, D. S. (2018, February) Comparison between flat and enhanced eCOA of MMSE Attention and Calculation in clinical trials of Alzheimer’s disease. Poster presented at the International Society for CNS Clinical Trials and Methodology Autumn 14th Annual Scientific Meeting, Washington, DC, USA.\n\n\n2017\nEnger, B. & Barbone, J. M. (2017, November) Distribution of performance and incorrect ratings in qualification video scoring. Poster presented at CNS Summit, Boca Raton, FL, USA.\nSolomon, T. M, Feaster, H .T., Barbone, J. M., & Miller, D. S. (2017, November) Utilizing audio review to improve ADCS-ADL data quality. Poster presented at Clinical Trials on Alzheimer’s Disease, Boston, MA, USA.\nSolomon, T. M., Barbone, J. M., Karas, S. M, & Feaster, H. T. (2017, November) Longitudinal impact of audio review on data quality. Poster presented at the Clinical Trials on Alzheimer’s Disease, Boston, MA, USA.\nFeaster, H. T., Solomon, T. M., Abi-Saab, D., Vogt, A., Barbone, J. M., Harrison, J., & Miller, D. S. (2017, July) The impact of electronic clinical outcome assessments (eCOA) on Alzheimer’s disease clinical trial data quality. Poster to be presented at the Alzheimer’s Association International Conference, London, England, UK.\nFeaster, H. T., Solomon, T. M., Barbone, J. M., & Miller, D. S. (2017, May) Using iterative user experience design to improve electronic clinical outcomes assessment data quality. Poster presented at the International Society for Pharmacoeconomics and Outcomes Research, Boston, MA, USA.\n\n\n2015\nBarbone, J. M., Saxena, S. & Irani, F. (2015, August) Does asthma effect neuropsychological performance? A meta-analysis. Poster presented at the American Psychological Association Convention Program, Toronto, Canada.\nMulligan, R. D., Barbone, J. M., Saxena, S., Carey, D., & Clappsy, J. (2015, April) Asthma cognition data collection. Poster presented at West Chester University’s Psychology Research Day, West Chester, PA, USA.\nBarbone, J. M., Saxena, S., & Irani, F. (2015, March) Neuropsychological performance in individuals with asthma: two meta-analyses. Poster presented at the Eastern Psychological Association Conference, Philadelphia, PA, USA."
  },
  {
    "objectID": "posts/quarto-update/index.html",
    "href": "posts/quarto-update/index.html",
    "title": "Site update",
    "section": "",
    "text": "I finally got around to updating this out-of-date site with some new information in my About and Anthology. Even more, I nuked the whole thing and remade the website as a quarto-blog.\nI’ve started using {quarto} for work reports and have been impressed with some of the functionality. While I’ve spend years with {knitr} (thanks Yihui Xie), {quarto} feels like a refreshing re-do and feels a little easier to use in some cases. One particularly good feature is the article layout, which feels particularly easy to use. There’s an example of this in the About page where a large graphic is nicely displayed without too much effort. This has been very help.\nWhile {quarto} feels nice, it still feels a little fresh and it seems some functionality might be pending. I’ve already had to write a quick wrapper for an easier replacement to rmarkdown::render()."
  },
  {
    "objectID": "posts/three-ways/index.html",
    "href": "posts/three-ways/index.html",
    "title": "Solving one task with three equivalent solutions",
    "section": "",
    "text": "I can spend a lot of time trying in futility to optimize custom functions and code. I’ll take the time to rewrite the same thing in two or more ways and run some basic benchmarks for performance differences. Most of the time I’m just exploring the simplest possible way to perform an action and what would be the most generalizable solution that could extend to other problems. I ran into an issue where I had to use a quick function to find the total amount of time a subject in a clinical trial was on a specific dose. However, subjects had doses adjusted through the study for optimization. Further, we also needed to count the dosing based on the lowest dose provided.\nSo, let’s explore three different ways to solve this problem and why all three are valid and just as efficient."
  },
  {
    "objectID": "posts/three-ways/index.html#set-up",
    "href": "posts/three-ways/index.html#set-up",
    "title": "Solving one task with three equivalent solutions",
    "section": "Set up",
    "text": "Set up\nLoading our tidyverse packages first. Because we will be grouping and summarising data in a data.frame, I really don’t feel like going through the trouble of using base solutions.\n\n\nCode\noptions(tidyverse.quiet = TRUE) # Silences messages\nlibrary(tidyverse)\n\n\nLet’s create some random data to represent our subjects, our doses, and an arbitrary time metric. We’re also going to make this harder by using the difftime.\nWe’re also going to grab some names using a function from the wakefield package which you can use to create fake data, appropriately named. 1\n1 I’m not too savvy with this package yet but would like to experiment more for building dummy data sets.\n\nCode\nsubj &lt;- sort(sample(wakefield::name(500), 1e4, TRUE))\ndose &lt;- sample(seq.int(10, 80, 10), 1e4, TRUE)\ntime &lt;- as.difftime(runif(1e4) * 100, units = \"days\")\n\ndf &lt;- tibble(subj, dose, time)\ndf\n#&gt; # A tibble: 10,000 × 3\n#&gt;    subj     dose time          \n#&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;drtn&gt;        \n#&gt;  1 Aadhya     10 81.430584 days\n#&gt;  2 Aadhya     10 80.324186 days\n#&gt;  3 Aadhya     40 64.917350 days\n#&gt;  4 Aadhya     80 17.631323 days\n#&gt;  5 Aadhya     40  8.970711 days\n#&gt;  6 Aadhya     20 96.518079 days\n#&gt;  7 Aadhya     60 61.493537 days\n#&gt;  8 Aadhya     20 29.073014 days\n#&gt;  9 Aadhya     80  6.221622 days\n#&gt; 10 Aaditri    50 54.387006 days\n#&gt; # … with 9,990 more rows\n\n\nFinding the total sum of time at each dose is easy:\n\n\nCode\ndf %&gt;% \n  group_by(subj, dose) %&gt;% \n  summarise(time = sum(time), .groups = \"drop\") %&gt;% \n  pivot_wider(\n    names_from = \"dose\",\n    names_sort = TRUE,\n    values_from = \"time\"\n    )\n#&gt; # A tibble: 500 × 9\n#&gt;    subj    `10`            `20`           `30`     `40`  `50`  `60`  `70`  `80` \n#&gt;    &lt;chr&gt;   &lt;drtn&gt;          &lt;drtn&gt;         &lt;drtn&gt;   &lt;drt&gt; &lt;drt&gt; &lt;drt&gt; &lt;drt&gt; &lt;drt&gt;\n#&gt;  1 Aadhya  161.754770 days 125.59109 days        …  73.…     …  61.…     …  23.…\n#&gt;  2 Aaditri  30.671230 days 172.20370 days        … 224.… 131.… 193.…  67.…  93.…\n#&gt;  3 Aaleeya 174.678539 days 115.27412 days 159.963… 198.… 184.…  77.… 214.… 292.…\n#&gt;  4 Aaleyah 183.860531 days  88.26322 days        …   9.… 261.… 259.… 214.… 100.…\n#&gt;  5 Aamia    70.429895 days 149.78110 days 122.847…  40.…  32.…  74.… 116.… 161.…\n#&gt;  6 Aarish  232.188970 days 234.42468 days  70.517…     …   7.… 111.… 153.… 118.…\n#&gt;  7 Abdulah   8.890604 days  37.66665 days 130.246…     …     … 215.… 159.… 162.…\n#&gt;  8 Abhinav 221.372920 days 108.87628 days 114.363… 280.… 136.… 100.…  97.…  95.…\n#&gt;  9 Abhiram  73.235939 days 182.81309 days 295.836… 107.…  96.… 137.… 123.…  68.…\n#&gt; 10 Abyade  107.638230 days 596.18206 days 168.711… 146.…  61.…  34.… 281.… 149.…\n#&gt; # … with 490 more rows\n\n\nOh, it looks like we have some missing values (see the first line where we are missing doses of 30, 50, and 70 for Aadhya.). tidyr::pivot_wider() allows us to fill in the missing values but like many tidyverse functions, we’ll have to be explicit with the type so as to not case any accidental issues.\n\n\nCode\ndifftime0 &lt;- as.difftime(0, units = \"days\")\n\nres &lt;- df %&gt;% \n  group_by(subj, dose) %&gt;% \n  summarise(time = sum(time), .groups = \"drop\") %&gt;% \n  pivot_wider(\n    names_from = \"dose\",\n    names_sort = TRUE,\n    values_from = \"time\",\n    values_fill = difftime0\n  )\nres\n#&gt; # A tibble: 500 × 9\n#&gt;    subj    `10`            `20`           `30`     `40`  `50`  `60`  `70`  `80` \n#&gt;    &lt;chr&gt;   &lt;drtn&gt;          &lt;drtn&gt;         &lt;drtn&gt;   &lt;drt&gt; &lt;drt&gt; &lt;drt&gt; &lt;drt&gt; &lt;drt&gt;\n#&gt;  1 Aadhya  161.754770 days 125.59109 days   0.000…  73.…   0.…  61.…   0.…  23.…\n#&gt;  2 Aaditri  30.671230 days 172.20370 days   0.000… 224.… 131.… 193.…  67.…  93.…\n#&gt;  3 Aaleeya 174.678539 days 115.27412 days 159.963… 198.… 184.…  77.… 214.… 292.…\n#&gt;  4 Aaleyah 183.860531 days  88.26322 days   0.000…   9.… 261.… 259.… 214.… 100.…\n#&gt;  5 Aamia    70.429895 days 149.78110 days 122.847…  40.…  32.…  74.… 116.… 161.…\n#&gt;  6 Aarish  232.188970 days 234.42468 days  70.517…   0.…   7.… 111.… 153.… 118.…\n#&gt;  7 Abdulah   8.890604 days  37.66665 days 130.246…   0.…   0.… 215.… 159.… 162.…\n#&gt;  8 Abhinav 221.372920 days 108.87628 days 114.363… 280.… 136.… 100.…  97.…  95.…\n#&gt;  9 Abhiram  73.235939 days 182.81309 days 295.836… 107.…  96.… 137.… 123.…  68.…\n#&gt; 10 Abyade  107.638230 days 596.18206 days 168.711… 146.…  61.…  34.… 281.… 149.…\n#&gt; # … with 490 more rows\n\n\nThis is much more troublesome as we have to create single length difftime vector.\nNow, we need to find the total length of time a subject was at each dose or greater.. This I wasn’t able to do with any built in functions (although I could have missed it). I’m not claiming this is the best function or anything, it’s not, but I have 3 different solutions and thought it was enough to spend my evening writing a blog post.\n\nSolution 1: The for Loop\nNever use a for loop, except if you have to. We’ll start with\n\n\nCode\nfoo1 &lt;- function(x, y) {\n  out &lt;- y\n  \n  for (i in seq_along(y)) {\n    out[i] &lt;- sum(y[x &gt;= x[i]])\n  }\n  \n  out\n}\n\n\n\n\nSolution 2: Combining lapply\nWhen having to play with dates before, I found that the way I could retain the date values and still use a function from the apply family was to stick with the lapply() and then use the do.call() function to apply the combine function over my list. lapply() retains the original classes and using the do.call(c, ...) method will turn my list into a vector without removing the structure of the output\n\n\nCode\nfoo2 &lt;- function(x, y) {\n  out &lt;- lapply(x, function(xx) sum(y[x &gt;= xx]))\n  do.call(c, out)\n}\n\n\nLet’s see how this plays out. If we use other methods, we lose the difftime class, which is noticeable as we don’t get our message of Time differences in days before our results.\n\n\nCode\nx &lt;- lapply(dose, function(xx) sum(time[dose &gt;= xx])) %&gt;% unlist() %&gt;% head()\nprint_with_class_type(x)\n#&gt; class  numeric\n#&gt; typeof double\n#&gt; [1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16\n\nx &lt;- sapply(dose, function(xx) sum(time[dose &gt;= xx])) %&gt;% head()\nprint_with_class_type(x)\n#&gt; class  numeric\n#&gt; typeof double\n#&gt; [1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16\n\nx &lt;- foo2(dose, time) %&gt;% head()\nprint_with_class_type(x)\n#&gt; class  difftime\n#&gt; typeof double\n#&gt; Time differences in days[1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16\n\n\n\n\nSolution 3: vapply with subset assigning\nHere’s another neat little trick that with a good use case. We’re going to subset our out object (again, a copy of the y intput) and assign over it the result of our vapply. We’re also going to cheat and use the first position of y as our FUN.VALUE.\n\n\nCode\nfoo3 &lt;- function(x, y) {\n  out &lt;- y\n  out[] &lt;- vapply(x, function(xx) sum(y[x &gt;= xx]), y[1], USE.NAMES = FALSE)\n  out\n}\n\n\nLet’s take look just like before. vapply() will try to simplify the FUN.VALUE but as long as we use a single vector from the original input we can safely assign it back into our mock subset without worrying about losing our classes.\n\n\nCode\nx &lt;- vapply(dose, function(xx) sum(time[dose &gt;= xx]), time[1]) %&gt;% head()\nprint_with_class_type(x)\n#&gt; class  numeric\n#&gt; typeof double\n#&gt; [1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16\n\nx &lt; -foo3(dose, time) %&gt;% head()\n#&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE\nprint_with_class_type(x)\n#&gt; class  numeric\n#&gt; typeof double\n#&gt; [1] 501497.92 501497.92 312102.52  61765.11 312102.52 437797.16"
  },
  {
    "objectID": "posts/three-ways/index.html#benchmarks",
    "href": "posts/three-ways/index.html#benchmarks",
    "title": "Solving one task with three equivalent solutions",
    "section": "Benchmarks",
    "text": "Benchmarks\nNow, all three of these solutions produce the same results and are fairly equivalent in human legibility. This means, for me at least, that the function which runs the fastest would be the result I keep. We’ll employ the bench package and eponymous name for our consideration.\nOf course, for this we’ll be running on the vectors first. We’ll also make certain that all of our outputs are the same with check = TRUE (default) to make sure we didn’t miss anything either.\n\n\nCode\nbench::mark(\n  `1` = foo1(dose, time),\n  `2` = foo2(dose, time),\n  `3` = foo3(dose, time),\n  check = TRUE\n)\n#&gt; Warning: Some expressions had a GC in every iteration; so filtering is\n#&gt; disabled.\n#&gt; # A tibble: 3 × 6\n#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 1             3.01s    3.01s     0.332    2.96GB     35.2\n#&gt; 2 2             1.84s    1.84s     0.545    2.22GB     30.0\n#&gt; 3 3             1.91s    1.91s     0.524    2.21GB     24.1\n\n\nAnd, well, they all run about the same. That kind of just leaves us with the sinking feeling that all of this was futile and that for loops really aren’t that bad. In fact, the for loop solution may be the easiest to read and doesn’t use any tricks that someone reviewing your code may not understand at first.\nWe know we have some missing values in our data, so we’re going to use the the tidyr::complete() function to help with that.\n\n\nCode\nres\n#&gt; # A tibble: 500 × 9\n#&gt;    subj    `10`            `20`           `30`     `40`  `50`  `60`  `70`  `80` \n#&gt;    &lt;chr&gt;   &lt;drtn&gt;          &lt;drtn&gt;         &lt;drtn&gt;   &lt;drt&gt; &lt;drt&gt; &lt;drt&gt; &lt;drt&gt; &lt;drt&gt;\n#&gt;  1 Aadhya  161.754770 days 125.59109 days   0.000…  73.…   0.…  61.…   0.…  23.…\n#&gt;  2 Aaditri  30.671230 days 172.20370 days   0.000… 224.… 131.… 193.…  67.…  93.…\n#&gt;  3 Aaleeya 174.678539 days 115.27412 days 159.963… 198.… 184.…  77.… 214.… 292.…\n#&gt;  4 Aaleyah 183.860531 days  88.26322 days   0.000…   9.… 261.… 259.… 214.… 100.…\n#&gt;  5 Aamia    70.429895 days 149.78110 days 122.847…  40.…  32.…  74.… 116.… 161.…\n#&gt;  6 Aarish  232.188970 days 234.42468 days  70.517…   0.…   7.… 111.… 153.… 118.…\n#&gt;  7 Abdulah   8.890604 days  37.66665 days 130.246…   0.…   0.… 215.… 159.… 162.…\n#&gt;  8 Abhinav 221.372920 days 108.87628 days 114.363… 280.… 136.… 100.…  97.…  95.…\n#&gt;  9 Abhiram  73.235939 days 182.81309 days 295.836… 107.…  96.… 137.… 123.…  68.…\n#&gt; 10 Abyade  107.638230 days 596.18206 days 168.711… 146.…  61.…  34.… 281.… 149.…\n#&gt; # … with 490 more rows\n\ndf %&gt;% \n  complete(subj, dose, fill = list(time = difftime0)) %&gt;% \n  group_by(subj, dose) %&gt;% \n  summarise(time = sum(time), .groups = \"drop_last\") %&gt;%\n  mutate(time = foo1(dose, time)) %&gt;% \n  pivot_wider(\n    names_from = \"dose\",\n    names_sort = TRUE,\n    values_from = \"time\"\n  )\n#&gt; # A tibble: 500 × 9\n#&gt; # Groups:   subj [500]\n#&gt;    subj    `10`           `20`           `30`      `40`  `50`  `60`  `70`  `80` \n#&gt;    &lt;chr&gt;   &lt;drtn&gt;         &lt;drtn&gt;         &lt;drtn&gt;    &lt;drt&gt; &lt;drt&gt; &lt;drt&gt; &lt;drt&gt; &lt;drt&gt;\n#&gt;  1 Aadhya   446.5804 days  284.8256 days  159.234… 159.…  85.…  85.…  23.…  23.…\n#&gt;  2 Aaditri  913.9830 days  883.3118 days  711.108… 711.… 486.… 354.… 161.…  93.…\n#&gt;  3 Aaleeya 1417.4559 days 1242.7773 days 1127.503… 967.… 769.… 584.… 507.… 292.…\n#&gt;  4 Aaleyah 1117.0842 days  933.2237 days  844.960… 844.… 835.… 573.… 314.… 100.…\n#&gt;  5 Aamia    769.4857 days  699.0558 days  549.274… 426.… 385.… 352.… 278.… 161.…\n#&gt;  6 Aarish   927.1830 days  694.9940 days  460.569… 390.… 390.… 382.… 271.… 118.…\n#&gt;  7 Abdulah  714.1323 days  705.2417 days  667.575… 537.… 537.… 537.… 321.… 162.…\n#&gt;  8 Abhinav 1154.8195 days  933.4466 days  824.570… 710.… 429.… 292.… 192.…  95.…\n#&gt;  9 Abhiram 1086.0283 days 1012.7924 days  829.979… 534.… 426.… 329.… 192.…  68.…\n#&gt; 10 Abyade  1545.7654 days 1438.1272 days  841.945… 673.… 526.… 465.… 431.… 149.…\n#&gt; # … with 490 more rows\n\n\nAnd there you go. Three solutions, all the same.\nSometimes it’s useful to try to optimize code. Other times it’s just results in a blog post.\nAs long as your code is easy to read and not apparently slow, it’s probably fine."
  },
  {
    "objectID": "posts/dunning-kruger/index.html",
    "href": "posts/dunning-kruger/index.html",
    "title": "Dunning-Kruger effect",
    "section": "",
    "text": "Note: I originally wrote this in February 2019. Also, don’t hate me for putting some links to Wikipedia, you were going to go there anyway. This isn’t a research paper."
  },
  {
    "objectID": "posts/dunning-kruger/index.html#introduction",
    "href": "posts/dunning-kruger/index.html#introduction",
    "title": "Dunning-Kruger effect",
    "section": "Introduction",
    "text": "Introduction\nIn my Organization Psychology graduate class at West Chester University, one of our assigned readings (among others) for our week on emotions and moods was (Sheldon, Dunning, and Ames 2014). This article focused on another finding relating to the Dunning-Kruger effect in the workplace. This time, in a task related to emotional intelligence (EI). During the time I remember vaguely hearing somewhere I will never recall, some mathematical issues relating to this well-known psychological phenomenon mentioned in many introductory text books.\nThe Dunning-Kruger effect is founding on the concept that an individual that lacks expertise will be more confident in their abilities than they really are, or overestimate their performance on a task. Yet experts may underestimate their own performance or abilities or be more accurate in their estimations. One thing we can derive from this is possibly that those with lower skill will overestimate their abilities while those more skilled will underestimate their abilities.\nThe following is in direct relation to an article by (Sheldon, Dunning, and Ames 2014). They report a significant relationship between an individual’s actual performance and the difference between their perceived ability and actual performance in three conditions (\\(r_1 = -0.83\\), \\(p_1 &lt; .001\\); \\(r_2 = -0.87\\), \\(p_2 &lt; .001\\); \\(r_3 = -0.84\\), \\(p_3 &lt; .001\\)).\nThey also used these two graphs to representing their findings:\n\n\n\nFigure 1. Overestimation of emotional intelligence (left panel) and performance on the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT; right panel) as a function of actual performance on the MSCEIT\n\n\nWe’ll go through and understand why this can be misleading and how to replicate the Dunning-Kruger effect with random data. Yes, random data. Data that are random."
  },
  {
    "objectID": "posts/dunning-kruger/index.html#set-up",
    "href": "posts/dunning-kruger/index.html#set-up",
    "title": "Dunning-Kruger effect",
    "section": "Set up",
    "text": "Set up\nSo let’s place with some data and see what we get. First, let’s setup our .Rmd file and choose a specific randomization seed so we can come back to our results (Douglas 1989):\n\n\nCode\nset.seed(42)\n\noptions(tidyverse.quiet = TRUE) ## silences warnings\nlibrary(tidyverse)\nlibrary(mark)                   ## percentile_rank() | github.com/jmbarbone/mark\n#&gt; \n#&gt; Attaching package: 'mark'\n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     none\nlibrary(broom)                  ## tidying statistical outputs into tables\ntheme_set(theme_minimal())\n\n\n\n\nCode\nf_nbins &lt;- function(x, n = 6) {\n  dplyr::ntile(x, n) / n\n}"
  },
  {
    "objectID": "posts/dunning-kruger/index.html#random-data",
    "href": "posts/dunning-kruger/index.html#random-data",
    "title": "Dunning-Kruger effect",
    "section": "Random data",
    "text": "Random data\nWe’ll start by creating a data frame with two vectors of independent, random data. These will be our randomly assigned percentile ranks of actual and estimate’d performance.\nTo clarify, the calculation of percentile rank is as follows:\n\\[\\text{PR}_i = \\frac{c_\\ell + 0.5 f_i}{N} \\times 100%\\]\nWhere \\(c_\\ell\\) is the count of scores lower than the score of interest, \\(f_i\\) is the frequency of the score of interest, and \\(N\\) is the total number of scores. With this formula, our percentile ranks will always be 0 &lt; \\(PR_i\\) &lt; 100.\n\n\nCode\nrandom_data &lt;- \n  tibble(\n    actual = rnorm(1000),\n    estimate = rnorm(1000)\n  ) %&gt;% \n  mutate(across(everything(), percentile_rank))\n\nrandom_data\n#&gt; # A tibble: 1,000 × 2\n#&gt;    actual estimate\n#&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1  0.920   0.988 \n#&gt;  2  0.292   0.704 \n#&gt;  3  0.656   0.846 \n#&gt;  4  0.738   0.646 \n#&gt;  5  0.670   0.150 \n#&gt;  6  0.462   0.274 \n#&gt;  7  0.946   0.576 \n#&gt;  8  0.464   0.0005\n#&gt;  9  0.980   0.198 \n#&gt; 10  0.480   0.798 \n#&gt; # … with 990 more rows\n\n\nWe also want to bin our data together just like in the article.\n\n\nCode\nbins &lt;- \n  random_data %&gt;% \n  mutate(\n    difference = estimate - actual,\n    bin = f_nbins(actual, 5)\n  ) %&gt;% \n  group_by(bin) %&gt;% \n  summarise(\n    n = n(),\n    mean = mean(difference)\n  )\n\nbins\n#&gt; # A tibble: 5 × 3\n#&gt;     bin     n    mean\n#&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1   0.2   200  0.398 \n#&gt; 2   0.4   200  0.208 \n#&gt; 3   0.6   200  0.0120\n#&gt; 4   0.8   200 -0.215 \n#&gt; 5   1     200 -0.402\n\n\nNow we’ll plot the data and take a look at this.\n\n\nCode\nggplot(random_data, aes(x = actual, y = estimate - actual)) +\n  geom_point(alpha = .2) +\n  geom_smooth(formula = \"y ~ x\", method = lm, se = FALSE, col = \"red\") +\n  geom_point(data = bins, aes(x = bin, y = mean), col = \"blue\", shape = 1, size = 5) +\n  geom_line( data = bins, aes(x = bin, y = mean), col = \"blue\", size = 1) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  labs(\n    title = \"Independent random samples of 'Actual' and 'Estimate' performance\",\n    x = \"'Actual' performance (Percentile Rank)\",\n    y = \"Percentile Overestimation\\n(estimate - actual)\"\n  )\n\n\n\n\n\n\n\n\n\nAlready we’re seeing a trend very similar to that reported in the article. What we also notice is that there are bounds to the overestimation value as a factor of the individual’s actual performance. An individual that performs at the 99th percentile cannot overestimate their own performance (but can be accurate) - much like an individual in the lower percentiles would unlikely underestimate. These is additionally worse by the use of a score derived in reference to others."
  },
  {
    "objectID": "posts/dunning-kruger/index.html#adjusting-random-data",
    "href": "posts/dunning-kruger/index.html#adjusting-random-data",
    "title": "Dunning-Kruger effect",
    "section": "Adjusting random data",
    "text": "Adjusting random data\nSo now we’re going to take some data and use some rough estimates for means. We’ll use the results from the study of interest. So simplicity, I’ll just use the rough means of the n, means, and sd reported from the first two studies.\n\nWe’ll shape our normal distributions around the values found in the paper. These values, to be clear, are the percentile ranks either estimated from the participant or the actual ones as they compare to percentile ranking among U.S. adults in EI. As such, we won’t need to use the percentile_rank() again.\n\n\n\n\nCode\nadj_random &lt;- tibble(\n  actual     = rnorm(161, 42.2, sd = 25.1) / 100,\n  estimate   = rnorm(161, 77.5, sd = 13.1) / 100,\n  difference = actual - estimate,\n  bin        = f_nbins(actual, 5)\n)\n\nadj_bins &lt;- \n  adj_random  %&gt;% \n  group_by(bin) %&gt;% \n  summarise(\n    n = n(),\n    mean = mean(difference)\n  )\n\n\nLet’s also take a look at the correlations we have. As expected, we have no correlation with random data. The article reported correlations of .20 and .19 between estimated and actual performance. Clearly, people are not that great at estimating their own performance.\n\n\nCode\ncor.test(~ actual + estimate, data = adj_random)\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  actual and estimate\n#&gt; t = 1.3, df = 159, p-value = 0.1955\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.05296245  0.25321085\n#&gt; sample estimates:\n#&gt;       cor \n#&gt; 0.1025525\n\n\nWell, no surprise that that our correlations are a weaker and less statistically significant, we’re using random data after all.\nNow we’re going to run a correlation on the actual scores and the difference between the estimated and actual performance.\n\n\nCode\ncor_test_result &lt;- cor.test(~ actual + difference, data = adj_random)\ncor_test_result\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  actual and difference\n#&gt; t = 21.717, df = 159, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.8197724 0.8991906\n#&gt; sample estimates:\n#&gt;       cor \n#&gt; 0.8647931\n\n\nNow, look at that. We have found an even more significant, negative correlation. This is roughly similar to those reported by in this article. This is with data that has absolutely no relationship between the two variables, as we have justed established.\nSo why is this?"
  },
  {
    "objectID": "posts/dunning-kruger/index.html#plotting-adjusted-random-data",
    "href": "posts/dunning-kruger/index.html#plotting-adjusted-random-data",
    "title": "Dunning-Kruger effect",
    "section": "Plotting adjusted random data",
    "text": "Plotting adjusted random data\nLet’s graph out our results with a little more care this time.\n\n\nCode\nggplot(adj_random, aes(x = actual, y = difference)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_point(alpha = .1) +\n  geom_smooth(\n    formula = \"y ~ x\",\n    method = \"lm\",\n    se = FALSE,\n    col = \"red\"\n  ) +\n  geom_point(\n    data = adj_bins,\n    aes(x = bin, y = mean),\n    col = \"blue\",\n    shape = 1,\n    size = 5\n  ) +\n  geom_line(\n    data = adj_bins,\n    aes(x = bin, y = mean),\n    col = \"blue\",\n    size = 1\n  ) +\n  labs(\n    title = \"Randomly generated differences in 'actual' vs 'estimated' performance\",\n    subtitle = \"Estimate: M = 75, SD = 15; Actual: M = 5, SD = 25\",\n    x = \"Actual performance\",\n    y = \"Estimated - Actual performance\"\n  ) +\n  annotate(\n    geom = \"text\",\n    label = glue::glue_data(cor_test_result, \"r = {round(estimate, 3)}, p = {format(p.value)}\"),\n    x = .65,\n    y = .50,\n    hjust = \"left\"\n  )"
  },
  {
    "objectID": "posts/dunning-kruger/index.html#more-random-data",
    "href": "posts/dunning-kruger/index.html#more-random-data",
    "title": "Dunning-Kruger effect",
    "section": "More random data",
    "text": "More random data\nSo what if we repeated this several times?\n\n\nCode\nrandom_helper &lt;- function(x) {\n  set.seed(42 + x)\n  tibble(\n    actual   = rnorm(161, 42.2, sd = 25.1) / 100,\n    estimate = rnorm(161, 77.5, sd = 13.1) / 100,\n  ) %&gt;%\n    mutate(across(everything(), percentile_rank))\n}\n\nsev_random &lt;- \n  as.list(seq(100)) %&gt;% \n  map(random_helper) %&gt;% \n  bind_rows(.id = \"id\") %&gt;% \n  mutate(id = as.numeric(id))\n\nsev_bins &lt;- sev_random %&gt;% \n  group_by(id) %&gt;% \n  mutate(\n    difference = estimate - actual,\n    bin = f_nbins(actual, 5) \n  ) %&gt;% \n  group_by(id, bin) %&gt;% \n  summarise(\n    n = n(),\n    mean_est = mean(estimate),\n    mean_diff = mean(difference)\n  )\n\nggplot(sev_bins, aes(x = bin, y = mean_est, col = factor(id))) +\n  geom_point() +\n  geom_line() +\n  # scale_color_discrete(name = \"Randomization\") +\n  scale_color_discrete(guide = FALSE) +\n  scale_y_continuous(limits = c(0, 1)) + \n  labs(x = \"Actual\", y = \"Estimate\")\n#&gt; Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please\n#&gt; use `guide = \"none\"` instead.\n\n\n\n\n\n\n\n\n\nSo what if we actually run a correlation on these numbers? We’ll create a nested function and install the broom package to help tidy up our results.\n\n\nCode\nrun_correlations &lt;- function(x, item_x, item_y) {\n  corr_helper &lt;- function(x, item_x, item_y) {\n    formula &lt;- str_c(\"~\", item_x, \"+\", item_y, sep = \" \")\n    cor.test(eval(parse(text = formula)), data = x)\n  }\n  \n  x %&gt;% \n    nest(data = -id) %&gt;% \n    mutate(\n      corr = map(data, corr_helper, item_x, item_y),\n      tidy = map(corr, tidy)\n    ) %&gt;% \n    unnest(tidy) %&gt;% \n    select(where(negate(is.list)))\n}\n\n(x &lt;- run_correlations(sev_random, \"actual\", \"estimate\") %&gt;% arrange(p.value))\n#&gt; # A tibble: 100 × 9\n#&gt;       id estimate statistic p.value parameter conf.low conf.high method  alter…¹\n#&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n#&gt;  1    35   -0.207     -2.67 0.00830       159 -0.351    -0.0545  Pearso… two.si…\n#&gt;  2    24    0.185      2.37 0.0188        159  0.0312    0.330   Pearso… two.si…\n#&gt;  3    99    0.170      2.17 0.0314        159  0.0154    0.316   Pearso… two.si…\n#&gt;  4    22   -0.164     -2.09 0.0379        159 -0.311    -0.00930 Pearso… two.si…\n#&gt;  5    90   -0.159     -2.03 0.0443        159 -0.306    -0.00417 Pearso… two.si…\n#&gt;  6     6    0.148      1.89 0.0610        159 -0.00685   0.296   Pearso… two.si…\n#&gt;  7     9    0.141      1.79 0.0747        159 -0.0141    0.289   Pearso… two.si…\n#&gt;  8    97    0.133      1.70 0.0914        159 -0.0216    0.282   Pearso… two.si…\n#&gt;  9    51   -0.127     -1.61 0.109         159 -0.276     0.0285  Pearso… two.si…\n#&gt; 10    89   -0.124     -1.57 0.118         159 -0.273     0.0316  Pearso… two.si…\n#&gt; # … with 90 more rows, and abbreviated variable name ¹​alternative\n(y &lt;- run_correlations(sev_bins, \"bin\", \"mean_est\") %&gt;% arrange(p.value))\n#&gt; # A tibble: 100 × 9\n#&gt; # Groups:   id [100]\n#&gt;       id estimate statistic p.value parameter conf.low conf.high method  alter…¹\n#&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n#&gt;  1    99    0.936      4.62  0.0191         3    0.312     0.996 Pearso… two.si…\n#&gt;  2    35   -0.936     -4.61  0.0192         3   -0.996    -0.309 Pearso… two.si…\n#&gt;  3    28   -0.907     -3.72  0.0337         3   -0.994    -0.122 Pearso… two.si…\n#&gt;  4    41    0.845      2.74  0.0715         3   -0.146     0.990 Pearso… two.si…\n#&gt;  5    68    0.836      2.64  0.0780         3   -0.177     0.989 Pearso… two.si…\n#&gt;  6     5   -0.804     -2.34  0.101          3   -0.987     0.269 Pearso… two.si…\n#&gt;  7     6    0.796      2.28  0.107          3   -0.290     0.986 Pearso… two.si…\n#&gt;  8    10    0.786      2.20  0.115          3   -0.314     0.985 Pearso… two.si…\n#&gt;  9    89   -0.761     -2.03  0.135          3   -0.983     0.369 Pearso… two.si…\n#&gt; 10    13   -0.758     -2.01  0.138          3   -0.983     0.376 Pearso… two.si…\n#&gt; # … with 90 more rows, and abbreviated variable name ¹​alternative\n\nmean(x$p.value &lt; .05)\n#&gt; [1] 0.05\nmean(y$p.value &lt; .05)\n#&gt; [1] 0.03\n\n\nWhen we calculated a correlation with the mean estimates we actually got a significant result from a few of our runs. If fact, about 5% or less are statistically significant… Let’s pull that one out to look at it again.\n\n\nCode\nsignificant_ids &lt;- x %&gt;% filter(p.value &lt; .05) %&gt;% pull(id) %&gt;% as.character()\ntemp &lt;- sev_bins %&gt;% filter(id %in% significant_ids)\n\nsev_random %&gt;% \n  filter(id %in% significant_ids) %&gt;% \n  ggplot(aes(x = actual, y = estimate, group = factor(id), color = factor(id))) +\n  geom_point(alpha = .2) +\n  geom_point(data = temp, aes(x = bin, y = mean_est)) +\n  geom_line(data = temp, aes(x = bin, y = mean_est))\n\n\n\n\n\n\n\n\n\nSo there you have it. A successful replication of this ‘effect’ with random data.\nBut why is this? This is partly because individuals at the lowest quantiles will have a greater likelihood of over-estimating their performance and those at the highest quantiles will underestimate. An individual that performs at the 99th quantile will have almost no choice but to estimate their performance to be below that of reality (see also (Nuhfer et al. 2016)). This seems to be further worsened by the bound nature of the scores. Were these scores and estimates to be something not bound in such a way (for instance the speed in which an individual could complete an assessment) examning the relationship between actual and estimate performance could yield more valid results. These graphical representations and analyses should be cautioned as they are not very meaningful to understanding their effects."
  },
  {
    "objectID": "posts/scribe-release/index.html",
    "href": "posts/scribe-release/index.html",
    "title": "{scribe} release",
    "section": "",
    "text": "I’m excited to be finalizing release preparations of {scribe}. This package supports writing your own Rscript files and executing them through a terminal.\nWe’ll start with a simple example. For most of these, I’ll be using the direct R interface. However, this package is best used with a shebang script 1.\n1 I’m pretty sure this is pronounced like sha-bang because it’s a hash (#) and bang (!). But I used to think she-bang, which conjures The Stone Roses’ She Bangs the Drums, a pleasant ear-worm. I think it also works to just shout octothorpe!Keep in mind that command_args() doesn’t need an explicit input, and when used with Rscript will automatically capture command line arguments.\n\n\nCode\nlibrary(scribe)\nca &lt;- command_args(string = \"-a 1 -b 0\")\nca$add_argument(\"-a\", default = 0L)\nca$add_argument(\"-b\", default = 0L)\nargs &lt;- ca$parse()\nargs$a + args$b\n#&gt; [1] 1\n\n\n\n\nCode\nca$set_input(c(\"-a 10 -b 10\"))\nargs &lt;- ca$parse()\n#&gt; Warning: Not all values parsed:\n#&gt; -a 10 -b 10\nargs$a + args$b\n#&gt; [1] 0\n\n\nThat’s a little easy, so maybe we can make something a bit more interesting.\nFirst we’ll make ourselves a little modeling function. This is not meant for completeness, but simply provides a few examples for creativity.\n\n\nCode\nmy_model &lt;- function(\n    data = c(\"penguins\", \"mtcars\", \"sat.act\"), \n    y, \n    x = NA, \n    family = \"gaussian\", \n    correlation = FALSE\n  ) {\n  data &lt;- match.arg(data)\n  \n  data &lt;- switch(\n    data,\n    penguins = palmerpenguins::penguins,\n    mtcars = datasets::mtcars,\n    sat.act = transform(\n      psych::sat.act, \n      gender = as.integer(gender == 1)\n    )\n  )\n  \n  if (isTRUE(is.na(x))) {\n    x &lt;- setdiff(colnames(data), y)\n  }\n  \n  data &lt;- data[, c(y, x)]\n  form &lt;- stats::DF2formula(data)\n  mod &lt;- stats::glm(form, data = data, family = family)\n  summary(mod, correlation = correlation)\n}\n\n\nNow that we have that, we can set up the command args to parse what our string inputs are.\n\n\nCode\n# we'll pass arguments after\nca &lt;- command_args()\nca$add_description(\"run a quick model\")\nca$add_argument(\n  \"data\",\n  default = \"penguins\",\n  info = \"a dataset to view\"\n)\nca$add_argument(\"y\", info = \"value to predict\")\nca$add_argument(\"x\", default = NA, info = \"variables\")\nca$add_argument(\n  \"--family\",\n  default = \"gaussian\",\n  info = \"error distribution, link function\"\n)\nca$add_argument(\n  \"--correlation\",\n  action = \"flag\",\n  info = \"when set, returns the correlation matrix\"\n)\nca$add_example(\"my-model.R penguins body_mass_g\")\nca$add_example(\"my-model.R mtcars mpg --correlation\")\n\n\nThere’s a default help arg added to the scribeCommandArg object. When --help is found in the command line arguments, the script will try to exit, returning only the help information.\n\n\nCode\noptions(scribe.interactive = TRUE)\nca$set_input(\"--help\")\nca$parse()\n#&gt; {scribe} command_args\n#&gt; \n#&gt; file : /home/jordan/github/quarto-cli/src/resources/rmd/rmd.R\n#&gt; \n#&gt; DESCRIPTION\n#&gt;   run a quick model\n#&gt; \n#&gt; USAGE\n#&gt;   rmd.R [--help | --version]\n#&gt;   rmd.R [data [ARG]] [y [ARG]] [x [ARG]] [--family [ARG]] [--correlation, --no-correlation] \n#&gt; \n#&gt; ARGUMENTS\n#&gt;   --help                          : prints this and quietly exits                   \n#&gt;   --version                       : prints the version of {scribe} and quietly exits\n#&gt;   data [ARG]                      : a dataset to view                               \n#&gt;   y [ARG]                         : value to predict                                \n#&gt;   x [ARG]                         : variables                                       \n#&gt;   --family [ARG]                  : error distribution, link function               \n#&gt;   --correlation, --no-correlation : when set, returns the correlation matrix        \n#&gt; \n#&gt; EXAMPLES\n#&gt;   $ my-model.R penguins body_mass_g    \n#&gt;   $ my-model.R mtcars mpg --correlation\n\n\nLet’s simulate a few examples:\nmy-model.R penguins body_mass_g\n\n\nCode\nca$set_input(c(\"penguins\", \"body_mass_g\"))\ndo.call(my_model, ca$parse())\n#&gt; \n#&gt; Call:\n#&gt; stats::glm(formula = form, family = family, data = data)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -809.70  -180.87    -6.25   176.76   864.22  \n#&gt; \n#&gt; Coefficients:\n#&gt;                    Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       84087.945  41912.019   2.006  0.04566 *  \n#&gt; speciesChinstrap   -282.539     88.790  -3.182  0.00160 ** \n#&gt; speciesGentoo       890.958    144.563   6.163 2.12e-09 ***\n#&gt; islandDream         -21.180     58.390  -0.363  0.71704    \n#&gt; islandTorgersen     -58.777     60.852  -0.966  0.33482    \n#&gt; bill_length_mm       18.964      7.112   2.667  0.00805 ** \n#&gt; bill_depth_mm        60.798     20.002   3.040  0.00256 ** \n#&gt; flipper_length_mm    18.504      3.128   5.915 8.46e-09 ***\n#&gt; sexmale             378.977     48.074   7.883 4.95e-14 ***\n#&gt; year                -42.785     20.949  -2.042  0.04194 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 82096.03)\n#&gt; \n#&gt;     Null deviance: 215259666  on 332  degrees of freedom\n#&gt; Residual deviance:  26517018  on 323  degrees of freedom\n#&gt;   (11 observations deleted due to missingness)\n#&gt; AIC: 4725\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2\n\n\nmy-mode.R mtcars mpg --correlation\n\n\nCode\nca$set_input(c(\"mtcars\", \"mpg\", \"--correlation\"))\ndo.call(my_model, ca$parse())\n#&gt; \n#&gt; Call:\n#&gt; stats::glm(formula = form, family = family, data = data)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -3.4506  -1.6044  -0.1196   1.2193   4.6271  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept) 12.30337   18.71788   0.657   0.5181  \n#&gt; cyl         -0.11144    1.04502  -0.107   0.9161  \n#&gt; disp         0.01334    0.01786   0.747   0.4635  \n#&gt; hp          -0.02148    0.02177  -0.987   0.3350  \n#&gt; drat         0.78711    1.63537   0.481   0.6353  \n#&gt; wt          -3.71530    1.89441  -1.961   0.0633 .\n#&gt; qsec         0.82104    0.73084   1.123   0.2739  \n#&gt; vs           0.31776    2.10451   0.151   0.8814  \n#&gt; am           2.52023    2.05665   1.225   0.2340  \n#&gt; gear         0.65541    1.49326   0.439   0.6652  \n#&gt; carb        -0.19942    0.82875  -0.241   0.8122  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 7.023544)\n#&gt; \n#&gt;     Null deviance: 1126.05  on 31  degrees of freedom\n#&gt; Residual deviance:  147.49  on 21  degrees of freedom\n#&gt; AIC: 163.71\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2\n#&gt; \n#&gt; Correlation of Coefficients:\n#&gt;      (Intercept) cyl   disp  hp    drat  wt    qsec  vs    am    gear \n#&gt; cyl  -0.67                                                            \n#&gt; disp -0.02       -0.27                                                \n#&gt; hp   -0.07       -0.18 -0.52                                          \n#&gt; drat -0.42        0.28 -0.12  0.09                                    \n#&gt; wt    0.09        0.11 -0.77  0.24  0.17                              \n#&gt; qsec -0.77        0.27  0.29  0.11  0.04 -0.51                        \n#&gt; vs    0.09        0.32  0.10 -0.27 -0.03  0.08 -0.37                  \n#&gt; am   -0.23        0.26  0.03 -0.05 -0.16  0.09  0.27  0.21            \n#&gt; gear -0.41        0.35 -0.08 -0.09 -0.07  0.18  0.08 -0.04 -0.31      \n#&gt; carb  0.12       -0.23  0.67 -0.53 -0.21 -0.70  0.27  0.09  0.06 -0.42\n\n\nmy-model.R sat.act gender --family binomial --correlation\n\n\nCode\nca$set_input(c(\"sat.act\", \"gender\", \"--family\", \"binomial\", \"--correlation\"))\ndo.call(my_model, ca$parse())\n#&gt; \n#&gt; Call:\n#&gt; stats::glm(formula = form, family = family, data = data)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -1.6500  -0.9385  -0.7658   1.2356   2.0129  \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -1.804944   0.587445  -3.073  0.00212 ** \n#&gt; education   -0.220411   0.069023  -3.193  0.00141 ** \n#&gt; age          0.024923   0.010339   2.411  0.01593 *  \n#&gt; ACT         -0.019895   0.022941  -0.867  0.38582    \n#&gt; SATV        -0.002496   0.001026  -2.434  0.01493 *  \n#&gt; SATQ         0.005462   0.001069   5.110 3.22e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 895.09  on 686  degrees of freedom\n#&gt; Residual deviance: 854.46  on 681  degrees of freedom\n#&gt;   (13 observations deleted due to missingness)\n#&gt; AIC: 866.46\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n#&gt; \n#&gt; Correlation of Coefficients:\n#&gt;           (Intercept) education age   ACT   SATV \n#&gt; education -0.01                                  \n#&gt; age       -0.28       -0.55                      \n#&gt; ACT       -0.28       -0.09     -0.11            \n#&gt; SATV      -0.25        0.01      0.07 -0.30      \n#&gt; SATQ      -0.24       -0.01      0.06 -0.38 -0.46\n\n\nIf I needed this, maybe it would make sense to be able to read the data from a file path, then execute something like:\nmy-model.R data/example.csv response\nFor a more real example, I’ll use a trimmed down version of a {pak} cli utiliy I’ve been using a lot. I really like using python’s pip and wanted to have something similar to R. {pak} is fantastic and highly recommended.\nSo, to make our own little command line utility, we just need to include small things and get going:\n#!/usr/bin/env -S Rscript --vanilla\n\nlibrary(scribe)\nca &lt;- command_args()\nca$add_argument(\"pkg\", action = \"dots\", default = \"local::.\")\nca$add_argument(\"-d\", \"--dependencies, action = \"list\", default = TRUE)\nargs &lt;- ca$parse()\ndo.call(pak::pak, args)\nNow, I can install packages nicely in a terminal:\npak github::jmbarbone/mark -d\npak dplyr dbplyr dtplyr"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "I wrote a few things",
    "section": "",
    "text": "Hello, did you find some hidden content? This should allow me to link back to mastodon to show that the profiles that are mine are me. Mastodon Mastodon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{scribe} 0.3.0 release\n\n\nA package to support Rscript files\n\n\n\nR\n\n\n{scribe}\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n{scribe} release\n\n\nA package to support Rscript files\n\n\n\nR\n\n\n{scribe}\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence intervals\n\n\nMean estimate\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSite update\n\n\nNow with quarto!\n\n\n\nmeta\n\n\n\n\n\n\n\n\n\nJul 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSelecting columns in {dplyr}\n\n\nQuick tips\n\n\n\nR\n\n\n{dplyr}\n\n\n{tidyselect}\n\n\n\n\n\n\n\n\n\nSep 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSolving one task with three equivalent solutions\n\n\nfor, do.call, vapply\n\n\n\nR\n\n\n{bench}\n\n\n\n\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nDunning-Kruger effect\n\n\nA case of itself?\n\n\n\nR\n\n\npsychology\n\n\ndatavis\n\n\n\n\n\n\n\n\n\nMay 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s alive!\n\n\nFirst\n\n\n\nmeta\n\n\n\n\n\n\n\n\n\nApr 26, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/confidence-intervals/index.html",
    "href": "posts/confidence-intervals/index.html",
    "title": "Confidence intervals",
    "section": "",
    "text": "Confidence intervals (CIs) provide an estimation of a true value within a population, and some type of certainty around that value. The certainty that is constructed can be a little confusion, and perhaps misleading. It is often communicated with less-than-desirable terminology and phrasing."
  },
  {
    "objectID": "posts/confidence-intervals/index.html#objectives",
    "href": "posts/confidence-intervals/index.html#objectives",
    "title": "Confidence intervals",
    "section": "Objectives",
    "text": "Objectives\n\nProvide basic formula for computing a CI for a sample\nProvide an accurate definition of the CI\nDemonstrate the concept of the CI with visual aids"
  },
  {
    "objectID": "posts/confidence-intervals/index.html#parameters",
    "href": "posts/confidence-intervals/index.html#parameters",
    "title": "Confidence intervals",
    "section": "Parameters",
    "text": "Parameters\n\\[\n\\begin{array}{l,l,c,r}\n  \\text{sample n} & n &=& 100 \\\\\n  \\text{population mean} & \\mu &=& 5 \\\\\n  \\text{population standard deviation} & \\sigma &=& 2 \\\\\n  \\text{alpha level} & \\alpha &=& 0.95\n\\end{array}\n\\]\n\nCode\nset.seed(20220804)\nn &lt;- 100\nmu &lt;- 5\nsigma &lt;- 2\nalpha &lt;- 0.95\n\ndo_sample &lt;- function() {\n  # n, mu, sigma defined outside\n  rnorm(n, mu, sigma) \n}\n\nx &lt;- do_sample()\ns &lt;- sd(x)\nxbar &lt;- mean(x)\nz &lt;- qnorm((1 - alpha) / 2, lower.tail = FALSE)\n\n\n\n\nName\nValue\n\n\n\n\nMean \\(\\bar{x}\\)\n5.4082339\n\n\nStandard deviation \\(s\\)\n1.9219276\n\n\nZ-score (\\(z\\))\n1.959964"
  },
  {
    "objectID": "posts/confidence-intervals/index.html#confidence-interval-objective-1-objective-2",
    "href": "posts/confidence-intervals/index.html#confidence-interval-objective-1-objective-2",
    "title": "Confidence intervals",
    "section": "Confidence interval (objective 1, objective 2)",
    "text": "Confidence interval (objective 1, objective 2)\n\\[\n\\text{CI}_\\mathit{mean}\\ =\\ \\bar{x}\\ \\pm\\ Z_{\\alpha/2}\\ \\times \\mathit{se} \\\\\n\\]\nWhen we construct a confidence interval, we are doing so with our specific sample of the population. We are using the mean of the sample as well as the standard deviation.\nThe certainty in the confidence interval is not directly but indirectly linked back to the true population. When we construct a .95 CI, we are constructing a range of values from a sample, and in doing so are making the assertion that were we to construct CIs for other samples within this population, that approximately 95% of those confidence intervals would contain the true population parameter. The .95 certainty is not the certainty that the true population parameter is within our given confidence interval. 1\n1 This is so common that we have to have a section in Wikipedia about this: Confidence intervals: Common misconceptions. This phrase is likely how I’ve been taught and is present within the textbook from my own graduate classes.\nSampling demonstration\nIf we want to check this assumption, we can get 100 new samples and compute the confidence intervals for each of those.\n\n\nCode\ndo_mean_ci &lt;- function(xbar, s) {\n  # n, z defined outside\n  se &lt;- s / sqrt(n)\n  se_z &lt;- se * z\n  c(lower = xbar - se_z, upper = xbar + se_z)\n}\n\nsem &lt;- do_mean_ci(xbar, s)\n\nr_mean_ci &lt;- replicate(100, {\n  r_x &lt;- do_sample()\n  r_xbar &lt;- mean(r_x)\n  r_sd &lt;- sd(r_x)\n  do_mean_ci(r_xbar, r_sd)\n})\n\nstr(r_mean_ci)\n#&gt;  num [1:2, 1:100] 4.77 5.55 4.76 5.52 4.63 ...\n#&gt;  - attr(*, \"dimnames\")=List of 2\n#&gt;   ..$ : chr [1:2] \"lower\" \"upper\"\n#&gt;   ..$ : NULL\n\n\nJust for the plotting, we’re going to sort of the r_mean_ci matrix by the mean value of upper and lower. This doesn’t effect our analysis.\n\n\nCode\n# reorder by mean of ranges -- simply for visuals\nr_mean_ci &lt;- r_mean_ci[, order(apply(r_mean_ci, 2L, mean))]\n\n\nNow we can check against this array of intervals, how many contain our population mean, .\n\n\nCode\nare_between &lt;- r_mean_ci[\"lower\", ] &lt; mu & r_mean_ci[\"upper\", ] &gt; mu\nmean(are_between)\n#&gt; [1] 0.96\n\n\n\n\nPlotting confidence intervals (objective 3)\nWe’re seeing that sum(are_between) of our 100 replications do contain the true population mean, . These are estimations, so we’re always going to get exactly 95 of 100.\nWe can also plot these confidence intervals.\n\n\nCode\n# plot the points\nplot(\n  x = c(r_mean_ci[\"lower\", ], r_mean_ci[\"upper\", ]),\n  y = c(1:100, 1:100),\n  col = ifelse(c(are_between, are_between), \"darkgreen\", \"purple\"),\n  main = \"Confidence intervals ordered by mean value\",\n  xlab = \"CI\",\n  ylab = \"Order of mean values\"\n)\n\n# connect points with lines\nsegments(\n  r_mean_ci[\"lower\", ], \n  1:100, \n  r_mean_ci[\"upper\", ], \n  1:100,\n  col = ifelse(are_between, \"darkgreen\", \"purple\") ,\n)\n\n# add vertical line for mu\nabline(v = mu, col = \"blue\", lwd = 2, lty = 2)\n\n# provide legend\nlegend(\n  \"bottomright\",\n  c(\"Within CI\", \"Outside CI\", expression(mu)),\n  col = c(\"darkgreen\", \"purple\", \"blue\"),\n  lty = c(1, 1, 2),\n  lwd = c(1, 1, 2),\n  pch = c(1, 1, NA)\n)"
  },
  {
    "objectID": "posts/scribe-0.3.0/index.html",
    "href": "posts/scribe-0.3.0/index.html",
    "title": "{scribe} 0.3.0 release",
    "section": "",
    "text": "After months of using my own Rscript utils with {scribe} I’ve failed to notice a now obvious error in my attempts to include type conversions. The original approach was a little too complicated and a too much for what would be needed in most cases.\nThis update includes a breaking change for how conversion works in {scribe}.\nIn the prior minor release, this would be valid:\n\n\nCode\nlibrary(scribe)\n\n\nca &lt;- command_args(string = \"-a 1 -b 0\")\nca$add_argument(\"-a\", convert = integer())\nca$add_argument(\"-b\", convert = character())\nargs &lt;- ca$parse()\nargs$a + args$b\n#&gt; a\n#&gt; [1] 1\n#&gt; \n#&gt; $b\n#&gt; [1] \"0\"\nHowever, there were issues with trying to use a default value and trying to use a conversion value. Instead, a new exported scribe_convert() is included to handle default and custom conversions.\n\n\nCode\nscribe_convert()\n#&gt; function (x, to = default_convert) \n#&gt; {\n#&gt;     if (!is.character(x) || is.null(to)) {\n#&gt;         return(x)\n#&gt;     }\n#&gt;     if (is.factor(to)) {\n#&gt;         return(factor(x, levels = levels(to), ordered = is.ordered(to)))\n#&gt;     }\n#&gt;     if (is.function(to)) {\n#&gt;         to &lt;- match.fun(to)\n#&gt;         return(to(x))\n#&gt;     }\n#&gt;     mode(x) &lt;- mode(to)\n#&gt;     storage.mode(x) &lt;- storage.mode(to)\n#&gt;     attributes(x) &lt;- attributes(to)\n#&gt;     class(x) &lt;- class(to)\n#&gt;     x\n#&gt; }\n#&gt; &lt;bytecode: 0x5579b81e89a0&gt;\n#&gt; &lt;environment: namespace:scribe&gt;\n\n\nThis defaults to returning value_convert() (which may be unexported in the future) and provides other options for \"none\", and \"evaluate\". The latter provides useful alternatives for including new values:\n\n\nCode\na &lt;- new_arg(convert = scribe_convert(\"evaluate\"))\na$convert(\"1:2\")\n#&gt; [1] 1 2\na$convert(\"data.frame(a = 1, b = 2)\")\n#&gt;   a b\n#&gt; 1 1 2\n\n\nYou still have the option for custom conversions, and use the methods of scribe_value() by just passing the method as a string.\n\n\nCode\nca &lt;- command_args(c(\"-a\", 0, \"-b\", \"2023-10-21\", \"-c\", \"list(2, 3)\", \"-d\", 4.4, \"-e\", 5))\nca$add_argument(\"-a\", convert = \"none\")\nca$add_argument(\"-b\", convert = \"default\")\nca$add_argument(\"-c\", convert = \"evaluate\")\nca$add_argument(\"-d\", convert = as.integer)\nca$add_argument(\"-e\", convert = \\(x) rep(\"value\", x))\nstr(ca$parse())\n#&gt; List of 5\n#&gt;  $ a: chr \"0\"\n#&gt;  $ b: POSIXct[1:1], format: \"2023-10-21\"\n#&gt;  $ c:List of 2\n#&gt;   ..$ : num 2\n#&gt;   ..$ : num 3\n#&gt;  $ d: int 4\n#&gt;  $ e: chr [1:5] \"value\" \"value\" \"value\" \"value\" ..."
  },
  {
    "objectID": "posts/its-alive/index.html",
    "href": "posts/its-alive/index.html",
    "title": "It’s alive!",
    "section": "",
    "text": "First post. Just testing, right?"
  },
  {
    "objectID": "posts/its-alive/index.html#its-alive",
    "href": "posts/its-alive/index.html#its-alive",
    "title": "It’s alive!",
    "section": "",
    "text": "First post. Just testing, right?"
  },
  {
    "objectID": "posts/dplyr-select/index.html",
    "href": "posts/dplyr-select/index.html",
    "title": "Selecting columns in {dplyr}",
    "section": "",
    "text": "One feature of report building that has always bothered me was adjusting column names in final so I could have something prettier than a series of columns_with_underscores that people who use too much Excel find repulsive. The same is true of anyType of CamelCase. There is a functionality with dplyr that let’s me manage updating these names through a clean, reversible, and friendly manner. I discovered this like a toddler just seeing what could happen if I passed a named vector into a select() function and delighted with the result. Weird though because I didn’t remember seeing this in any of the documentation, and when I searched harder through dplyr and tidyselect I found nothing except for a sort of close but not really close enough reference in an faq in from tidyselect which warns against the use of external vectors. However, we should be safeguarded against accidents (and warnings) if we employ all_of() (and any_of()).\nLet’s walk through an example.\n\n\nCode\nlibrary(dplyr, warn.conflicts = FALSE)\n\n\nWe’ll use a data set from the psych package that has relatively, very short column names. We’ll want to take these and make them a bit more specific without incurring much penalty with ourselves. Trying to manage column names with spaces and other special characters can be a real thorn in the index finger.\n\n\nCode\ndata(\"bfi\", package = \"psych\")\nsapa &lt;- tibble::as_tibble(bfi)[1:100, c(1:2, 6:7, 11:12, 16:17, 26:28)] # shorter\nsapa\n#&gt; # A tibble: 100 × 11\n#&gt;       A1    A2    C1    C2    E1    E2    N1    N2 gender education   age\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;     &lt;int&gt; &lt;int&gt;\n#&gt;  1     2     4     2     3     3     3     3     4      1        NA    16\n#&gt;  2     2     4     5     4     1     1     3     3      2        NA    18\n#&gt;  3     5     4     4     5     2     4     4     5      2        NA    17\n#&gt;  4     4     4     4     4     5     3     2     5      2        NA    17\n#&gt;  5     2     3     4     4     2     2     2     3      1        NA    17\n#&gt;  6     6     6     6     6     2     1     3     5      2         3    21\n#&gt;  7     2     5     5     4     4     3     1     2      1        NA    18\n#&gt;  8     4     3     3     2     3     6     6     3      1         2    19\n#&gt;  9     4     3     6     6     5     3     5     5      1         1    19\n#&gt; 10     2     5     6     5     2     2     5     5      2        NA    17\n#&gt; # … with 90 more rows\n\n\nWe can create a named vector to help keep track of the longer, more specific names of our output data. The first 8 columns will be renamed and the demographic information will be moved to the start.\n\n\nCode\nlong_names &lt;- c(\n  \"gender\",\n  \"education\",\n  \"age\",\n  \"Indifferent to feelings\"      = \"A1\",\n  \"Inquire about well-being\"     = \"A2\",\n  \"Exacting about work\"          = \"C1\",\n  \"Continue until perfection\"    = \"C2\",\n  \"Don't talk a lot\"             = \"E1\",\n  \"Difficult to approach others\" = \"E2\",\n  \"Get angry easily\"             = \"N1\",\n  \"Get irritated easily\"         = \"N2\"\n)\n\n\nHere’s the typical solution.\n\n\nCode\nsapa %&gt;% \n  select(\n    gender,\n    education,\n    age,\n    `Indifferent to feelings`      = A1,\n    `Inquire about well-being`     = A2,\n    `Exacting about work`          = C1,\n    `Continue until perfection`    = C2,\n    `Don't talk a lot`             = E1,\n    `Difficult to approach others` = E2,\n    `Get angry easily`             = N1,\n    `Get irritated easily`         = N2\n  )\n#&gt; # A tibble: 100 × 11\n#&gt;    gender education   age `Indifferent to fe…` `Inquire about…` `Exacting abou…`\n#&gt;     &lt;int&gt;     &lt;int&gt; &lt;int&gt;                &lt;int&gt;            &lt;int&gt;            &lt;int&gt;\n#&gt;  1      1        NA    16                    2                4                2\n#&gt;  2      2        NA    18                    2                4                5\n#&gt;  3      2        NA    17                    5                4                4\n#&gt;  4      2        NA    17                    4                4                4\n#&gt;  5      1        NA    17                    2                3                4\n#&gt;  6      2         3    21                    6                6                6\n#&gt;  7      1        NA    18                    2                5                5\n#&gt;  8      1         2    19                    4                3                3\n#&gt;  9      1         1    19                    4                3                6\n#&gt; 10      2        NA    17                    2                5                6\n#&gt; # … with 90 more rows, and 5 more variables: `Continue until perfection` &lt;int&gt;,\n#&gt; #   `Don't talk a lot` &lt;int&gt;, `Difficult to approach others` &lt;int&gt;,\n#&gt; #   `Get angry easily` &lt;int&gt;, `Get irritated easily` &lt;int&gt;\n\n\nWe can use the tidyselect::all_of() function without as it is reexported with dplyr.\n\n\nCode\nsapa %&gt;% \n  select(all_of(long_names))\n#&gt; # A tibble: 100 × 11\n#&gt;    gender education   age `Indifferent to fe…` `Inquire about…` `Exacting abou…`\n#&gt;     &lt;int&gt;     &lt;int&gt; &lt;int&gt;                &lt;int&gt;            &lt;int&gt;            &lt;int&gt;\n#&gt;  1      1        NA    16                    2                4                2\n#&gt;  2      2        NA    18                    2                4                5\n#&gt;  3      2        NA    17                    5                4                4\n#&gt;  4      2        NA    17                    4                4                4\n#&gt;  5      1        NA    17                    2                3                4\n#&gt;  6      2         3    21                    6                6                6\n#&gt;  7      1        NA    18                    2                5                5\n#&gt;  8      1         2    19                    4                3                3\n#&gt;  9      1         1    19                    4                3                6\n#&gt; 10      2        NA    17                    2                5                6\n#&gt; # … with 90 more rows, and 5 more variables: `Continue until perfection` &lt;int&gt;,\n#&gt; #   `Don't talk a lot` &lt;int&gt;, `Difficult to approach others` &lt;int&gt;,\n#&gt; #   `Get angry easily` &lt;int&gt;, `Get irritated easily` &lt;int&gt;\n\n\n\n\nBut is it faster?\n\n\n\nCode\nfoo_select_all_of &lt;- function() {\n  long_names &lt;- c(\n    \"gender\",\n    \"education\",\n    \"age\",\n    \"Indifferent to feelings\"      = \"A1\",\n    \"Inquire about well-being\"     = \"A2\",\n    \"Exacting about work\"          = \"C1\",\n    \"Continue until perfection\"    = \"C2\",\n    \"Don't talk a lot\"             = \"E1\",\n    \"Difficult to approach others\" = \"E2\",\n    \"Get angry easily\"             = \"N1\",\n    \"Get irritated easily\"         = \"N2\"\n  )\n  \n  sapa %&gt;% \n  select(all_of(long_names))\n}\n\nfoo_select &lt;- function() {\n  sapa %&gt;% \n  select(\n    gender,\n    education,\n    age,\n    `Indifferent to feelings`      = A1,\n    `Inquire about well-being`     = A2,\n    `Exacting about work`          = C1,\n    `Continue until perfection`    = C2,\n    `Don't talk a lot`             = E1,\n    `Difficult to approach others` = E2,\n    `Get angry easily`             = N1,\n    `Get irritated easily`         = N2\n  )\n}\n\nbench::mark(\n  foo_select_all_of(),\n  foo_select()\n)\n#&gt; # A tibble: 2 × 6\n#&gt;   expression               min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt;          &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 foo_select_all_of()   1.51ms   1.81ms      446.    8.67KB     19.1\n#&gt; 2 foo_select()          5.97ms   6.77ms      143.   41.45KB     28.6\n\n\nYes.\n\nWe get the same result and don’t need to clog up the piped if we need to do some mutation, grouping, summarising, etc. This also lets us separate out definitions of the data in case we need to change things:\n\n\nCode\nlong_names_less &lt;- long_names[c(1, 3, grep(\"about\", names(long_names)))]\n\nsapa %&gt;% \n  select(all_of(long_names_less))\n#&gt; # A tibble: 100 × 4\n#&gt;    gender   age `Inquire about well-being` `Exacting about work`\n#&gt;     &lt;int&gt; &lt;int&gt;                      &lt;int&gt;                 &lt;int&gt;\n#&gt;  1      1    16                          4                     2\n#&gt;  2      2    18                          4                     5\n#&gt;  3      2    17                          4                     4\n#&gt;  4      2    17                          4                     4\n#&gt;  5      1    17                          3                     4\n#&gt;  6      2    21                          6                     6\n#&gt;  7      1    18                          5                     5\n#&gt;  8      1    19                          3                     3\n#&gt;  9      1    19                          3                     6\n#&gt; 10      2    17                          5                     6\n#&gt; # … with 90 more rows\n\n\n\nUsing any_of() instead we could essentially pre-define more “programming” and “output” names and pass it to whatever you are working with. This has been useful by establishing a saved vector of names and using it across multiple reports to keep our naming convention consistent.\n\nWe can even write some short functions in case we need to use an output we’ve created before:\n\n\nCode\nnames_fill &lt;- function(x) {\n  nm &lt;- names(x)\n  blanks &lt;- nm == \"\"\n  names(x)[blanks] &lt;- x[blanks]\n  x\n}\n\n\n\n\nCode\nsapa2 &lt;- sapa %&gt;% select(all_of(long_names))\nlong_names_switched &lt;- mark::names_switch(names_fill(long_names))\nlong_names\n#&gt;                                                           \n#&gt;                     \"gender\"                  \"education\" \n#&gt;                                   Indifferent to feelings \n#&gt;                        \"age\"                         \"A1\" \n#&gt;     Inquire about well-being          Exacting about work \n#&gt;                         \"A2\"                         \"C1\" \n#&gt;    Continue until perfection             Don't talk a lot \n#&gt;                         \"C2\"                         \"E1\" \n#&gt; Difficult to approach others             Get angry easily \n#&gt;                         \"E2\"                         \"N1\" \n#&gt;         Get irritated easily \n#&gt;                         \"N2\"\nlong_names_switched\n#&gt;                         gender                      education \n#&gt;                       \"gender\"                    \"education\" \n#&gt;                            age                             A1 \n#&gt;                          \"age\"      \"Indifferent to feelings\" \n#&gt;                             A2                             C1 \n#&gt;     \"Inquire about well-being\"          \"Exacting about work\" \n#&gt;                             C2                             E1 \n#&gt;    \"Continue until perfection\"             \"Don't talk a lot\" \n#&gt;                             E2                             N1 \n#&gt; \"Difficult to approach others\"             \"Get angry easily\" \n#&gt;                             N2 \n#&gt;         \"Get irritated easily\"\n\nsapa2 %&gt;% \n  select(all_of(long_names_switched))\n#&gt; # A tibble: 100 × 11\n#&gt;    gender education   age    A1    A2    C1    C2    E1    E2    N1    N2\n#&gt;     &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1      1        NA    16     2     4     2     3     3     3     3     4\n#&gt;  2      2        NA    18     2     4     5     4     1     1     3     3\n#&gt;  3      2        NA    17     5     4     4     5     2     4     4     5\n#&gt;  4      2        NA    17     4     4     4     4     5     3     2     5\n#&gt;  5      1        NA    17     2     3     4     4     2     2     2     3\n#&gt;  6      2         3    21     6     6     6     6     2     1     3     5\n#&gt;  7      1        NA    18     2     5     5     4     4     3     1     2\n#&gt;  8      1         2    19     4     3     3     2     3     6     6     3\n#&gt;  9      1         1    19     4     3     6     6     5     3     5     5\n#&gt; 10      2        NA    17     2     5     6     5     2     2     5     5\n#&gt; # … with 90 more rows\n\n\nAnd now our names are back to normal."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Visual above made with Nathaniel Phillip’s VisualResume.\nI am currently working as a Data Scientist at Cogstate where I get to work with our Science and Data teams work with outcomes from computerized, cognitive assessments and other outcomes data (e.g., COA, ClinRO, PRO).\nI started learning R in my gradate statistics classes to avoid the future pains of having to use or only being familiar with SPSS. Learning R then launched me into a bigger role at Bracket (now Signant Health) where I had the pleasure to work with our Clinical Science team to understand and interpret their data. This gave us the opportunity to learn more about our technologies, approaches, and quality of reporting in the clinical trials we were assisting. Here I was a charter member of the Data Science team and have the pleasure of enhancing our trial monitoring products, training other Data Science programmers, and contributing to automation and reporting efforts of our Clinical Data Management team. I worked as the Principal Data Analyst for Madrigal Pharmaceuticals, helping our Research & Development team monitor our current trials and prepare summary reports. This was a bit of a departure for me to work with more lab and imaging data and in an area I didn’t have any experience: liver disease. Despite that, there were plenty of data I recognized: from clinical operations, data management, and even some interested PRO QoL data.\nI have a Masters of Art in Psychology and have spent my graduate and undergrad classes engaging in Neuropsychology and Cognitive topics. Some of these topics have included understanding how music affects our ability on tests of aptitude. This, of course, was the basis of my Masters Thesis and hours of reading journal article after journal article. Of course I’ve also been very interested in immersing myself more into statistics, statistical programming, and special psychometric subjects. Please browse the Anthology page where I have tried to compile projects, papers, and presentations I’ve written in and out of class.\nAnd of course, here is my dog’s IG TheLabBuckley. It’s mostly him sleeping."
  }
]