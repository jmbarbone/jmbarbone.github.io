{"categories":[{"title":"r code","uri":"https://jmbarbone.github.io/categories/r-code/"}],"posts":[{"content":"\r\rI can spend a lot of time trying in futility to optimize custom functions and code.\rI’ll take the time to rewrite the same thing in two or more ways and run some basic benchmarks for performance differences.\rMost of the time I’m just exploring the simplest possible way to perform an action and what would be the most generalizable solution that could extend to other problems.\rI ran into an issue where I had to use a quick function to find the total amount of time a subject in a clinical trial was on a specific dose.\rHowever, subjects had doses adjusted through the study for optimization.\rFurther, we also needed to count the dosing based on the lowest dose provided.\n\rFor example, how long was Subject A on a \u0026gt;= 10mg dose?\n\rSo, let’s explore three different ways to solve this problem and why all three are valid and just as efficient.\nSet up\rLoading our tidyverse packages first.\rBecause we will be grouping and summarising data in a data.frame, I really don’t feel like going through the trouble of using base solutions.\noptions(tidyverse.quiet = TRUE) # Silences messages\rlibrary(tidyverse)\rLet’s create some random data to represent our subjects, our doses, and an arbitrary time metric.\rWe’re also going to make this harder by using the difftime.\nWe’re also going to grab some names using a function from the wakefield package which you can use to create fake data, appropriately named. [^note]\n^note: I’m not too savvy with this package yet but would like to experiment more for building dummy data sets.\nsubj \u0026lt;- sort(sample(wakefield::name(500), 1e4, TRUE))\rdose \u0026lt;- sample(seq.int(10, 80, 10), 1e4, TRUE)\rtime \u0026lt;- as.difftime(runif(1e4) * 100, units = \u0026quot;days\u0026quot;)\rdf \u0026lt;- tibble(subj, dose, time)\rdf\r# A tibble: 10,000 x 3\rsubj dose time \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;drtn\u0026gt; 1 Aadhya 10 81.430584 days\r2 Aadhya 10 80.324186 days\r3 Aadhya 40 64.917350 days\r4 Aadhya 80 17.631323 days\r5 Aadhya 40 8.970711 days\r6 Aadhya 20 96.518079 days\r7 Aadhya 60 61.493537 days\r8 Aadhya 20 29.073014 days\r9 Aadhya 80 6.221622 days\r10 Aaditri 50 54.387006 days\r# ... with 9,990 more rows\rFinding the total sum of time at each dose is easy:\ndf %\u0026gt;% group_by(subj, dose) %\u0026gt;% summarise(time = sum(time), .groups = \u0026quot;drop\u0026quot;) %\u0026gt;% pivot_wider(names_from = \u0026quot;dose\u0026quot;,\rnames_sort = TRUE,\rvalues_from = \u0026quot;time\u0026quot;)\r# A tibble: 500 x 9\rsubj `10` `20` `30` `40` `50` `60` `70` `80` \u0026lt;chr\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; 1 Aadhya 161.7547~ 125.591~ ~ 73.8880~ ~ 61.49~ ~ 23.85~\r2 Aadit~ 30.6712~ 172.203~ ~ 224.9110~ 131.421~ 193.56~ 67.98~ 93.22~\r3 Aalee~ 174.6785~ 115.274~ 159.963~ 198.0695~ 184.672~ 77.62~ 214.77~ 292.39~\r4 Aaley~ 183.8605~ 88.263~ ~ 9.1255~ 261.859~ 259.78~ 214.15~ 100.03~\r5 Aamia 70.4298~ 149.781~ 122.847~ 40.4504~ 32.994~ 74.87~ 116.54~ 161.57~\r6 Aarish 232.1889~ 234.424~ 70.517~ ~ 7.558~ 111.19~ 153.24~ 118.05~\r7 Abdul~ 8.8906~ 37.666~ 130.246~ ~ ~ 215.40~ 159.77~ 162.15~\r8 Abhin~ 221.3729~ 108.876~ 114.363~ 280.9107~ 136.460~ 100.08~ 97.43~ 95.31~\r9 Abhir~ 73.2359~ 182.813~ 295.836~ 107.8884~ 96.700~ 137.15~ 123.40~ 68.99~\r10 Abyade 107.6382~ 596.182~ 168.711~ 146.6280~ 61.121~ 34.35~ 281.28~ 149.84~\r# ... with 490 more rows\rOh, it looks like we have some missing values (see the first line where we are missing doses of 30, 50, and 70 for Aadhya.).\rtidyr::pivot_wider() allows us to fill in the missing values but like many tidyverse functions, we’ll have to be explicit with the type so as to not case any accidental issues.\ndifftime0 \u0026lt;- as.difftime(0, units = \u0026quot;days\u0026quot;)\rres \u0026lt;- df %\u0026gt;% group_by(subj, dose) %\u0026gt;% summarise(time = sum(time), .groups = \u0026quot;drop\u0026quot;) %\u0026gt;% pivot_wider(names_from = \u0026quot;dose\u0026quot;,\rnames_sort = TRUE,\rvalues_from = \u0026quot;time\u0026quot;,\rvalues_fill = difftime0)\rres\r# A tibble: 500 x 9\rsubj `10` `20` `30` `40` `50` `60` `70` `80` \u0026lt;chr\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; 1 Aadhya 161.7547~ 125.591~ 0.000~ 73.8880~ 0.000~ 61.49~ 0.00~ 23.85~\r2 Aadit~ 30.6712~ 172.203~ 0.000~ 224.9110~ 131.421~ 193.56~ 67.98~ 93.22~\r3 Aalee~ 174.6785~ 115.274~ 159.963~ 198.0695~ 184.672~ 77.62~ 214.77~ 292.39~\r4 Aaley~ 183.8605~ 88.263~ 0.000~ 9.1255~ 261.859~ 259.78~ 214.15~ 100.03~\r5 Aamia 70.4298~ 149.781~ 122.847~ 40.4504~ 32.994~ 74.87~ 116.54~ 161.57~\r6 Aarish 232.1889~ 234.424~ 70.517~ 0.0000~ 7.558~ 111.19~ 153.24~ 118.05~\r7 Abdul~ 8.8906~ 37.666~ 130.246~ 0.0000~ 0.000~ 215.40~ 159.77~ 162.15~\r8 Abhin~ 221.3729~ 108.876~ 114.363~ 280.9107~ 136.460~ 100.08~ 97.43~ 95.31~\r9 Abhir~ 73.2359~ 182.813~ 295.836~ 107.8884~ 96.700~ 137.15~ 123.40~ 68.99~\r10 Abyade 107.6382~ 596.182~ 168.711~ 146.6280~ 61.121~ 34.35~ 281.28~ 149.84~\r# ... with 490 more rows\rThis is much more troublesome as we have to create single length difftime vector.\nNow, we need to find the total length of time a subject was at each dose or greater..\rThis I wasn’t able to do with any built in functions (although I could have missed it).\rI’m not claiming this is the best function or anything, it’s not, but I have 3 different solutions and thought it was enough to spend my evening writing a blog post.\nSolution 1: The for Loop\rNever use a for loop, except if you have to.\rWe’ll start with\nfoo1 \u0026lt;- function(x, y) {\rout \u0026lt;- y\rfor (i in seq_along(y)) {\rout[i] \u0026lt;- sum(y[x \u0026gt;= x[i]])\r}\rout\r}\r\rSolution 2: Combining lapply\rWhen having to play with dates before, I found that the way I could retain the date values and still use a function from the apply family was to stick with the lapply() and then use the do.call() function to apply the combine function over my list.\rlapply() retains the original classes and using the do.call(c, ...) method will turn my list into a vector without removing the structure of the output\nfoo2 \u0026lt;- function(x, y) {\rout \u0026lt;- lapply(x, function(xx) sum(y[x \u0026gt;= xx]))\rdo.call(c, out)\r}\rLet’s see how this plays out.\rIf we use other methods, we lose the difftime class, which is noticeable as we don’t get our message of Time differences in days before our results.\nx \u0026lt;- lapply(dose, function(xx) sum(time[dose \u0026gt;= xx])) %\u0026gt;% unlist() %\u0026gt;% head()\rprint_with_class_type(x)\rclass numeric\rtypeof double\r[1] 501497.92 501497.92 312102.52 61765.11 312102.52 437797.16\rx \u0026lt;- sapply(dose, function(xx) sum(time[dose \u0026gt;= xx])) %\u0026gt;% head()\rprint_with_class_type(x)\rclass numeric\rtypeof double\r[1] 501497.92 501497.92 312102.52 61765.11 312102.52 437797.16\rx \u0026lt;- foo2(dose, time) %\u0026gt;% head()\rprint_with_class_type(x)\rclass difftime\rtypeof double\rTime differences in days[1] 501497.92 501497.92 312102.52 61765.11 312102.52 437797.16\r\rSolution 3: vapply with subset assigning\rHere’s another neat little trick that with a good use case.\rWe’re going to subset our out object (again, a copy of the y intput) and assign over it the result of our vapply.\rWe’re also going to cheat and use the first position of y as our FUN.VALUE.\nfoo3 \u0026lt;- function(x, y) {\rout \u0026lt;- y\rout[] \u0026lt;- vapply(x, function(xx) sum(y[x \u0026gt;= xx]), y[1], USE.NAMES = FALSE)\rout\r}\rLet’s take look just like before.\rvapply() will try to simplify the FUN.VALUE but as long as we use a single vector from the original input we can safely assign it back into our mock subset without worrying about losing our classes.\nx \u0026lt;- vapply(dose, function(xx) sum(time[dose \u0026gt;= xx]), time[1]) %\u0026gt;% head()\rprint_with_class_type(x)\rclass numeric\rtypeof double\r[1] 501497.92 501497.92 312102.52 61765.11 312102.52 437797.16\rx \u0026lt; -foo3(dose, time) %\u0026gt;% head()\r[1] FALSE FALSE FALSE FALSE FALSE FALSE\rprint_with_class_type(x)\rclass numeric\rtypeof double\r[1] 501497.92 501497.92 312102.52 61765.11 312102.52 437797.16\r\r\rBenchmarks\rNow, all three of these solutions produce the same results and are fairly equivalent in human legibility.\rThis means, for me at least, that the function which runs the fastest would be the result I keep.\rWe’ll employ the microbenchmark package and eponymous name for our consideration.\nOf course, for this we’ll be running on the vectors first.\rWe’ll also make certain that all of our outputs are the same with check = \"equal\" to make sure we didn’t miss anything either.\nbench::mark(\r`1` = foo1(dose, time),\r`2` = foo2(dose, time),\r`3` = foo3(dose, time)\r)[c(2:5, 9)]\rWarning: Some expressions had a GC in every iteration; so filtering is disabled.\r# A tibble: 3 x 4\rmin median `itr/sec` mem_alloc\r\u0026lt;bch:tm\u0026gt; \u0026lt;bch:tm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;bch:byt\u0026gt;\r1 2.07s 2.07s 0.483 2.21GB\r2 1.6s 1.6s 0.624 2.22GB\r3 1.58s 1.58s 0.631 2.21GB\rAnd, well, they all run about the same.\rThat kind of just leaves us with the sinking feeling that all of this was futile and that for loops really aren’t that bad.\rIn fact, the for loop solution may be the easiest to read and doesn’t use any tricks that someone reviewing your code may not understand at first.\nWe know we have some missing values in our data, so we’re going to use the the tidyr::complete() function to help with that.\nres\r# A tibble: 500 x 9\rsubj `10` `20` `30` `40` `50` `60` `70` `80` \u0026lt;chr\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; 1 Aadhya 161.7547~ 125.591~ 0.000~ 73.8880~ 0.000~ 61.49~ 0.00~ 23.85~\r2 Aadit~ 30.6712~ 172.203~ 0.000~ 224.9110~ 131.421~ 193.56~ 67.98~ 93.22~\r3 Aalee~ 174.6785~ 115.274~ 159.963~ 198.0695~ 184.672~ 77.62~ 214.77~ 292.39~\r4 Aaley~ 183.8605~ 88.263~ 0.000~ 9.1255~ 261.859~ 259.78~ 214.15~ 100.03~\r5 Aamia 70.4298~ 149.781~ 122.847~ 40.4504~ 32.994~ 74.87~ 116.54~ 161.57~\r6 Aarish 232.1889~ 234.424~ 70.517~ 0.0000~ 7.558~ 111.19~ 153.24~ 118.05~\r7 Abdul~ 8.8906~ 37.666~ 130.246~ 0.0000~ 0.000~ 215.40~ 159.77~ 162.15~\r8 Abhin~ 221.3729~ 108.876~ 114.363~ 280.9107~ 136.460~ 100.08~ 97.43~ 95.31~\r9 Abhir~ 73.2359~ 182.813~ 295.836~ 107.8884~ 96.700~ 137.15~ 123.40~ 68.99~\r10 Abyade 107.6382~ 596.182~ 168.711~ 146.6280~ 61.121~ 34.35~ 281.28~ 149.84~\r# ... with 490 more rows\rdf %\u0026gt;% complete(subj, dose, fill = list(time = difftime0)) %\u0026gt;% group_by(subj, dose) %\u0026gt;% summarise(time = sum(time), .groups = \u0026quot;drop_last\u0026quot;) %\u0026gt;%\rmutate(time = foo1(dose, time)) %\u0026gt;% pivot_wider(names_from = \u0026quot;dose\u0026quot;,\rnames_sort = TRUE,\rvalues_from = \u0026quot;time\u0026quot;)\r# A tibble: 500 x 9\r# Groups: subj [500]\rsubj `10` `20` `30` `40` `50` `60` `70` `80` \u0026lt;chr\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; \u0026lt;drtn\u0026gt; 1 Aadhya 446.58~ 284.82~ 159.23~ 159.234~ 85.346~ 85.346~ 23.852~ 23.85~\r2 Aadit~ 913.98~ 883.31~ 711.10~ 711.108~ 486.197~ 354.775~ 161.214~ 93.22~\r3 Aalee~ 1417.45~ 1242.77~ 1127.50~ 967.539~ 769.470~ 584.797~ 507.168~ 292.39~\r4 Aaley~ 1117.08~ 933.22~ 844.96~ 844.960~ 835.834~ 573.975~ 314.189~ 100.03~\r5 Aamia 769.48~ 699.05~ 549.27~ 426.427~ 385.976~ 352.982~ 278.111~ 161.57~\r6 Aarish 927.18~ 694.99~ 460.56~ 390.051~ 390.051~ 382.493~ 271.300~ 118.05~\r7 Abdul~ 714.13~ 705.24~ 667.57~ 537.329~ 537.328~ 537.328~ 321.928~ 162.15~\r8 Abhin~ 1154.81~ 933.44~ 824.57~ 710.207~ 429.296~ 292.835~ 192.746~ 95.31~\r9 Abhir~ 1086.02~ 1012.79~ 829.97~ 534.142~ 426.253~ 329.553~ 192.403~ 68.99~\r10 Abyade 1545.76~ 1438.12~ 841.94~ 673.233~ 526.605~ 465.484~ 431.129~ 149.84~\r# ... with 490 more rows\rAnd there you go.\rThree solutions, all the same.\nSometimes it’s useful to try to optimize code.\rOther times it’s just results in a blog post.\nAs long as your code is easy to read and not apparently slow, it’s probably fine.\n\r","id":0,"section":"posts","summary":"I can spend a lot of time trying in futility to optimize custom functions and code.\rI’ll take the time to rewrite the same thing in two or more ways and run some basic benchmarks for performance differences.\rMost of the time I’m just exploring the simplest possible way to perform an action and what would be the most generalizable solution that could extend to other problems.\rI ran into an issue where I had to use a quick function to find the total amount of time a subject in a clinical trial was on a specific dose.","tags":["R","benchmark"],"title":"Solving one task with three equivalent solutions: for, do.call, vapply","uri":"https://jmbarbone.github.io/2020/08/three-ways/","year":"2020"},{"content":"\r\r\rNote: I originally wrote this in February 2019.\rAlso, don’t hate me for putting some links to Wikipedia, you were going to go there anyway.\rThis isn’t a research paper.\n\rIntroduction\rIn my Organization Psychology graduate class at West Chester University, one of our assigned readings (among others) for our week on emotions and moods was (Sheldon, Dunning, and Ames 2014).\rThis article focused on another finding relating to the Dunning-Kruger effect in the workplace.\rThis time, in a task related to emotional intelligence (EI).\rDuring the time I remember vaguely hearing somewhere I will never recall, some mathematical issues relating to this well-known psychological phenomenon mentioned in many introductory text books.\nThe Dunning-Kruger effect is founding on the concept that an individual that lacks expertise will be more confident in their abilities than they really are, or overestimate their performance on a task.\rYet experts may underestimate their own performance or abilities or be more accurate in their estimations.\rOne thing we can derive from this is possibly that those with lower skill will overestimate their abilities while those more skilled will underestimate their abilities.\nThe following is in direct relation to an article by (Sheldon, Dunning, and Ames 2014).\rThey report a significant relationship between an individual’s actual performance and the difference between their perceived ability and actual performance in three conditions (\\(r_1 = -0.83\\), \\(p_1 \u0026lt; .001\\); \\(r_2 = -0.87\\), \\(p_2 \u0026lt; .001\\); \\(r_3 = -0.84\\), \\(p_3 \u0026lt; .001\\)).\nThey also used these two graphs to representing their findings:\nFigure 1. Overestimation of emotional intelligence (left panel) and performance on the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT; right panel) as a function of actual performance on the MSCEIT\n\rWe’ll go through and understand why this can be misleading and how to replicate the Dunning-Kruger effect with random data.\rYes, random data.\rData that are random.\n\rSet up\rSo let’s place with some data and see what we get.\rFirst, let’s setup our .Rmd file and choose a specific randomization seed so we can come back to our results (Douglas 1989):\nset.seed(42)\roptions(tidyverse.quiet = TRUE) ## silences warnings\rlibrary(tidyverse)\rlibrary(jordan) ## percentile_rank() | github.com/jmbarbone/jordan\rlibrary(broom) ## tidying statistical outputs into tables\rtheme_set(theme_minimal())\r\rRandom data\rWe’ll start by creating a data frame with two vectors of independent, random data.\rThese will be our randomly assigned percentile ranks of actual and estimate’d performance.\nTo clarify, the calculation of percentile rank is as follows:\n\\[\\text{PR}_i = \\frac{c_\\ell + 0.5 f_i}{N} \\times 100%\\]\nWhere \\(c_\\ell\\) is the count of scores lower than the score of interest, \\(f_i\\) is the frequency of the score of interest, and \\(N\\) is the total number of scores.\rWith this formula, our percentile ranks will always be 0 \u0026lt; \\(PR_i\\) \u0026lt; 100.\nrandom_data \u0026lt;- tibble(actual = rnorm(1000),\restimate = rnorm(1000)) %\u0026gt;% mutate_all(percentile_rank)\rrandom_data\r# A tibble: 1,000 x 2\ractual estimate\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 92.0 98.9 2 29.2 70.4 3 65.6 84.6 4 73.8 64.6 5 67.0 15.0 6 46.2 27.4 7 94.6 57.6 8 46.4 0.05\r9 98.0 19.8 10 47.9 79.9 # ... with 990 more rows\rWe also want to bin our data together just like in the article.\nbins \u0026lt;- random_data %\u0026gt;% mutate(difference = estimate - actual,\rbin = ntile(actual, 5) * 20 - 10) %\u0026gt;% group_by(bin) %\u0026gt;% summarise(n = n(),\rmean = mean(difference))\rbins\r# A tibble: 5 x 3\rbin n mean\r\u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r1 10 200 39.8 2 30 200 20.8 3 50 200 1.20\r4 70 200 -21.5 5 90 200 -40.2 \rNow we’ll plot the data and take a look at this.\nggplot(random_data, aes(x = actual, y = estimate - actual)) +\rgeom_point(alpha = .2) +\rgeom_smooth(formula = \u0026quot;y ~ x\u0026quot;, method = lm, se = FALSE, col = \u0026quot;red\u0026quot;) +\rgeom_point(data = bins, aes(x = bin, y = mean), col = \u0026quot;blue\u0026quot;, shape = 1, size = 5) +\rgeom_line(data = bins, aes(x = bin, y = mean), col = \u0026quot;blue\u0026quot;, size = 1) +\rgeom_hline(yintercept = 0, linetype = 2) +\rlabs(title = \u0026quot;Independent random samples of \u0026#39;Actual\u0026#39; and \u0026#39;Estimate\u0026#39; performance\u0026quot;,\rx = \u0026quot;\u0026#39;Actual\u0026#39; performance (Percentile Rank)\u0026quot;,\ry = \u0026quot;Percentile Overestimation\\n(estimate - actual)\u0026quot;)\rAlready we’re seeing a trend very similar to that reported in the article.\rWhat we also notice is that there are bounds to the overestimation value as a factor of the individual’s actual performance.\rAn individual that performs at the 99th percentile cannot overestimate their own performance (but can be accurate) - much like an individual in the lower percentiles would unlikely underestimate.\rThese is additionally worse by the use of a score derived in reference to others.\n\rAdjusting random data\rSo now we’re going to take some data and use some rough estimates for means.\rWe’ll use the results from the study of interest.\rSo simplicity, I’ll just use the rough means of the n, means, and sd reported from the first two studies.\nWe’ll shape our normal distributions around the values found in the paper.\rThese values, to be clear, are the percentile ranks either estimated from the participant or the actual ones as they compare to percentile ranking among U.S. adults in EI.\rAs such, we won’t need to use the percentile_rank() again.\nadj_random \u0026lt;- tibble(actual = rnorm(161, 42.2, sd = 25.1),\restimate = rnorm(161, 77.5, sd = 13.1),\rdifference = actual - estimate,\rbin = ntile(actual, 5) * 20 - 10)\radj_bins \u0026lt;- adj_random %\u0026gt;% group_by(bin) %\u0026gt;% summarise(n = n(),\rmean = mean(difference))\rLet’s also take a look at the correlations we have.\rAs expected, we have no correlation with random data.\rThe article reported correlations of .20 and .19 between estimated and actual performance.\rClearly, people are not that great at estimating their own performance.\ncor.test(~ actual + estimate, data = adj_random)\r\rPearson\u0026#39;s product-moment correlation\rdata: actual and estimate\rt = 1.3, df = 159, p-value = 0.1955\ralternative hypothesis: true correlation is not equal to 0\r95 percent confidence interval:\r-0.05296245 0.25321085\rsample estimates:\rcor 0.1025525 \rWell, no surprise that that our correlations are a weaker and less statistically significant, we’re using random data after all.\nNow we’re going to run a correlation on the actual scores and the difference between the estimated and actual performance.\n(cor_test_result \u0026lt;- cor.test(~ actual + difference, data = adj_random))\r\rPearson\u0026#39;s product-moment correlation\rdata: actual and difference\rt = 21.717, df = 159, p-value \u0026lt; 2.2e-16\ralternative hypothesis: true correlation is not equal to 0\r95 percent confidence interval:\r0.8197724 0.8991906\rsample estimates:\rcor 0.8647931 \rNow, look at that.\rWe have found an even more significant, negative correlation.\rThis is roughly similar to those reported by in this article.\rThis is with data that has absolutely no relationship between the two variables, as we have justed established.\nSo why is this?\n\rPlotting adjusted random data\rLet’s graph out our results with a little more care this time.\nggplot(adj_random, aes(x = actual, y = difference)) +\rgeom_hline(yintercept = 0, linetype = 2) +\rgeom_point(alpha = .1) +\rgeom_smooth(formula = \u0026quot;y ~ x\u0026quot;,\rmethod = \u0026quot;lm\u0026quot;,\rse = FALSE,\rcol = \u0026quot;red\u0026quot;) +\rgeom_point(data = adj_bins,\raes(x = bin, y = mean),\rcol = \u0026quot;blue\u0026quot;,\rshape = 1,\rsize = 5) +\rgeom_line(data = adj_bins,\raes(x = bin, y = mean),\rcol = \u0026quot;blue\u0026quot;,\rsize = 1) +\rlabs(title = \u0026quot;Randomly generated differences in \u0026#39;actual\u0026#39; vs \u0026#39;estimated\u0026#39; performance\u0026quot;,\rsubtitle = \u0026quot;Estimate: M = 75, SD = 15; Actual: M = 5, SD = 25\u0026quot;,\rx = \u0026quot;Actual performance\u0026quot;,\ry = \u0026quot;Estimated - Actual performance\u0026quot;) +\rannotate(geom = \u0026quot;text\u0026quot;,\rlabel = glue::glue_data(\rcor_test_result,\r\u0026quot;r = {round(estimate, 3)}, p = {format(p.value)}\u0026quot;),\rx = 65,\ry = 50,\rhjust = \u0026quot;left\u0026quot;)\r\rMore random data\rSo what if we repeated this several times?\nrandom_helper \u0026lt;- function(x) {\rset.seed(42 + x)\rtibble(actual = rnorm(161, 42.2, sd = 25.1),\restimate = rnorm(161, 77.5, sd = 13.1)) %\u0026gt;%\rmutate_all(percentile_rank)\r}\rsev_random \u0026lt;- as.list(seq(100)) %\u0026gt;% map(random_helper) %\u0026gt;% bind_rows(.id = \u0026quot;id\u0026quot;) %\u0026gt;% mutate(id = as.numeric(id))\rsev_bins \u0026lt;- sev_random %\u0026gt;% group_by(id) %\u0026gt;% mutate(difference = estimate - actual,\rbin = ntile(actual, 5) * 20 - 10) %\u0026gt;% group_by(id, bin) %\u0026gt;% summarise(n = n(),\rmean_est = mean(estimate),\rmean_diff = mean(difference))\rggplot(sev_bins, aes(x = bin, y = mean_est, col = factor(id))) +\rgeom_point() +\rgeom_line() +\r# scale_color_discrete(name = \u0026quot;Randomization\u0026quot;) +\rscale_color_discrete(guide = FALSE) +\rscale_y_continuous(limits = c(0, 99)) + labs(x = \u0026quot;Actual\u0026quot;,\ry = \u0026quot;Estimate\u0026quot;)\rSo what if we actually run a correlation on these numbers?\rWe’ll create a nested function and install the broom package to help tidy up our results.\nrun_correlations \u0026lt;- function(x, item_x, item_y) {\rcorr_helper \u0026lt;- function(x, item_x, item_y) {\rformula \u0026lt;- str_c(\u0026quot;~\u0026quot;, item_x, \u0026quot;+\u0026quot;, item_y, sep = \u0026quot; \u0026quot;)\rcor.test(eval(parse(text = formula)), data = x)\r}\rx %\u0026gt;% nest(data = -id) %\u0026gt;% mutate(corr = map(data, corr_helper, item_x, item_y),\rtidy = map(corr, tidy)) %\u0026gt;% unnest(tidy) %\u0026gt;% select_if(negate(is.list))\r}\r(x \u0026lt;- run_correlations(sev_random, \u0026quot;actual\u0026quot;, \u0026quot;estimate\u0026quot;) %\u0026gt;% arrange(p.value))\r# A tibble: 100 x 9\rid estimate statistic p.value parameter conf.low conf.high method\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; 1 35 -0.207 -2.67 0.00830 159 -0.351 -0.0545 Pears~\r2 24 0.185 2.37 0.0188 159 0.0312 0.330 Pears~\r3 99 0.170 2.17 0.0314 159 0.0154 0.316 Pears~\r4 22 -0.164 -2.09 0.0379 159 -0.311 -0.00930 Pears~\r5 90 -0.159 -2.03 0.0443 159 -0.306 -0.00417 Pears~\r6 6 0.148 1.89 0.0610 159 -0.00685 0.296 Pears~\r7 9 0.141 1.79 0.0747 159 -0.0141 0.289 Pears~\r8 97 0.133 1.70 0.0914 159 -0.0216 0.282 Pears~\r9 51 -0.127 -1.61 0.109 159 -0.276 0.0285 Pears~\r10 89 -0.124 -1.57 0.118 159 -0.273 0.0316 Pears~\r# ... with 90 more rows, and 1 more variable: alternative \u0026lt;chr\u0026gt;\r(y \u0026lt;- run_correlations(sev_bins, \u0026quot;bin\u0026quot;, \u0026quot;mean_est\u0026quot;) %\u0026gt;% arrange(p.value))\r# A tibble: 100 x 9\r# Groups: id [100]\rid estimate statistic p.value parameter conf.low conf.high method\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; 1 99 0.936 4.62 0.0191 3 0.312 0.996 Pears~\r2 35 -0.936 -4.61 0.0192 3 -0.996 -0.309 Pears~\r3 28 -0.907 -3.72 0.0337 3 -0.994 -0.122 Pears~\r4 41 0.845 2.74 0.0715 3 -0.146 0.990 Pears~\r5 68 0.836 2.64 0.0780 3 -0.177 0.989 Pears~\r6 5 -0.804 -2.34 0.101 3 -0.987 0.269 Pears~\r7 6 0.796 2.28 0.107 3 -0.290 0.986 Pears~\r8 10 0.786 2.20 0.115 3 -0.314 0.985 Pears~\r9 89 -0.761 -2.03 0.135 3 -0.983 0.369 Pears~\r10 13 -0.758 -2.01 0.138 3 -0.983 0.376 Pears~\r# ... with 90 more rows, and 1 more variable: alternative \u0026lt;chr\u0026gt;\rmean(x$p.value \u0026lt; .05)\r[1] 0.05\rmean(y$p.value \u0026lt; .05)\r[1] 0.03\rWhen we calculated a correlation with the mean estimates we actually got a significant result from a few of our runs.\rIf fact, about 5% or less are statistically significant…\rLet’s pull that one out to look at it again.\nsignificant_ids \u0026lt;- x %\u0026gt;% filter(p.value \u0026lt; .05) %\u0026gt;% pull(id) %\u0026gt;% as.character()\rtemp \u0026lt;- sev_bins %\u0026gt;% filter(id %in% significant_ids)\rsev_random %\u0026gt;% filter(id %in% significant_ids) %\u0026gt;% ggplot(aes(x = actual, y = estimate, group = factor(id), color = factor(id))) +\rgeom_point(alpha = .2) +\rgeom_point(data = temp, aes(x = bin, y = mean_est)) +\rgeom_line(data = temp, aes(x = bin, y = mean_est))\rSo there you have it.\rA successful replication of this ‘effect’ with random data.\nBut why is this?\rThis is partly because individuals at the lowest quantiles will have a greater likelihood of over-estimating their performance and those at the highest quantiles will underestimate.\rAn individual that performs at the 99th quantile will have almost no choice but to estimate their performance to be below that of reality (see also (Nuhfer et al. 2016)).\rThis seems to be further worsened by the bound nature of the scores.\rWere these scores and estimates to be something not bound in such a way (for instance the speed in which an individual could complete an assessment) examning the relationship between actual and estimate performance could yield more valid results.\rThese graphical representations and analyses should be cautioned as they are not very meaningful to understanding their effects.\n\rReferences\rDouglas, Adams. 1989. The Hitchhiker’s Guide to the Galaxy. New York: Harmony Books.\n\rNuhfer, Edward, Christopher Cogan, Steven Fleisher, Eric Gaze, and Karl Wirth. 2016. “Random Number Simulations Reveal How Random Noise Affects the Measurements and Graphical Portrayals of Self-Assessed Competency.” Numeracy: Advancing Education in Quantitative Literacy 9 (1). https://doi.org/10.5038/1936-4660.9.1.4.\n\rSheldon, Oliver J, David Dunning, and Daniel R Ames. 2014. “Emotionally Unskilled, Unaware, and Uninterested in Learning More: Reactions to Feedback About Deficits in Emotional Intelligence.” Journal of Applied Psychology 99 (1): 125. https://doi.org/10.1037/a0034138.\n\r\r\r","id":1,"section":"posts","summary":"Note: I originally wrote this in February 2019.\rAlso, don’t hate me for putting some links to Wikipedia, you were going to go there anyway.\rThis isn’t a research paper.\n\rIntroduction\rIn my Organization Psychology graduate class at West Chester University, one of our assigned readings (among others) for our week on emotions and moods was (Sheldon, Dunning, and Ames 2014).\rThis article focused on another finding relating to the Dunning-Kruger effect in the workplace.","tags":["psychology","datavis"],"title":"Dunning Kruger Effect","uri":"https://jmbarbone.github.io/2020/05/dunning-kruger/","year":"2020"},{"content":"\r\rIt’s Alive\rFirst post. Just testing, right?\n\r","id":2,"section":"posts","summary":"\r\rIt’s Alive\rFirst post. Just testing, right?\n\r","tags":null,"title":"It's Alive","uri":"https://jmbarbone.github.io/2020/04/its-alive/","year":"2020"}],"tags":[{"title":"benchmark","uri":"https://jmbarbone.github.io/tags/benchmark/"},{"title":"datavis","uri":"https://jmbarbone.github.io/tags/datavis/"},{"title":"psychology","uri":"https://jmbarbone.github.io/tags/psychology/"},{"title":"R","uri":"https://jmbarbone.github.io/tags/r/"}]}