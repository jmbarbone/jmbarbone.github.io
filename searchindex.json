{"categories":[{"title":"paper","uri":"/categories/paper/"},{"title":"presentation","uri":"/categories/presentation/"},{"title":"r code","uri":"/categories/r-code/"}],"posts":[{"content":"\r\rIt’s Alive\rFirst post. Just testing, right?\n\r","id":0,"section":"posts","summary":"\r\rIt’s Alive\rFirst post. Just testing, right?\n\r","tags":null,"title":"It's Alive","uri":"/2020/04/its-alive/","year":"2020"},{"content":"\r\r\rNote: I originally wrote this in February 2019.\n\rFull code found here:\nIntroduction\rIn my Organization Psychology graduate class at West Chester University, one of our assigned readings (among others) for our week on emotions and moods was (Sheldon, Dunning, and Ames 2014).\nThe Dunning-Kruger effect is founding on the concept that an individual that lacks expertise will be more confident in their abilities than they really are, or overestimate their performance on a task.\rYet experts may underestimate their own performance or abilities or be more accurate in their estimations.\rOne thing we can derive from this is possibly that those with lower skill will overestimate their abilities while those more skilled will underestimate their abilities.\nThe following is in direct relation to an article by (Sheldon, Dunning, and Ames 2014).\rThey report a significant relationship between an individual’s actual performance and the difference between their perceived ability and actual performance in three conditions (\\(r_1 = -0.83\\), \\(p_1 \u0026lt; .001\\); \\(r_2 = -0.87\\), \\(p_2 \u0026lt; .001\\); \\(r_3 = -0.84\\), \\(p_3 \u0026lt; .001\\)).\nThey also used these two graphs to representing their findings:\nFigure 1. Overestimation of emotional intelligence (left panel) and performance on the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT; right panel) as a function of actual performance on the MSCEIT\n\rWe’ll go through and understand why this can be misleading and how to replicate the Dunning-Kruger effect with random data.\n\rSet up\rSo let’s place with some random data and see what we get.\rFirst, let’s setup our RMD file and choose a specific randomization seed so we can come back to our results:\noptions(tidyverse.quiet = TRUE) ## silences warnings\rlibrary(tidyverse)\rlibrary(jordan) ## percentile_rank() | github.com/jmbarbone/jordan\rtheme_set(theme_minimal())\r\rRandom data\rWe’ll start by creating a data frame with two vectors of independent, random data.\rThese will be our randomly assigned percentile ranks of actual and estimate’d performance.\nTo clarify, the calculation of percentile rank is as follows:\n\\[\\text{PR}_i = \\frac{c_\\ell + 0.5 f_i}{N} \\times 100%\\]\nWhere \\(c_\\ell\\) is the count of scores lower than the score of interest, \\(f_i\\) is the frequency of the score of interest, and \\(N\\) is the total number of scores.\rWith this formula, our percentile ranks will always be 0 \u0026lt; \\(PR_i\\) \u0026lt; 100.\nrandom_data \u0026lt;- tibble(actual = rnorm(1000),\restimate = rnorm(1000)) %\u0026gt;% mutate_all(percentile_rank)\rrandom_data\r# A tibble: 1,000 x 2\ractual estimate\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 92.0 98.8 2 29.2 70.4 3 65.6 84.6 4 73.8 64.6 5 67.0 15.0 6 46.2 27.4 7 94.6 57.6 8 46.4 0.05\r9 97.9 19.8 10 47.9 79.8 # ... with 990 more rows\rWe also want to bin our data together just like in the article.\nbins \u0026lt;- random_data %\u0026gt;% mutate(difference = estimate - actual,\rbin = ntile(actual, 5) * 20 - 10) %\u0026gt;% group_by(bin) %\u0026gt;% summarise(n = n(),\rmean = mean(difference))\rbins\r# A tibble: 5 x 3\rbin n mean\r\u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r1 10 200 39.8 2 30 200 20.8 3 50 200 1.20\r4 70 200 -21.5 5 90 200 -40.2 \rNow we’ll plot the data and take a look at this.\nggplot(random_data, aes(x = actual, y = estimate - actual)) +\rgeom_point(alpha = .2) +\rgeom_smooth(formula = \u0026quot;y ~ x\u0026quot;, method = lm, se = FALSE, col = \u0026quot;red\u0026quot;) +\rgeom_point(data = bins, aes(x = bin, y = mean), col = \u0026quot;blue\u0026quot;, shape = 1, size = 5) +\rgeom_line(data = bins, aes(x = bin, y = mean), col = \u0026quot;blue\u0026quot;, size = 1) +\rgeom_hline(yintercept = 0, linetype = 2) +\rlabs(title = \u0026quot;Independent random samples of \u0026#39;Actual\u0026#39; and \u0026#39;Estimate\u0026#39; performance\u0026quot;,\rx = \u0026quot;\u0026#39;Actual\u0026#39; performance (Percentile Rank)\u0026quot;,\ry = \u0026quot;Percentile Overestimation\\n(estimate - actual)\u0026quot;)\rAlready we’re seeing a trend very similar to that reported in the article.\rWhat we also notice is that there are bounds to the overestimation value as a factor of the individual’s actual performance.\rAn individual that performs at the 99th percentile cannot overestimate their own performance (but can be accurate) - much like an individual in the lower percentiles would unlikely underestimate.\rThese is additionally worsed by the use of a score derived in reference to others.\n\rAdjusting random data\rSo now we’re going to take some data and use some rough estimates for means.\rWe’ll use the results from the study of interest.\nWe’ll shape our normal distributions around the values found in the paper.\npercentile_rank \u0026lt;- function(x) {\rfloor(percent_rank(x) * 100)\r}\radj_random \u0026lt;- tibble(actual = rnorm(1000, 50, sd = 20),\restimate = rnorm(1000, 75, sd = 15)) %\u0026gt;%\rmutate_all(percentile_rank)\radj_random\r# A tibble: 1,000 x 2\ractual estimate\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 44 50\r2 22 80\r3 36 62\r4 63 43\r5 2 36\r6 17 44\r7 59 78\r8 14 57\r9 84 29\r10 16 40\r# ... with 990 more rows\radj_bins \u0026lt;- adj_random %\u0026gt;% mutate(difference = estimate - actual,\rbin = ntile(actual, 5) * 20 - 10) %\u0026gt;% group_by(bin) %\u0026gt;% summarise(n = n(),\rmean = mean(difference))\rLet’s also take a look at the correlations we have.\rAs expected, we have no correlation with random data.\rThe article reported correlations of .20 and .19 between estimate and actual performance.\rClearly, people are not that great at estimating their own performance.\ncor.test(adj_random$actual, adj_random$estimate)\r\rPearson\u0026#39;s product-moment correlation\rdata: adj_random$actual and adj_random$estimate\rt = 1.3821, df = 998, p-value = 0.1673\ralternative hypothesis: true correlation is not equal to 0\r95 percent confidence interval:\r-0.01833604 0.10541419\rsample estimates:\rcor 0.04370672 \rNow let’s run a correlation on the actual scores and the difference between the estimated and actual performance.\ncor_test_result \u0026lt;- cor.test(adj_random$actual, adj_random$estimate - adj_random$actual)\rcor_test_result\r\rPearson\u0026#39;s product-moment correlation\rdata: adj_random$actual and adj_random$estimate - adj_random$actual\rt = -30.239, df = 998, p-value \u0026lt; 2.2e-16\ralternative hypothesis: true correlation is not equal to 0\r95 percent confidence interval:\r-0.7225030 -0.6576813\rsample estimates:\rcor -0.6914815 \rInteresting…\rSo we already have a significant correlation – roughly similar to those reported by in this article.\rThis is with data that has absolutely no relationship between the two variables.\n\rPlotting adjusted random data\rLet’s graph out our results with a little more care this time.\nggplot(adj_random, aes(x = actual, y = estimate - actual)) +\rgeom_hline(yintercept = 0, linetype = 2) +\rgeom_point(alpha = .1) +\rgeom_smooth(formula = \u0026quot;y ~ x\u0026quot;, method = \u0026quot;lm\u0026quot;, se = F, col = \u0026quot;red\u0026quot;) +\rgeom_point(data = adj_bins, aes(x = bin, y = mean), col = \u0026quot;blue\u0026quot;, shape = 1, size = 5) +\rgeom_line(data = adj_bins, aes(x = bin, y = mean), col = \u0026quot;blue\u0026quot;, size = 1) +\rlabs(title = \u0026quot;Randomly generated differences in \u0026#39;actual\u0026#39; vs \u0026#39;estimated\u0026#39; performance\u0026quot;,\rsubtitle = \u0026quot;Estimate: M = 75, SD = 15; Actual: M = 5, SD = 25\u0026quot;,\rx = \u0026quot;Actual performance\u0026quot;,\ry = \u0026quot;Estimated - Actual performance\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;,\rlabel = str_c(\u0026quot;r = \u0026quot;, round(cor_test_result$estimate, 3)),\rx = 75, y = 50, hjust = \u0026quot;left\u0026quot;)\r\rMore random data\rSo what if we repeated this several times but with smaller sample sets?\nrandom_helper \u0026lt;- function(x) {\rset.seed(42 + x)\rtibble(actual = rnorm(60, 50, sd = 25),\restimate = rnorm(60, 75, sd = 15)) %\u0026gt;%\rmutate_all(percentile_rank)\r}\rsev_random \u0026lt;- as.list(1:5) %\u0026gt;% map(random_helper) %\u0026gt;% bind_rows(.id = \u0026quot;id\u0026quot;) %\u0026gt;% mutate(id = as.numeric(id))\rsev_bins \u0026lt;- sev_random %\u0026gt;% group_by(id) %\u0026gt;% mutate(difference = estimate - actual,\rbin = ntile(actual, 5) * 20 - 10) %\u0026gt;% group_by(id, bin) %\u0026gt;% summarise(n = n(),\rmean_est = mean(estimate),\rmean_diff = mean(difference))\rggplot(sev_bins, aes(x = bin, y = mean_est, col = factor(id))) +\rgeom_point() +\rgeom_line() +\rscale_color_discrete(name = \u0026quot;Randomization\u0026quot;) +\rscale_y_continuous(limits = c(0, 99)) + labs(x = \u0026quot;Actual\u0026quot;,\ry = \u0026quot;Estimate\u0026quot;)\rSo what if we actually run a correlation on these numbers?\rWe’ll create a nested function and install the broom package to help tidy up our results.\nlibrary(broom)\rrun_correlations \u0026lt;- function(x, item_x, item_y) {\rcorr_helper \u0026lt;- function(x, item_x, item_y) {\rformula \u0026lt;- str_c(\u0026quot;~\u0026quot;, item_x, \u0026quot;+\u0026quot;, item_y, sep = \u0026quot; \u0026quot;)\rcor.test(eval(parse(text = formula)), data = x)\r}\rx %\u0026gt;% nest(data = -id) %\u0026gt;% mutate(corr = map(data, corr_helper, item_x, item_y),\rtidy = map(corr, tidy)) %\u0026gt;% unnest(tidy) %\u0026gt;% select_if(negate(is.list))\r}\rrun_correlations(sev_random, \u0026quot;actual\u0026quot;, \u0026quot;estimate\u0026quot;)\r# A tibble: 5 x 9\rid estimate statistic p.value parameter conf.low conf.high method\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; 1 1 0.172 1.33 0.189 58 -0.0856 0.408 Pears~\r2 2 -0.0333 -0.254 0.801 58 -0.285 0.223 Pears~\r3 3 0.149 1.15 0.256 58 -0.109 0.388 Pears~\r4 4 0.0419 0.319 0.751 58 -0.214 0.293 Pears~\r5 5 -0.137 -1.05 0.296 58 -0.378 0.121 Pears~\r# ... with 1 more variable: alternative \u0026lt;chr\u0026gt;\rrun_correlations(sev_bins, \u0026quot;bin\u0026quot;, \u0026quot;mean_est\u0026quot;)\r# A tibble: 5 x 9\r# Groups: id [5]\rid estimate statistic p.value parameter conf.low conf.high method\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; 1 1 0.413 0.785 0.490 3 -0.738 0.949 Pears~\r2 2 -0.648 -1.47 0.237 3 -0.974 0.547 Pears~\r3 3 0.931 4.42 0.0215 3 0.273 0.996 Pears~\r4 4 0.251 0.450 0.683 3 -0.811 0.928 Pears~\r5 5 -0.554 -1.15 0.333 3 -0.965 0.642 Pears~\r# ... with 1 more variable: alternative \u0026lt;chr\u0026gt;\rInteresting.\rWhen we calculated a correlation with the mean estimates we actually got a significant result from one of our random samples, id 3.\rLet’s pull that one out to look at it again.\nggplot(filter(sev_random, id == 3), aes(x = actual, y = estimate)) +\rgeom_point(alpha = .2) +\rgeom_point(data = filter(sev_bins, id == 3), aes(x = bin, y = mean_est)) +\rgeom_line(data = filter(sev_bins, id == 3), aes(x = bin, y = mean_est))\rSo there you have it.\rA successful replication of this ‘effect’ with random data.\rFor more information see articles by (Nuhfer et al. 2016).\n\rReferences\rNuhfer, Edward, Christopher Cogan, Steven Fleisher, Eric Gaze, and Karl Wirth. 2016. “Random Number Simulations Reveal How Random Noise Affects the Measurements and Graphical Portrayals of Self-Assessed Competency.” Numeracy: Advancing Education in Quantitative Literacy 9 (1). https://doi.org/10.5038/1936-4660.9.1.4.\n\rSheldon, Oliver J, David Dunning, and Daniel R Ames. 2014. “Emotionally Unskilled, Unaware, and Uninterested in Learning More: Reactions to Feedback About Deficits in Emotional Intelligence.” Journal of Applied Psychology 99 (1): 125. https://doi.org/10.1037/a0034138.\n\r\r\r","id":1,"section":"posts","summary":"Note: I originally wrote this in February 2019.\n\rFull code found here:\nIntroduction\rIn my Organization Psychology graduate class at West Chester University, one of our assigned readings (among others) for our week on emotions and moods was (Sheldon, Dunning, and Ames 2014).\nThe Dunning-Kruger effect is founding on the concept that an individual that lacks expertise will be more confident in their abilities than they really are, or overestimate their performance on a task.","tags":["psychology","datavis"],"title":"Dunning Kruger Effect","uri":"/1/01/dunning-kruger/","year":"0001"},{"content":"\r\rThis is bit of a write up for my graduate class in psychometrics.\rI thought it would be fun to show the process by which my class designed and tested a scale.\rI use those accents because this was a class activity and we completed this in a summer semester.\rThis isn’t a valid measure, so please don’t use it.\nIntroduction\rControlling behaviors have the potential to impact group activities and projects in a disruptive manner.\rNatural leadership may be subverted by another actor’s desire to maintain power over the group for any number of reasons.\rDetermining an estimate of an individual’s control would be beneficial in identifying those who may have the potential to cause negative outcomes and proactively remedying or preventing an issue from manifesting.\nMotivations for behind seeking control can come from different initial factors.\rBurger and Cooper compiled a small set of theoretical approaches and statements understanding the reason for seeking control and power (1979).\rAn individual may be motivated by wanting to demonstrate their own competence in their life.\rAnother may be motivated because of power or ability.\rControl can also be sought as means to end stress and taking autonomy of one’s life.\nPeople who display a strong sense of control are not always viewed favorably by those they wish to act upon.\nSuggestions of personal gain or a superiority complex are easily expressed when expertise or entitlement is not readily known.\rSome leaders fall into the role while others actively seek it.\rMore accepted forms of leadership do not build upon an open foundation of manipulation but by making sacrifices that are for the best of the group.\rUnselfish acts may be modulated by the leader’s own empathy and the consideration for each other individual affected by the decision.\rThreats to leadership can be provoke protective responses.\rWhen an alarming situation arises, acts of aggression could follow to preserve status.\rAn uninvolved role would not be as beneficial if action is required.\nWe have set out to create a scale to measure the degree of control an individual possesses in relation to social situations and to tests its internal reliability and validation against like, unlike, and unrelated measures.\nScale Testing\r\r\rMethods\rParticipants\rGraduate students currently enrolled in a graduate level psychometrics class at West Chester University of Pennsylvania (n = 11) volunteer to complete the assessment.\rAdditional subjects were recruited by the students (n = 8) to increase the sample size.\rParticipants filled out scantron sheets with the unidentified scales.\rOrder effects were not accounted for. No demographic information was obtained to maintain anonymity with a small sample size and convenience in sampling.\n\rInstruments\rScale for the Need of Control in Social Settings (SNCSS).\rThe SNCSS scale consists of 31 items with a maximum score of 155.\rEach item is scored on a Likert scale from Strongly Disagree to Strongly Agree with reverse scoring on select items.\rItems were developed after an in-class discussion of defining the construct of control.\rA prior literary search was not made before this intuition driven discussion.\rIn initially 50 items were developed and 19 removed due either not maintaining a strong face validity for the construct of control or redundancy.\rAll students in the graduate level psychometrics class contributed to refining items and removing items with the aid of the professor.\n\rAdaptive and Aggressive Assertiveness Scale (AAA-S)\rThe AAA-S (Thompson \u0026amp; Berenbaum, 2011) consists of 19 situations with a possible response or responses which are rated from a 5-point scale of Never to Always, with non-labeled intermediate values, for the extent to which the response best describes how the test taker would response.\rThe scale has a total of 30 responses posed; 15 measuring degree of Aggressive Assertiveness and 15 measuring degree of Adaptive Assertiveness.\rIn a sample of 261 psychology students, the developers reported α coefficient value of .82 and Spearman-Brown reliability of .79 for Adaptive assertive; and an α coefficient of .87 and Spearman-Brown reliability of .71 for Aggressive assertiveness.\n\rThe Basic Empathy Scale in Adults (BES-A)\rThe BES-A (Carré et al., 2013) is a 20-item measure to assess empathy specifically in adults developed from the BES for children and adolescence.\rThe scale consists of an 11-item sub scale for Affective empathy and a 9-item sub scale for Cognitive empathy. Each item is answered on a 5-point scale Likert scale.\rWith a sample of 370 participants drawn from psychology and social science students, employees, and retired persons, the scale authors reported a Cronbach α of .84 and reliability coefficient of .637 for the Affective empathy subscale; and a Cronbach α of .71 and reliability coefficient of .373 for the Cognitive empathy subscale.\n\rLeadership Self-Report Scale (LSRS)\rThe LLS (Dussault et al., 2013) is a 46-item questionnaire to measure leadership factors. The scale consists of the following factors: Transformational leadership (25 items); Transactional leadership (15 items); and Laissez-faire (6 items).\rThe Transformational leadership factor is composing of three scales: Charisma factor (8-items); Intellectual stimulation (8-items); and individual stimulation (8 items).\rThe Transactional leadership factor is composed of two scales: Contingent reward (8 items); and Management-by-exception (7 items).\rEach item is rated on a 4-point scale from Strongly Disagree to Strongly Agree.\rHigher scores on the total scale and each subscale suggest higher levels of leadership.\rDussault and colleagues reported a Cronbach α reliability of .85 for the Transformational leadership scale, .78 for the Transactional leadership scale, and .65 for the Laissez-faire scale, with a sample of 222 participants from Quebec French schools.\n\rThe Narcissism Personality Inventory\rNPI-16 (Ames et al., 2006) is an abbreviated version of the original 40 item Narcissism Personality Inventory.\rEach item poses a pair of statements to choose between based on which statement best aligns with the test taker’s feelings and beliefs about themselves.\rIn the validation study with a sample of 776 undergraduate psychology students, the developers reported an alpha of .72 with the NPI-16 and an alpha of .84 with the full length 40-item NPI.\rThe NPI-16 was correlated with the unused 24 items of the NPI-40 at r = .71.\n\r\r\rResults \u0026amp; Data Analysis\rData Wrangling\rlibrary(psychometric) ## psychometric analyses\rlibrary(broom) ## tidying up analysis results\rlibrary(janitor) ## clean_names() is a life saver\rlibrary(tidyverse) ## general data manipulation\rlibrary(kableExtra) ## better html outputs\rWhen loading data I have a preference for assigning col_types for each variable.\rThis helps in 2 ways:\r1. I have to look at each column and assert the correct type\r1. If there is an error in assignment, a warning will appear\r1. If a new column is added it will throw errors - preventing downstream accidents\nThe third point is very import for scripts and programs that rely on constantly updating data, but not very important for this.\rNone-the-less, it’s now a habit.\ncsv \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/jmbarbone/psychometrics-class/master/sncss.csv\u0026quot;,\rcol_types = list(\rid = col_character(),\rname = col_character(),\rscale = col_character(),\rdomain = col_character(),\ritem = col_integer(),\rresponse = col_integer()\r))\rcsv\rWe’re also going to want to make a separate table for our total scores.\ntots \u0026lt;- csv %\u0026gt;%\rgroup_by(id, scale, domain) %\u0026gt;%\rsummarise(item = \u0026quot;total\u0026quot;,\rtotal = sum(response))\rtots\rtots %\u0026gt;% ungroup %\u0026gt;% unite(scale_domain, scale, domain) %\u0026gt;% ## Previously `name`\rselect(-item) %\u0026gt;% pivot_wider(names_from = scale_domain,\rvalues_from = total)\rAnd lastly we want to use the broom package to create a tibble which contains some item analysis.\rWe’ll write a little helper function to\nitem_example_helper \u0026lt;- function(x) {\rx %\u0026gt;% pivot_wider(names_from = item,\rvalues_from = response) %\u0026gt;% # spread(item, response) %\u0026gt;% select(-id, -scale, -domain) %\u0026gt;% psychometric::item.exam() %\u0026gt;% janitor::clean_names() %\u0026gt;% as_tibble(rownames = \u0026quot;item\u0026quot;) %\u0026gt;% select_if(~ !all(is.na(.)))\r}\rie_tib \u0026lt;- csv %\u0026gt;% nest(-name) %\u0026gt;%\rmutate(item_exam = map(data, item_example_helper)) %\u0026gt;%\runnest(item_exam)\r# kableExtra::kable(ie_tib) %\u0026gt;%\r# kable_styling() %\u0026gt;%\r# scroll_box(width = \u0026quot;100%\u0026quot;, height = \u0026quot;500px\u0026quot;)\rie_tib %\u0026gt;%\rfilter(name == \u0026quot;CON_full\u0026quot;) %\u0026gt;%\rselect(name, item, item_tot_woi) %\u0026gt;%\rarrange(desc(item_tot_woi))\rkeep_items \u0026lt;- ie_tib %\u0026gt;%\rfilter(name == \u0026quot;CON_full\u0026quot;,\ritem_tot_woi \u0026gt; .25) %\u0026gt;% pull(item)\rkeep_items\r\rReliability\r\\[\\text{Cronbach}\\ \\alpha = \\left( \\frac{k}{k-1} \\right) \\left(1 - \\frac{\\sum \\sigma^2_i}{\\sigma^2_X} \\right)\\]\n\\(k\\) = number of items on test\n\\(\\sigma^2_i\\) = variance of the item score of interest\n\\(\\sigma^2_X\\) = variance of the total score\nAn item analysis was performed on the 31 original items using the psych and psychometric packages.\rThe analysis yielded a Cronbach’s α of .735.\rA threshold for item reliability of .250 was set to determining whether an item would be removed or not.\rOf the 31 items, 14 yielded reliability coefficients below this threshold and were subsequently removed.\rA second item analysis was performed with the remaining 17 items.\rAlthough item 30 in the second analysis showed a correlation coefficient of r = .182, removal of the item would have had only a slight positive effect on Cronbach’s α; an increase from .808 to .814 if deleted.\rThe question itself, “I like to micromanage others” was agreed to have a strong face value and retained for the SNCSS-17.\ncsv %\u0026gt;%\rfilter(scale == \u0026quot;CON\u0026quot;) %\u0026gt;%\rpivot_wider(names_from = item,\rvalues_from = response) %\u0026gt;% select(-c(id:domain)) %\u0026gt;% ## requires wide format\rcronbach %\u0026gt;%\rpluck(\u0026quot;Alpha\u0026quot;)\rcsv %\u0026gt;%\rfilter(scale == \u0026quot;CON\u0026quot;,\ritem %in% keep_items) %\u0026gt;% ## keeps in specific items\rpivot_wider(names_from = item,\rvalues_from = response) %\u0026gt;% select(-c(id:domain)) %\u0026gt;% cronbach %\u0026gt;%\rpluck(\u0026quot;Alpha\u0026quot;)\r## add in the 17-item \u0026quot;short\u0026quot;\rprocessed \u0026lt;- csv %\u0026gt;% filter(scale == \u0026quot;CON\u0026quot;,\ritem %in% keep_items) %\u0026gt;% mutate(name = \u0026quot;CON_Short\u0026quot;,\rdomain = \u0026quot;Short\u0026quot;) %\u0026gt;% bind_rows(csv)\rprocessed\r\rValidity\rInternal consistency coefficients were computed for each factor/subscale: (BES-A) Affective Empathy; (BES-A) Cognitive Empathy; (AAA-S) Adaptive Assertiveness; AAA-S Aggressive Assertiveness; NPI.\rAs the sample size was limited, a more generous α = .15 was decided for measuring statistical significance.\rThe results of these analyses showed only one large difference in alpha coefficients.\rThe Laissez-faire subscale of the Leadership Self-Report Scale was originally reported to have a coefficient .65 by the developers but was found to have a much lower .352 in the present study.\rThe Laissez-faire subscale’s low correlation and significance (r = -.002, p = .993) is not surprising given its lowered alpha coefficient, low number of items, and small sample.\ncronbach_helper \u0026lt;- function(x) {\rres \u0026lt;- x %\u0026gt;% pivot_wider(id_cols = c(id, scale, domain),\rnames_from = item,\rvalues_from = response) %\u0026gt;% select(-c(id:domain)) %\u0026gt;% cronbach()\rtibble(alpha = res$Alpha,\rn = res$N)\r}\rcsv %\u0026gt;% nest(scores = -name) %\u0026gt;% mutate(scores = map(scores, cronbach_helper)) %\u0026gt;% unnest(scores)\rItems dropped\r\rItem 4 of the BES-A Affective Empathy scale “I get frightened when I watch characters in a good scary movie” was removed as it was deleted from the original validation because of its weakness in a two-factor and three-factor model (Carré et al., 2013).\rThe following items from the AAA-S were removed from this analysis as they were also removed from the original study due to low factor loadings – values below .3: 4a, 4b, 7b, 9a, 13b, and 16b.\rItem 11a was removed as an item 11b was noted in the original article but not on the scale\r14a was removed for a similar reason as well\rItems 13a (“If I am at a performance and someone keeps talking loudly, I … Would tell the person to shut up.”) and 16a (“If someone cuts in line ahead of me at the moves, I … Start making loud comments about how rude the person is.”) were both labeled as adaptive responses which appeared incorrect. As it was not easily determined whether there was an error in numbering or labeling, these items were removed.\r\r\rConvergent Validity\rThe AAA-S Aggressive assertiveness and NPI-16 were chosen to have expected positive correlations with the SNCSS-17 on the proposed concept that individuals more likely to need or assume control would have greater or more frequency aggressive behaviors and greater views of themselves.\rTable 2 summarized these results.\rLow and non-statistically significant correlations were found for Aggressive assertiveness and the NPI-16 (r = .289, p = .230; r = .271, p = .262).\nThe BES-A Affective empathy and Cognitive empathy, AAA-S Adaptive assertiveness, and Leadership Self-Report Scale Laissez-faire were expected to negatively correlate with the SNCSS-17.\rA necessity for control was originally viewed as requiring less empathetic considerations.\rBehaviors and attitudes underlining a desire for control was not recognized as a demonstration of an adaptive or uninvolved leader but more so a destructive and demanding one.\rA significant and strong positive correlation was found for the Adaptive assertiveness (r = .493, p = .032) but low correlation and significance were found for Affective empathy (r = .325, p = .174), Cognitive empathy (r = .297, p = .174), and Laissez-faire (r = -.002, p = .993).\ncon_short \u0026lt;- processed %\u0026gt;% filter(name == \u0026quot;CON_Short\u0026quot;) %\u0026gt;% select(-name, -domain) %\u0026gt;%\rmutate(scale = \u0026quot;CON_Short\u0026quot;)\rcor_test_helper \u0026lt;- function(x) {\rx_scale \u0026lt;- unique(x$scale)\rcon_short \u0026lt;- get0(\u0026quot;con_short\u0026quot;)\rdata \u0026lt;- x %\u0026gt;% bind_rows(con_short) %\u0026gt;% ## check\rgroup_by(id, scale) %\u0026gt;% summarise(total = sum(response)) %\u0026gt;% pivot_wider(names_from = scale,\rvalues_from = total)\rf \u0026lt;- formula(sprintf(\u0026quot;~ CON_Short + %s\u0026quot;, x_scale))\rcor.test(f, data = data)\r# cor.test(~ CON_Short + eval(parse(text = x_scale)), data = data)\r}\rcsv %\u0026gt;%\rfilter(name != \u0026quot;CON_full\u0026quot;) %\u0026gt;%\rnest(scores = -name) %\u0026gt;% mutate(cor_test = map(scores, cor_test_helper),\rtidy = map(cor_test, list(tidy, clean_names))) %\u0026gt;% unnest(tidy) %\u0026gt;%\rselect(-c(scores, cor_test, method))\rcsv %\u0026gt;%\rfilter(name != \u0026quot;CON_Short\u0026quot;) %\u0026gt;% nest(scores = -name) %\u0026gt;% mutate(cor_test = map(scores, cor_test_helper),\rtidy = map(cor_test, list(tidy, clean_names))) %\u0026gt;% unnest(tidy) %\u0026gt;%\rselect(-c(scores, cor_test, method))\rAffective empathy and cognitive empathy had surprisingly positive correlation coefficients, thus contradicting the initial stance that attempting to acquire control is inversely correlated with an individual’s potential empathy.\rThe thoughts that all desire for control stems from selfish reasonings would need to reexamine.\rParsing out motivations and desires behind control, other than personal power, would help in understanding this trend in the analysis.\nLaissez-faire leadership subscale only contained 6 items, and with a sample size of n = 17, the underwhelming results are of no surprise.\rThe correlation and significance were negligible; thus no substantial considerations can be made.\nThe strong positive correlation of the SNCSS-17 to the AAA-S Adaptive assertiveness subscale was an initial surprise.\rA strong desire for control was not originally viewed as an adaptive trait in leadership. However, adaption could function as a means of maintaining control.\rMiron and Brehm (2006) explain reactance as the emotional-motivational response to situations perceived to constrict choice.\rThe individual reacts to these events with increased efforts to demonstrate their own control.\rAn adaptive individual would better be able to maintain their control in different situations than a non-adaptive or aggressive leader.\rTake for instance item 4b and 12b of the AAA-S, “I am at the grocery store and several of my items ring up incorrectly, I … Ask the cashier to do a price check on the particular items” and “If someone I don’t know well disagrees with me during a conversation, I … Continue elaborating on my opinion until the person understands it.”.\rBoth items demonstrate an adaptive reactance to an event which may be more successful than their alternatives; getting angry.\rAlternative means of a solution (e.g., the item price-check) and persistence or persuasion, would do well for an individually trying to maintain control in a situation.\n\rDiscriminant validity\rTwo subscales from the Leadership Self-Report Scale, Transformational and Transactional leadership, were not hypothesized to have any indication for control.\rA weak and non-significant correlation was found for Transactional leadership (r = .026, p = .915).\rHowever, a significant negative correlation was found for Transformational leadership (r = -.356, p = .135).\nThe results of Transactional leadership subscale confirm the intuition that there would exist no correlation between this measure and the SCNSS-17.\rHowever, the correlation between Transformational leadership and desire for control require some explanation.\rThis measure is based upon self-reports of cheerfully, optimism, and encouragement.\rThese are inspirational factors from a leader.\rGlass and Singer (1972) demonstrate that when faced with pestering, uncontrollable stimuli, an individual’s cognitive ability is impaired and stress is elevated.\rPerhaps our individuals that desire control are negatively affected by the lack of it or are perpetually worried about losing their control.\rTheir stress, from maintaining their position, may factor into their overall expression of joyfulness and transformational values.\ncon_short %\u0026gt;% group_by(id) %\u0026gt;% summarise(scale = \u0026quot;CON\u0026quot;,\rdomain = \u0026quot;Short\u0026quot;,\ritem = \u0026quot;total\u0026quot;,\rtotal = sum(response)) %\u0026gt;% bind_rows(tots) %\u0026gt;% pivot_wider(names_from = c(scale, domain),\rvalues_from = total) %\u0026gt;% pivot_longer(cols = -c(id, item, CON_full),\rnames_to = \u0026quot;scale\u0026quot;,\rvalues_to = \u0026quot;scale_total\u0026quot;) %\u0026gt;% ggplot(aes(x = CON_full, y = scale_total, group = scale, name = scale)) +\rgeom_point(aes(col = scale), show.legend = FALSE) +\rgeom_smooth(formula = \u0026quot;y ~ x\u0026quot;,\rmethod = \u0026quot;lm\u0026quot;) +\rfacet_wrap(~scale, scales = \u0026quot;free\u0026quot;) +\rtheme_minimal() +\rlabs(title = \u0026quot;Correlations of CON Full scale\u0026quot;,\rx = \u0026quot;CON Full Total\u0026quot;,\ry = \u0026quot;Comparison Total\u0026quot;)\r\r\rFactor analysis\rA two component factor analysis was performed on the SCNSS-17 with a oblimin rotation using the gpa package.\rSee Table 3 for a summary.\rResults demonstrated a strong factor loading of 5 items into one component and 8 items into a second component.\rFour items did not show strong factor loading values for one particular component.\rThat is, the factor loading values for these components were below .30 for either component or showed cross loading between the two factors (Costello \u0026amp; Osborne, 2005).\rAfter analyzing each item in relation to these components, the sets were categorized into Motivation items and Ability items.\nThe factor loading analysis showed a suspicion that the items deemed suitable from class discussion and then filtered out with an item analysis, may be the cummulation of more than one factor in a need for control.\rAfter the items were categorized by component, a determination of these components was made.\rAbility items (e.g., “It feels natural for me to lead conversations when working on group projects.”, “I can get my way through talking to people.”) illustrate the possible magnitude of control an individual may posses through either persuasion or a feeling of nature leadership.\rItems from the second component, Motivation (e.g., “When I go on a trip, I like to keep a predetermined schedule.”, “When things don’t go my way, I get upset.”, “Typically, my judgement is better than my peers”) illustrate desires to be involved in planning and making decisions as well reactions to possible threats of losing or not gaining control.\rSeparate analyses had not been performed with these possible subscales.\ndf \u0026lt;- csv %\u0026gt;% filter(name == \u0026quot;CON_full\u0026quot;) %\u0026gt;% pivot_wider(names_from = item,\rvalues_from = response) %\u0026gt;% select(-(id:domain))\rfa_results \u0026lt;- psych::fa(df, nfactors = 2, rotate = \u0026quot;varimax\u0026quot;)\rfa_results$loadings\rplot(fa_results)\r\r\rOther considerations\rBurger and Cooper (1979) developed a 20-item Desirability of Control Scale (DC) which measures an individual’s general desire for control over events.\rThe items were specifically created to related to a direst for control in relation to events in an individual’s environment in a general and specific terms.\rEach statement is rated on a 7-point scale from “This statement doesn’t apply to me at all” to “This statement always applies to me”.\rThe DC was intended to measure Assertion, Decision, and Activity in as factors of desire.\rThe DC has many similar items to the SNCSS-17 and would appear to be a great resource and to determine convergent validity but was unused in the present study as it was obtained only after the analyses were completed.\nOverall, the results from the analysis with a limited sample suggest some relevance to the current measure.\rItems could be modified further or account for more slight redundancies and gearing towards a bifactor consideration for actions involving a leadership role and attitudes and motivations for seeking control.\rAnother relevant scale was identified that could supply greater backing for the validity of the current measure and should be explored.\n\rReferences\rAmes, D. R., Rose, P., \u0026amp; Anderson, C. P. (2006). The NPI-16 as a short measure of narcissism. Journal of Research in Personality, 40(4), 440–450. https://doi.org/10.1016/j.jrp.2005.03.002\n\rBurger, J. M., \u0026amp; Cooper, H. M. (1979). The desirability of control. Motivation and Emotion, 3(4), 381–393. https://doi.org/10.1007/BF00994052\n\rCarré, A., Stefaniak, N., D’Ambrosio, F., Bensalah, L., \u0026amp; Besche-Richard, C. (2013). The basic empathy scale in adults (bes-a): Factor structure of a revised form. Psychological Assessment, 25(3), 679. https://doi.org/10.1037/a0032297\n\rCostello, A. B., \u0026amp; Osborne, J. W. (2005). Best practices in exploratory factor analysis: Four recommendations for getting the most from your analysis. Practical Assessment, Research \u0026amp; Evaluation, 10(7), 1–9. https://doi.org/10.7275/jyj1-4868\n\rDussault, M., Frenette, É., \u0026amp; Fernet, C. (2013). Leadership: Validation of a self-report scale. Psychological Reports, 112(2), 419–436. https://doi.org/10.2466/01.08.PR0.112.2.419-436\n\rGlass, D. C., \u0026amp; Singer, J. E. (1972). Behavioral aftereffects of unpredictable and uncontrollable aversive events. American Scientist, 60(4), 457–465. https://doi.org/10.2307/27843244\n\rMiron, A. M., \u0026amp; Brehm, J. W. (2006). Reactance theory-40 years later. Zeitschrift Für Sozialpsychologie, 37(1), 9–18. https://doi.org/10.1024/0044-3514.37.1.9\n\rThompson, R. J., \u0026amp; Berenbaum, H. (2011). Adaptive and aggressive assertiveness scales (AAA-S). Journal of Psychopathology and Behavioral Assessment, 33(3), 323–334. https://doi.org/10.1007/s10862-011-9226-9\n\r\r\r","id":2,"section":"posts","summary":"This is bit of a write up for my graduate class in psychometrics.\rI thought it would be fun to show the process by which my class designed and tested a scale.\rI use those accents because this was a class activity and we completed this in a summer semester.\rThis isn’t a valid measure, so please don’t use it.\nIntroduction\rControlling behaviors have the potential to impact group activities and projects in a disruptive manner.","tags":["r","psychology","psychometrics","factor analysis"],"title":"Psychometrics (class) scale development","uri":"/1/01/psychometrics-class-paper/","year":"0001"},{"content":"\r\rUntil recently, I hadn’t really had to do any receiver operator characteristic curves (ROC) or areas under them (AUROC).\rAfter stumbling through documentation and a few videos, I found them a lot less intiminating and rather accessible.\rI could also be entirely wrong.\rI hope not.\nWe’re also going to challenge ourselves (myself) here and complete this without the use of the almost obligatory library(tidyverse).\rThis isn’t really anything necessary, but I find it nice to understand some of our base R functions - especially the versatile plot().\nThe code isn’t always the cleanest, so maybe I’ll put together a tidyerse example as well.\n# library(tidyverse) ## Skip to assure ourselves we can do this with \u0026quot;base\u0026quot; R\rsuppressPackageStartupMessages(library(pROC)) ## ROC analyses\rlibrary(caret, quietly = TRUE) ## confusionMatrix()\rdata(MedGPA, package = \u0026quot;Stat2Data\u0026quot;) ## data set\r# med_gpa \u0026lt;- as_tibble(MedGPA)\r# theme_set(theme_minimal())\rThe curve() function is a little bit of a weird one, or at least to me.\rThe function creates a call object with the first argument (which consists entirely of predict(mod, data.frame(GPA = x), type = \"resp\")).\nform \u0026lt;- formula(Acceptance ~ GPA)\rmod \u0026lt;- glm(form, data = MedGPA, family = binomial)\rplot(form, data = MedGPA)\rcurve(predict(mod, data.frame(GPA = x), type = \u0026quot;resp\u0026quot;), add = TRUE)\rmod\r\rCall: glm(formula = form, family = binomial, data = MedGPA)\rCoefficients:\r(Intercept) GPA -19.207 5.454 Degrees of Freedom: 54 Total (i.e. Null); 53 Residual\rNull Deviance: 75.79 Residual Deviance: 56.84 AIC: 60.84\rsummary(mod)\r\rCall:\rglm(formula = form, family = binomial, data = MedGPA)\rDeviance Residuals: Min 1Q Median 3Q Max -1.7805 -0.8522 0.4407 0.7819 2.0967 Coefficients:\rEstimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -19.207 5.629 -3.412 0.000644 ***\rGPA 5.454 1.579 3.454 0.000553 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r(Dispersion parameter for binomial family taken to be 1)\rNull deviance: 75.791 on 54 degrees of freedom\rResidual deviance: 56.839 on 53 degrees of freedom\rAIC: 60.839\rNumber of Fisher Scoring iterations: 4\r# par(mfrow = c(2, 2))\r# plot(mod)\r# par(mfrow = c(1, 1))\rWe have a very (statistically) significant relationship between GPA and Acceptance.\nBut what’s the “cutoff” for GPA?\rCan we determine other sorts of “diagnostic” criteria in here?\nres \u0026lt;- roc(Acceptance ~ GPA,\rdata = MedGPA,\rlevels = c(control = 0,\rcase = 1),\rdirection = \u0026quot;\u0026lt;\u0026quot;)\rres\r\rCall:\rroc.formula(formula = Acceptance ~ GPA, data = MedGPA, levels = c(control = 0, case = 1), direction = \u0026quot;\u0026lt;\u0026quot;)\rData: GPA in 25 controls (Acceptance 0) \u0026lt; 30 cases (Acceptance 1).\rArea under the curve: 0.824\rplot.roc(res,\rlegacy.axes = TRUE,\rasp = NA,\rprint.auc = TRUE,\rprint.thres = \u0026quot;best\u0026quot;,\rprint.thres.best.method = \u0026quot;youden\u0026quot;)\rplot(smooth(res), add = TRUE, col = \u0026quot;blue\u0026quot;, lwd = 2, lty = 2)\rlegend(\u0026quot;bottomright\u0026quot;, legend=c(\u0026quot;Empirical\u0026quot;, \u0026quot;Smoothed\u0026quot;), col=c(par(\u0026quot;fg\u0026quot;), \u0026quot;blue\u0026quot;), lwd=2)\rThe Youden’s J is a simple calculation:\n\\[J = \\text{sensitivity} + \\text{specificity} - 1\\]\n\\[\\text{sensitivity} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}} = \\frac{\\text{true positives}}{\\text{true conditions}}\\]\n\\[\\text{specificity} = \\frac{\\text{true negatives}}{\\text{true negatives} + \\text{false positives}} = \\frac{\\text{true negatives}}{\\text{false conditions}}\\]\nBut this isn’t the only metric.\rplot.roc() also gives us the option to use closest.topleft.\nThis is calculated as:\n\\[\\text{closest top left} = min((1 - \\text{sensitivity})^2 + (1 - \\text{specificity})^2)\\]\nplot.roc(res,\rlegacy.axes = TRUE,\rasp = NA,\rprint.auc = TRUE,\rprint.thres = \u0026quot;best\u0026quot;,\rprint.thres.best.method = \u0026quot;closest.topleft\u0026quot;) ## method changed\rplot(smooth(res), add = TRUE, col = \u0026quot;blue\u0026quot;, lwd = 2, lty = 2)\rlegend(\u0026quot;bottomright\u0026quot;, legend=c(\u0026quot;Empirical\u0026quot;, \u0026quot;Smoothed\u0026quot;), col=c(par(\u0026quot;fg\u0026quot;), \u0026quot;blue\u0026quot;), lwd=2)\rOur value here is now slightly lower, and maybe more pleasing to undergraduates looking at prospective programs.\nWe’ll take a look at our confusion matrix through caret::confusionMatrix() to give us some easy to read values on sensitivity, specificity (which we know from the graph above) as well as overall accuracy, positive predictive value, and negative predictive value.\ntab1 \u0026lt;- table(\rAcceptance = factor(MedGPA$Acceptance, levels = c(1, 0), labels = c(\u0026quot;TRUE\u0026quot;, \u0026quot;FALSE\u0026quot;)),\rGPA_High = factor(MedGPA$GPA \u0026gt; 3.705, levels = c(\u0026quot;TRUE\u0026quot;, \u0026quot;FALSE\u0026quot;)))\rtab1\r GPA_High\rAcceptance TRUE FALSE\rTRUE 17 13\rFALSE 1 24\r(cm1 \u0026lt;- confusionMatrix(tab1))\rConfusion Matrix and Statistics\rGPA_High\rAcceptance TRUE FALSE\rTRUE 17 13\rFALSE 1 24\rAccuracy : 0.7455 95% CI : (0.61, 0.8533)\rNo Information Rate : 0.6727 P-Value [Acc \u0026gt; NIR] : 0.157130 Kappa : 0.5064 Mcnemar\u0026#39;s Test P-Value : 0.003283 Sensitivity : 0.9444 Specificity : 0.6486 Pos Pred Value : 0.5667 Neg Pred Value : 0.9600 Prevalence : 0.3273 Detection Rate : 0.3091 Detection Prevalence : 0.5455 Balanced Accuracy : 0.7965 \u0026#39;Positive\u0026#39; Class : TRUE \rtab2 \u0026lt;- table(\rAcceptance = factor(MedGPA$Acceptance, levels = c(1, 0), labels = c(\u0026quot;TRUE\u0026quot;, \u0026quot;FALSE\u0026quot;)),\rGPA_High = factor(MedGPA$GPA \u0026gt; 3.460, levels = c(\u0026quot;TRUE\u0026quot;, \u0026quot;FALSE\u0026quot;)))\rtab2\r GPA_High\rAcceptance TRUE FALSE\rTRUE 26 4\rFALSE 9 16\r(cm2 \u0026lt;- confusionMatrix(tab2))\rConfusion Matrix and Statistics\rGPA_High\rAcceptance TRUE FALSE\rTRUE 26 4\rFALSE 9 16\rAccuracy : 0.7636 95% CI : (0.6298, 0.8677)\rNo Information Rate : 0.6364 P-Value [Acc \u0026gt; NIR] : 0.03133 Kappa : 0.5153 Mcnemar\u0026#39;s Test P-Value : 0.26726 Sensitivity : 0.7429 Specificity : 0.8000 Pos Pred Value : 0.8667 Neg Pred Value : 0.6400 Prevalence : 0.6364 Detection Rate : 0.4727 Detection Prevalence : 0.5455 Balanced Accuracy : 0.7714 \u0026#39;Positive\u0026#39; Class : TRUE \rLet’s make this a little easier to compare these differences.\rWe can take apart the confusionMatrix objects and put them side-by-side.\nt(rbind(youden = c(cm1$overall, cm1$byClass),\rtopleft = c(cm2$overall, cm2$byClass)))\r youden topleft\rAccuracy 0.745454545 0.76363636\rKappa 0.506410256 0.51525424\rAccuracyLower 0.609971557 0.62980492\rAccuracyUpper 0.853299554 0.86772101\rAccuracyNull 0.672727273 0.63636364\rAccuracyPValue 0.157130233 0.03133271\rMcnemarPValue 0.003283461 0.26725749\rSensitivity 0.944444444 0.74285714\rSpecificity 0.648648649 0.80000000\rPos Pred Value 0.566666667 0.86666667\rNeg Pred Value 0.960000000 0.64000000\rPrecision 0.566666667 0.86666667\rRecall 0.944444444 0.74285714\rF1 0.708333333 0.80000000\rPrevalence 0.327272727 0.63636364\rDetection Rate 0.309090909 0.47272727\rDetection Prevalence 0.545454545 0.54545455\rBalanced Accuracy 0.796546547 0.77142857\rOne interest between these two thresholds is the result of the McNemar’s Chi-squared test conducted on the contingency tables we produced.\rThis is a test for the null hypothesis that the marginal proportions are the same.\rBy this we mean that the test’s false positive and false negative rates are the same.\nTake our first example:\n GPA_High\rAcceptance TRUE FALSE\rTRUE 17 13\rFALSE 1 24\rAnd understand that we will code the conditions as followed:\n GPA_High\rAcceptance TRUE FALSE\rTRUE A B\rFALSE C D\rThe test statistic is derived as:\n\\[\\chi^2 = \\frac{(B - C)^2}{B + C}\\]\nWe can expand this with a continuity correction, which is reported by default in the stats::mcnemar.test() employed in the confusion matrix results.\n\\[\\chi^2 = \\frac{(|B - C| - 1)^2}{B + C}\\]\nWe see a significant difference using the Youden’s J statistic but not from the second threshold generated.\nThe downside to the McNemar test is that it does not take into account the diagonals.\rAnother metric calculated in the confusion matrix function.\rThe accuracy is tested against the larger class proportion.\n","id":3,"section":"posts","summary":"Until recently, I hadn’t really had to do any receiver operator characteristic curves (ROC) or areas under them (AUROC).\rAfter stumbling through documentation and a few videos, I found them a lot less intiminating and rather accessible.\rI could also be entirely wrong.\rI hope not.\nWe’re also going to challenge ourselves (myself) here and complete this without the use of the almost obligatory library(tidyverse).\rThis isn’t really anything necessary, but I find it nice to understand some of our base R functions - especially the versatile plot().","tags":["R","roc","pkg:caret","pgk:pROC"],"title":"Receiver operator characteristic curve","uri":"/1/01/roc/","year":"0001"},{"content":"\r\r\rIntroduction\rWhat is the “Replication Crisis”?\rLarge-scale Replication Attempts\r\rThe Open Science Collaboration\rThe “Many Labs” Project\r\rCase Studies\r\rThe Marshmallow test\rViolence in video games\r\rFuture Directions in Social Psychology\rConcluding Remarks\rReferences\r\r\rAdvanced Social Psychology.\rA project on contemporary topics in Social Psychology.\rI managed to weasel around the requirements (social isn’t quite my field) and present and write on research methodologies, specifically on the “replication crisis” and how it relates to Social Psychology.\nThe xaringan presentation shown below.\rYou can find all the files and the paper in my poorly created psy609 repository.\n\r\r\rIntroduction\rIn the 17th century Robert Boyle’s reported success in observing anomalous suspension of water with his sophisticated air pump was put to question. These findings were difficult to replicate due to the instruments cost and complexity. Christiaan Huygens was able to produce a similar effect with his own device but was met with the same reaction. A debate over validating claims and differentiating between “thought experiments” and actual experiments was birthed. The Royal Society was not satisfied until in 1663 when Huygens was invited to England and able to successfully replicate this phenomenon (Shapin 1984).\nA growing sense of the necessity of replicability and establishing facts that could be observed by others came from these events. Today, to publish in well-respected journals, researchers must describe the methods of their study or experiment well enough for the reader to be able to successfully implement the same procedures and assumingly reach similar conclusions. Yet without formal testing, these might be more akin to the thought experiments Boyle discussed. Replicability is easily – and perhaps suitably – stressed through the words of the others, such as Braude (2002) who comments, “only experiments whose results can be repeated are considered genuine and reliable” and that this can be used as a “demarcation criterion between science and non-science”.\n\rWhat is the “Replication Crisis”?\rSanja Srivastava maintains a blog, “The Hardest Science”, with insights he holds as a professor and researcher in social psychology. His blog is named in part to retaliate against the conceptualization of science existing on a continuum, from the soft (e.g., psychology, sociology) to the hard (e.g., physics) (Srivastava 2009). He argues that each field is equal to the other, scientifically, as they all seek to answer inquiries by applying logic and reasoning to evidence, although the issues undertaken vary. Psychology focuses on understanding complex systems and tries to find patterns and reason to behavior. In this way, the issues psychology faces are more difficult to answer clearly. Therefore, we may want to discontinue describing psychology as a “soft science” and adopt “the hardest science”. This will be an important idea to maintain as issues with psychological replication are put forth and when attention is focused on social psychology and while understanding that these problems are not just unique to this field but may span across the whole of science (Ioannidis 2005; Sterne and Smith 2001).\nOne estimate places the rate of replication in 100 psychology journals at 1.07% of publications (Makel, Plucker, and Hegarty 2012), with rates steadily increasing towards 2.5% in the 2010s. It is easy to consider why this is the case. Conducting a replication can be a tedious, time consuming and resource draining process. The replicating researchers may have to go to additional lengths to understand the methodology of the original researchers to ensure a set-up as similar to the original as possible. Even if the replication is a success, there is a general consensus that journals favor novelty and positive finding.\nMakel and colleagues (2012) also note that the median number of citations for replication articles was estimated at 17 which was lower than the median rate for the original study at 64.5. But 17 citations for an article is still a decent accomplishment; only 3 of the 100 journals examined had 5-year impact factors above 17. This suggests that replication articles are generally well-received by researchers. Publishing companies may want to focus on new research findings but metrics like these may providing a convincing argument that that accepting replication articles might contribute to higher impact factors and attention to journals. Submitting a replication to the journal of the original article may also provide an additional incentive as a reference to the replication would likely be paired with a reference to the original work.\nReplication in psychology is a rare yet rewarded occurrence. However, it is worthwhile to really delve into the necessity of replications. After all, calls and requests for greater replication may require large, consuming endeavors. Psychology has been functioning with a small number of replications for quite some time, so to question the necessity is reasonable.\nPashler and Harris (2012) examined three arguments against the magnitude of the replication crisis. The first argument puts forth the standard 5% likelihood of acceptance of a false null hypothesis and 80% likelihood of rejecting a false null hypothesis. These may seem like safe thresholds, but the false positive rate of published articles is likely much higher than these would insinuate. We can crudely calculate the probability that a positive result is false finding (\\(\\alpha^1\\)) by dividing the proportion of false positives (\\(P_\\alpha\\)) by the sum of the proportion of false positives and the proportion of correct rejections (\\(P_\\beta\\)); i.e., \\(\\alpha^1 = \\frac{P_\\alpha}{P_\\alpha + P_\\beta}\\). Assuming a prior probability of a true effect is 10%, and given a Type I error level of .05, \\(P_\\alpha = .05 * (1 - 10) = .045\\). Given a power level of .80, we would find that \\(\\alpha^1 = .045 / (.045 * .80) = .36\\). We can then conclude that given the standard statistical assumptions, and a prior probability of 10%, the likelihood that a finding is false is 36%. This is a generous assumption that does not take into account other factors that could lead to biased false positives.\nIn his brazenly titled article, “Why most published research findings are false”, Ioannidis (2005) provides a more expansive review of the probability of false positive or exaggerated articles. His formulae include variables for researcher bias and the number of teams involved in a particular field. Using these methods, Iaonnidis identifies six corrollaries to estimated false positive rates. Inversely related are sample sizes and effect sizes. Positively related are number of tests (with less pre-selection of tests); design or methodology flexibility; financial or interest in publishing the finding; and the number teams involved in a particular area. His last corollary seems at first counterintuitive. Ioannidis proposes that the “hotter” the area of research the more pressure researchers have to disseminate their “impressive” findings. Ioannidis comes to the conclusion that, given all these factors, the majority of research – across all fields – are likely to be false or exaggerated reports of effects.\nThe second argument Pashler and Harris (2012) tackle is the notion that despite a lack of direct replication, conceptual replications are more frequent and test not just the validity of the original research but also the generalizability. The authors echo concerns from Ioannidis (2005) that published conceptual replications represent the favored “interesting” findings. If conceptual replications did provide a more rigorous testing of a hypothesis it might reason that the rate of failure here would be higher than it is. Of the estimate 1.07% of publications; direct replications constituting only 14% of these (Makel, Plucker, and Hegarty 2012). The publication rate for failed direct replications (14.6%) is nearly twice that of failed conceptual replications (7.5%). This difference in positive findings could be result of the difficulty of estimating the likelihood that a false finding is due to the original hypothesis being incorrect, that the theory does not generalize to the specific difference, or that the replication was not designed or executed well enough (Earp and Trafimow 2015).\nCohen (1994) provides insight that can be applied in parallel to this the conceptual replication through criticizing thresholds of null hypothesis significance testing (NHST) and outlining a few misconceptions and misuses of statistical analyses. Often the null hypothesis is interpreted as finding no effect rather than the hypothesis which is to be nulled. These tests are not just to compare whether there exists an effect but that levels of effect ought to be compared as well. To highlight a misuse, Cohen states the absurdity of testing rater reliability: that is, testing whichever computed statistic against the null hypothesis that there exists no reliability between or amongst raters; even with a small sample these results would look significant. This fundamental misuse of NHST may then lead researchers to believe that because they have successfully reject a null hypothesis – rather than nullified a previous hypothesis – that their theory must be true (Meehl 1990). Greater understanding of null hypothesis testing may aid in the design and interpretation of direct and conceptual replications.\nScience, we like to think, is self-correcting. Pashler and Harris (2012) challenge this argument last. They note their quick Google Scholar search for replication failures showed that the median replication attempt delay was 4 years1 with 10% of replications occurring longer than 10 years. This is could be taken as a good sign that researchers are quick to scrutinize research with their own attempts. However, Pasher and Harris contend that this may simply represent the “faddish” nature of psychological research and that research which has failed to replicate may have also failed to maintain the herd’s interest. Older, possibly less contemporarily interesting research, should also be provided the service of scrutiny and re-examination.\nWhen 1,500 scientists were surveyed on comments and rates of replications, roughly half of those in psychology (approximately 54 respondents) reported having failed to reproduce their own work or someone else’s work (Baker 2016). Approximately 55% of respondents all failed to replicate their own work and more than 70% failed to replicate another’s; only 16% of these respondents successfully published a failure to reproduce. Self-correction, through publication of failed replications, does not seem to be that great of an argument if these results are all lost to the dreaded file drawer – or, increasingly appropriate, forgotten flash drive.\nReplications, although well-received by researchers, are a bit of a rarity in the literature. However, there also appears to be growing interest as the rates of replications are increasing and as groups are formed to take on this problem.\n\rLarge-scale Replication Attempts\rThe task of reproducibility testing can be performed with single attempts and thorough examination of specific results as they were reported in the original article. Possibly a more effective means of garnering more attention is large-scale replications attempts which tests a great many hypotheses once more or multiple testing of a select few hypotheses. With such a great endeavor, researchers are leveraging cooperation across multiple research sites and labs. Two examples of large-scale replication attempts are described below.\nThe Open Science Collaboration\rThe Open Science Collaboration (OSC) of the Center for Open Science (COS) has promoted interest in replication studies and called for setting standards for replication attempts (Open Science Collaboration 2012). The OSC selected 100 contemporary studies across three journals in psychology and reported the results of their replications in what would become a highly cited and publicized article. Of these 100, 93 originally reported significant findings2, but only 36 replication findings reached the same conclusion. When separating by discipline, the rate of success for cognitive studies was greater than social (21/42 and 14/55, respectively3). This demonstrates a much lower proportion of false positives than 36% of positive findings (Pashler and Harris 2012) and provides support for models suggesting that the bulk of published research is false (Ioannidis 2005). Original studies that reported greater effect sizes and more significant findings were more likely to be replicated than those with smaller effects and less significance, in line with suggestions from Ioannidis (Ioannidis 2005) and presumed correlates of false positives.\nThe COS has made clear possible issues with their findings. They acknowledge that there is no single standard for determining the success of a replication and thus report on several measures that may be taken into account (Open Science Collaboration 2012, 2015). Their selection of articles was not entirely random either, and they acknowledge that there are inherently greater challenges in replicating some psychological research that may rely on a specific population or dependent on an historical event. There is further criticism regarding some changes made in the replications which might invalidate their status as direct replications (Gilbert et al. 2016).\n\rThe “Many Labs” Project\rIn a different method, the “Many Labs” project attempted 36 replications of 13 studies, with the main purpose to understand the variability of replication findings (Klein et al. 2014). This method, although more demanding than the OSC’s attempts, provides much a more comprehensive evaluation of findings reported by studies. Variations in effect sizes compared against original reports and p values from each replication were taken into consideration of replication success. Of the 13 articles tested, 10 of them showed clear indications of successful replications. The articles tested were chosen for their simplicity, ability to be completed both in person or online, and the general level of procedural replicability – similar to the criteria from the OSC.\nKlein and colleagues (2014) also tested the variation of effect sizes found across each lab by computing an intra-class correlation and using an ANOVA across each lab and between online or in-person testing. These showed an acceptable level of agreement (ICC = .75) across study sites and little impact of study location (all \\(\\eta^2_P\\) \u0026lt; .023). The only impact which may have been noted was from better base knowledge in the anchoring tests4.\n\r\rCase Studies\rAlthough these may represent singular instances of less than ideal research, attributing the concepts and ideas above may help provide an example of their effects. Two case studies have been provided below. The first of this will examine a well-known study and some of the limitations in understanding confounding variables. The second takes a look at a divisive field and the possibly of bias and poor data handling.\nThe Marshmallow test\rWalter Mischel’s now famous Stanford marshmallow studies have generally found correlates between the delay of gratification to better life outcomes (Mischel, Shoda, and Rodriguez 1989; Ayduk et al. 2000). A recent conceptual replication identified some criticism of the original studies (Watts, Duncan, and Quan 2018). Namely, that the original children tested were from a highly selected sample in the Stanford University community and the studies also failed to account for possible confounds such as mother’s education and home environment. The sample retained for longitudinal studies were also much lower than their original experiment (35-89 and over 600, respectively). In their replication, Watts, Duncan, and Quan utilized data from the National Institute of Child Health and Human Development (NICHD) Study of Early Child Care and Youth Development (SECCYD). Data included information on a delayed gratification test and behavioral outcomes at age 15. These children were all born of mothers who did not have or complete a college education. Only this group was examined due to concerns with truncation of gratification delay measures from children born to mothers who completed college and because the examined population is more appropriate and of greater interest to policy-makers of developmental interventions.\nIn their analysis, Watts and colleagues were able to show a replication of achievement scores at age 15, although this effect was smaller than in the original studies. They were not able to find significance with delayed gratification and behavioral measures. The significant results they found were also moderated by variables such as child background, home environment, and early cognitive skills – so much so that the interaction of delayed gratification being insignificant. Albeit this is one study that has dampened the relationship of self-control to later life outcomes, it is an important one. Displayed here is the concern of controlling for variables that may not have been of interested to the original researchers. Although the initial and follow-up findings have shown significance and the track record appears sound, revisiting the study itself highlights concerns. Replications like these are necessary for understanding how even ubiquitous findings that seem unanimous in the literature may require additional scrutiny.\n\rViolence in video games\rThere exists a contentious forum between researchers in the field of media whether violent media – specifically video games – contribute to violent acts by individuals. A meta-analysis (Anderson and Bushman 2001) concluded that violent video games increase aggression, physiological arousal, and aggression-related thoughts and feelings – with small but positive relationships. Yet another meta-analysis (Ferguson 2007) found that video games were not linked to aggression after controlling for publication bias. This second meta-analysis also reported on improvement on visuospatial cognition. Here, too, there was an issue with publication bias – but even when adjusting for this, the improvement effect was still significant.\nNow, with conflicting meta-analyses, researchers may be stuck inter canem et lupum when reasoning out how to interpret these findings. There also exists another bias, other than publication, from either side: the first meta-analysis was presented in part by Brad Bushman – a researcher known in this niche for finding ways media and video games influence aggressive behaviors. The second was published by Christopher John Ferguson, a researcher who quite frequently challenges these views.\nAlong with these claims, this area of social psychology has been subjected to a complicated scandal centering around Brad Bushman and a doctoral student of his, Jodie Whitaker. In April 2013, Whitaker and Bushman published their article, “Boom, Headshot!”: Effect of video game play and controller type on firing aim and accuracy online in the journal Communication Research (2012). A data request made because of concerns over appropriateness of statistical analyses initiated a long process of correspondences and a research investigation5. Four years later, the article was retracted (Communication Research Editors 2017) and Jodie Whitaker’s PhD was revoked by Ohio State University. The senior author was dismissed of all allegations and was able to publish a replication of this study shortly after (Bushman 2018). Not all original effects were replicated, including the main findings of controller type. Further, reported in the replication were small effects, significance levels – in most cases – just under .05, and an increase in sample size. This replication appears to be a partial success with outcomes that may suggest a chance finding.\n\r\rFuture Directions in Social Psychology\rIt is clear that replication is an issue to be resolved. The COS has considered ways in which to examine the results of replications (Open Science Collaboration 2012, 2015). In a similar vein, there need to be a further discussion on how to go about replications. Designing a replication study may not be as easy as exactly reproducing the same methods from the investigative study (Maxwell, Lau, and Howard 2015), particularly around sample size and estimated power, to the point where we may have to be just as critical of replication articles as the original investigative article.\nReplicability is also not well addressed in psychology education and training. We are taught to think about how our study, our experiment is unique and what it offers. What’s our twist on the topic? How are we differentiating this work from the work of others? All in attempts to find your work in a reputable academic journal. The publish or perish attitudes that may be conveyed at some institutions would lead to prioritization of new ideas than the confirmation of new and current ones.\nBegeley and Ioannidis (2015) have curated a list of ways in which we can help correct for the lack of replications and the overall goodness of science. Among these are emphasizing greater statistical and experimental methodology knowledge; providing open access to data for examination and analytic replication; using more meta-analytic techniques for establishing general findings in areas of research; lobbying journals to solicit replication bids; and pushing institutional responsibility, possibly to the point of requiring some level of open access or replicability. A suggested cultural change is to consider, more greatly, the quality of research and judge academics and other researchers on their reproducibility, openness and sharing. This may help switch the needed focus towards confirmation rather than simply discovery.\n\rConcluding Remarks\rThe lack of direct replication should undoubtedly be an issue in psychology – especially in the social field – but that that does not mean that all past work is questionable. Assuming that older published works are wrong would create even more issues with meta-analytic methods (Makel, Plucker, and Hegarty 2012). As mentioned earlier, indirect and conceptual replications occur often. For now, these will have to do to demonstrate that phenomenon of past research is sound and replicable.\nWe should leave with a few notes of caution and tips. Research reproduced outside the original laboratory or collaborators aids more in demonstrating reproducibility than multiple articles from the same institution. Although most published replications have at least one of the original authors (Makel, Plucker, and Hegarty 2012) Failure to replicate does not mean that the original effect is invalid. As the COS has suggested, the issue may even be with the attempt the replicate and methodological differences that were overlooked or unnoticed (Open Science Collaboration 2012). There may exists some concepts and theories that are downright wrong and invalid but bad research continues to push these to publishers. Inveresly, a good idea may have gone unnoticed because the first experiment happened to fail. But we ought not to disqualify any large portion of psychology simply due to concerns about reproducibility. Early findings may be a little grim, but those tested may have been chosen for simplicity in design and not necessarily to challenge well-cited work. In fact, Daniel Kahneman’s framing (Tversky and Kahneman 1981) and anchoring effects (Jacowitz and Kahneman 1995) were found to be even stronger in replication attempts (Klein et al. 2014).\nTwo schools of thought, extreme in opposition, can be called upon when reacting to the overall threat of the “replication crisis”. To take the pessimistic meta-inductionism approach would be to begin throwing out research on the grounds that the previous works have been falsified, disproved, or un-proven. It would take that if studies and theories, which were one or still are held as truth are incorrect, why should be not dismiss the bulk of what we know? The epistemic optimist would assert that through rigorous scientific process, what we know now is true, or the approximately true. Yes, some long-held concepts will be dismantled, and others held in a possibly state of limbo of acceptance, but we cannot simply disregard everything and start at square one, again.\n\rReferences\rAnderson, Craig A, and Brad J Bushman. 2001. “Effects of Violent Video Games on Aggressive Behavior, Aggressive Cognition, Aggressive Affect, Physiological Arousal, and Prosocial Behavior: A Meta-Analytic Review of the Scientific Literature.” Psychological Science 12 (5): 353–59. https://doi.org/10.1111/1467-9280.00366.\n\rAyduk, Ozlem, Rodolfo Mendoza-Denton, Walter Mischel, Geraldine Downey, Philip K Peake, and Monica Rodriguez. 2000. “Regulating the Interpersonal Self: Strategic Self-Regulation for Coping with Rejection Sensitivity.” Journal of Personality and Social Psychology 79 (5): 776. https://doi.org/10.1037/0022-3514.79.5.776.\n\rBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature News 533 (7604): 452. https://doi.org/10.1038/533452a.\n\rBegley, C Glenn, and John PA Ioannidis. 2015. “Reproducibility in Science: Improving the Standard for Basic and Preclinical Research.” Circulation Research 116 (1): 116–26. https://doi.org/10.1161/CIRCRESAHA.114.303819.\n\rBraude, Stephen E. 2002. ESP and Psychokinesis: A Philosophical Examination. Universal-Publishers.\n\rBushman, Brad J. 2018. ““Boom, Headshot!”: Violent First-Person Shooter (FPS) Video Games That Reward Headshots Train Individuals to Aim for the Head When Shooting a Realistic Firearm.” Aggressive Behavior 45 (1): 33–41. https://doi.org/10.1002/ab.21794.\n\rCohen, Jacob. 1994. “The Earth Is Round (P0.6em\\(\\less\\)0.6em.05).” American Psychologist 49 (12): 997–1003. https://doi.org/10.1037/0003-066X.49.12.997.\n\rCommunication Research Editors. 2017. “Retraction Notice.” Communication Research 44 (1): 144–44. https://doi.org/10.1177/0093650217690274.\n\rEarp, Brian D, and David Trafimow. 2015. “Replication, Falsification, and the Crisis of Confidence in Social Psychology.” Frontiers in Psychology 6: 621. https://doi.org/10.3389/fpsyg.2015.00621.\n\rFerguson, Christopher John. 2007. “The Good, the Bad and the Ugly: A Meta-Analytic Review of Positive and Negative Effects of Violent Video Games.” Psychiatric Quarterly 78 (4): 309–16. https://doi.org/10.1007/s11126-007-9056-9.\n\rGilbert, Daniel T, Gary King, Stephen Pettigrew, and Timothy D Wilson. 2016. “Comment on ‘Estimating the Reproducibility of Psychological Science’.” Science 351 (6277): 1037–7. https://doi.org/10.1126/science.aad7243.\n\rIoannidis, John PA. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\rJacowitz, Karen E, and Daniel Kahneman. 1995. “Measures of Anchoring in Estimation Tasks.” Personality and Social Psychology Bulletin 21 (11): 1161–6. https://doi.org/10.1177/01461672952111004.\n\rKlein, Richard A., Kate A. Ratliff, Michelangelo Vianello, Reginald B. Adams, Štěpán Bahnı́k, Michael J. Bernstein, Konrad Bocian, et al. 2014. “Investigating Variation in Replicability a \"Many Labs\" Replication Project.” Social Psychology 45 (3): 142–52. https://doi.org/10.1027/1864-9335/a000178.\n\rMakel, Matthew C, Jonathan A Plucker, and Boyd Hegarty. 2012. “Replications in Psychology Research: How Often Do They Really Occur?” Perspectives on Psychological Science 7 (6): 537–42. https://doi.org/10.1177/1745691612460688.\n\rMaxwell, Scott E, Michael Y Lau, and George S Howard. 2015. “Is Psychology Suffering from a Replication Crisis? What Does ‘Failure to Replicate’ Really Mean?” American Psychologist 70 (6): 487. https://doi.org/10.1037/a0039400.\n\rMeehl, Paul E. 1990. “Why Summaries of Research on Psychological Theories Are Often Uninterpretable.” Psychological Reports 66 (1): 195–244. https://doi.org/10.2466/pr0.1990.66.1.195.\n\rMischel, Walter, Yuichi Shoda, and Monica I Rodriguez. 1989. “Delay of Gratification in Children.” Science 244 (4907): 933–38. https://doi.org/10.1126/science.2658056.\n\rOpen Science Collaboration. 2012. “An Open, Large-Scale, Collaborative Effort to Estimate the Reproducibility of Psychological Science.” Perspectives on Psychological Science 7 (6): 657–60. https://doi.org/10.1177/1745691612462588.\n\r———. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251). https://doi.org/10.1126/science.aac4716.\n\rPashler, Harold, and Christine R Harris. 2012. “Is the Replicability Crisis Overblown? Three Arguments Examined.” Perspectives on Psychological Science 7 (6): 531–36. https://doi.org/10.1177/1745691612463401.\n\rShapin, Steven. 1984. “Pump and Circumstance: Robert Boyle’s Literary Technology.” Social Studies of Science 14 (4): 481–520. https://doi.org/10.1177/030631284014004001.\n\rSrivastava, Sanjay. 2009. “Making Progress in the Hardest Science.” https://thehardestscience.com/2009/03/14/making-progress-in-the-hardest-science/.\n\rSterne, Jonathan AC, and George Davey Smith. 2001. “Sifting the Evidence—What’s Wrong with Significance Tests?” Physical Therapy 81 (8): 1464–9. https://doi.org/10.1093/ptj/81.8.1464.\n\rTversky, Amos, and Daniel Kahneman. 1981. “The Framing of Decisions and the Psychology of Choice.” Science 211 (4481): 453–58. https://doi.org/10.1126/science.7455683.\n\rWatts, Tyler W., Greg J. Duncan, and Haonan Quan. 2018. “Revisiting the Marshmallow Test: A Conceptual Replication Investigating Links Between Early Delay of Gratification and Later Outcomes.” Psychological Science 29 (7): 1159–77. https://doi.org/10.1177/0956797618761661.\n\rWhitaker, Jodi L., and Brad J. Bushman. 2012. “RETRACTED: ‘Boom, Headshot!’: Effect of Video Game Play and Controller Type on Firing Aim and Accuracy.” Communication Research 41 (7): 879–91. https://doi.org/10.1177/0093650212446622.\n\r\r\r\rI attempted a similar search by looking at the first 15 articles to which I had immediate access. The median and approximate mean was 6 years – although 11 of the replication articles were published in the 80s or 90s.↩︎\n\rSignificant effects here being defined by a reported p of less than .05.↩︎\n\rThis reported only on the 97 original articles with reported significance and reported on the those with p \u0026lt; .05 in the original direction.↩︎\n\rIncluded examples were the height of Mt Everest, distance to New York City, NY, and the population of Chicago, IL.↩︎\n\rA timeline of the events and correspondences, as well as records of files and reports exchanged, is available here: http://www.malte-elson.com/headshot↩︎\n\r\r\r","id":4,"section":"posts","summary":"Introduction\rWhat is the “Replication Crisis”?\rLarge-scale Replication Attempts\r\rThe Open Science Collaboration\rThe “Many Labs” Project\r\rCase Studies\r\rThe Marshmallow test\rViolence in video games\r\rFuture Directions in Social Psychology\rConcluding Remarks\rReferences\r\r\rAdvanced Social Psychology.\rA project on contemporary topics in Social Psychology.\rI managed to weasel around the requirements (social isn’t quite my field) and present and write on research methodologies, specifically on the “replication crisis” and how it relates to Social Psychology.","tags":["psychology","replication"],"title":"Replication in Psychology","uri":"/1/01/replication-social-psychology/","year":"0001"}],"tags":[{"title":"datavis","uri":"/tags/datavis/"},{"title":"factor analysis","uri":"/tags/factor-analysis/"},{"title":"pgk:pROC","uri":"/tags/pgkproc/"},{"title":"pkg:caret","uri":"/tags/pkgcaret/"},{"title":"psychology","uri":"/tags/psychology/"},{"title":"psychometrics","uri":"/tags/psychometrics/"},{"title":"r","uri":"/tags/r/"},{"title":"replication","uri":"/tags/replication/"},{"title":"roc","uri":"/tags/roc/"}]}